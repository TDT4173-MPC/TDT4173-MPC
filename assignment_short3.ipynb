{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostRegressor\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the data to hourly with some aggregation methods for each column\n",
    "aggregation_methods = {\n",
    "    'date_forecast': 'first',\n",
    "    'diffuse_rad:W': 'sum',\n",
    "    'direct_rad:W': 'last',\n",
    "    'clear_sky_rad:W': 'sum',\n",
    "    'diffuse_rad_1h:J': 'last',\n",
    "    'direct_rad_1h:J': 'last',\n",
    "    'clear_sky_energy_1h:J': 'last',\n",
    "    'absolute_humidity_2m:gm3': 'mean',\n",
    "    'air_density_2m:kgm3': 'mean',\n",
    "    'ceiling_height_agl:m': 'max',\n",
    "    'cloud_base_agl:m': 'mean',\n",
    "    'dew_or_rime:idx': 'min',\n",
    "    'dew_point_2m:K': 'mean',\n",
    "    'effective_cloud_cover:p': 'sum',\n",
    "    'elevation:m': 'first',\n",
    "    'fresh_snow_12h:cm': 'max',\n",
    "    'fresh_snow_1h:cm': 'sum',\n",
    "    'fresh_snow_24h:cm': 'max',\n",
    "    'fresh_snow_3h:cm': 'max',\n",
    "    'fresh_snow_6h:cm': 'max',\n",
    "    'is_day:idx': 'max',\n",
    "    'is_in_shadow:idx': 'max',\n",
    "    'msl_pressure:hPa': 'mean',\n",
    "    'precip_5min:mm': 'sum',\n",
    "    'precip_type_5min:idx': 'sum',\n",
    "    'pressure_100m:hPa': 'mean',\n",
    "    'pressure_50m:hPa': 'mean',\n",
    "    'prob_rime:p': 'max',\n",
    "    'rain_water:kgm2': 'sum',\n",
    "    'relative_humidity_1000hPa:p': 'mean',\n",
    "    'sfc_pressure:hPa': 'mean',\n",
    "    'snow_density:kgm3': 'mean',\n",
    "    'snow_depth:cm': 'max',\n",
    "    'snow_drift:idx': 'max',\n",
    "    'snow_melt_10min:mm': 'sum',\n",
    "    'snow_water:kgm2': 'sum',\n",
    "    'sun_azimuth:d': 'first',\n",
    "    'sun_elevation:d': 'sum',\n",
    "    'super_cooled_liquid_water:kgm2': 'sum',\n",
    "    't_1000hPa:K': 'mean',\n",
    "    'total_cloud_cover:p': 'mean',\n",
    "    'visibility:m': 'mean',\n",
    "    'wind_speed_10m:ms': 'mean',\n",
    "    'wind_speed_u_10m:ms': 'mean',\n",
    "    'wind_speed_v_10m:ms': 'mean',\n",
    "    'wind_speed_w_1000hPa:ms': 'mean',\n",
    "    'cloud_base_agl:m': 'max',\n",
    "    'snow_density:kgm3': 'mean'\n",
    "}\n",
    "\n",
    "# Read in the data\n",
    "x_target_A = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "x_train_obs_A = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "x_train_est_A = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "x_test_est_A = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "x_target_B = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "x_train_obs_B = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "x_train_est_B = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "x_test_est_B = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "x_target_C = pd.read_parquet('./data/C/train_targets.parquet')\n",
    "x_train_obs_C = pd.read_parquet('./data/C/X_train_observed.parquet')\n",
    "x_train_est_C = pd.read_parquet('./data/C/X_train_estimated.parquet')\n",
    "x_test_est_C = pd.read_parquet('./data/C/X_test_estimated.parquet')\n",
    "\n",
    "# Rename time to date_forecast in target\n",
    "x_target_A.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "x_target_B.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "x_target_C.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "\n",
    "# Fix missing data for test set. Assumin NaN means 0 in these categories\n",
    "x_test_est_A['effective_cloud_cover:p'] = x_test_est_A['effective_cloud_cover:p'].fillna(0)\n",
    "x_test_est_B['effective_cloud_cover:p'] = x_test_est_B['effective_cloud_cover:p'].fillna(0)\n",
    "x_test_est_C['effective_cloud_cover:p'] = x_test_est_C['effective_cloud_cover:p'].fillna(0)\n",
    "\n",
    "x_test_est_A['total_cloud_cover:p'] = x_test_est_A['total_cloud_cover:p'].fillna(0)\n",
    "x_test_est_B['total_cloud_cover:p'] = x_test_est_B['total_cloud_cover:p'].fillna(0)\n",
    "x_test_est_C['total_cloud_cover:p'] = x_test_est_C['total_cloud_cover:p'].fillna(0)\n",
    "\n",
    "x_test_est_A['cloud_base_agl:m'] = x_test_est_A['cloud_base_agl:m'].fillna(0)\n",
    "x_test_est_B['cloud_base_agl:m'] = x_test_est_B['cloud_base_agl:m'].fillna(0)\n",
    "x_test_est_C['cloud_base_agl:m'] = x_test_est_C['cloud_base_agl:m'].fillna(0)\n",
    "\n",
    "x_test_est_A['ceiling_height_agl:m'] = x_test_est_A['ceiling_height_agl:m'].fillna(0)\n",
    "x_test_est_B['ceiling_height_agl:m'] = x_test_est_B['ceiling_height_agl:m'].fillna(0)\n",
    "x_test_est_C['ceiling_height_agl:m'] = x_test_est_C['ceiling_height_agl:m'].fillna(0)\n",
    "\n",
    "x_test_est_A['snow_density:kgm3'] = x_test_est_A['snow_density:kgm3'].fillna(0)\n",
    "x_test_est_B['snow_density:kgm3'] = x_test_est_B['snow_density:kgm3'].fillna(0)\n",
    "x_test_est_C['snow_density:kgm3'] = x_test_est_C['snow_density:kgm3'].fillna(0)\n",
    "\n",
    "x_test_est_A['snow_drift:idx'] = x_test_est_A['snow_drift:idx'].fillna(0)\n",
    "x_test_est_B['snow_drift:idx'] = x_test_est_B['snow_drift:idx'].fillna(0)\n",
    "x_test_est_C['snow_drift:idx'] = x_test_est_C['snow_drift:idx'].fillna(0)\n",
    "\n",
    "# Resample\n",
    "x_train_obs_A_resampled = x_train_obs_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_A_resampled = x_train_est_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_A_resampled = x_test_est_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "x_train_obs_B_resampled = x_train_obs_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_B_resampled = x_train_est_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_B_resampled = x_test_est_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "x_train_obs_C_resampled = x_train_obs_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_C_resampled = x_train_est_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_C_resampled = x_test_est_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "# Merge\n",
    "split_value = x_train_est_A['date_forecast'].iloc[0]\n",
    "split_index = x_target_A[x_target_A['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_A = x_target_A.iloc[:split_index]\n",
    "x_target_est_A = x_target_A.iloc[split_index:]\n",
    "\n",
    "obs_A = x_train_obs_A_resampled.merge(x_target_obs_A, left_index=True, right_on='date_forecast')\n",
    "est_A = x_train_est_A_resampled.merge(x_target_est_A, left_index=True, right_on='date_forecast')\n",
    "\n",
    "split_value = x_train_est_B['date_forecast'].iloc[0]\n",
    "split_index = x_target_B[x_target_B['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_B = x_target_B.iloc[:split_index]\n",
    "x_target_est_B = x_target_B.iloc[split_index:]\n",
    "\n",
    "obs_B = x_train_obs_B_resampled.merge(x_target_obs_B, left_index=True, right_on='date_forecast')\n",
    "est_B = x_train_est_B_resampled.merge(x_target_est_B, left_index=True, right_on='date_forecast')\n",
    "\n",
    "split_value = x_train_est_C['date_forecast'].iloc[0]\n",
    "split_index = x_target_C[x_target_C['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_C = x_target_C.iloc[:split_index]\n",
    "x_target_est_C = x_target_C.iloc[split_index:]\n",
    "\n",
    "obs_C = x_train_obs_C_resampled.merge(x_target_obs_C, left_index=True, right_on='date_forecast')\n",
    "est_C = x_train_est_C_resampled.merge(x_target_est_C, left_index=True, right_on='date_forecast')\n",
    "\n",
    "# Keep date_forecast in test dfs\n",
    "test_A = x_test_est_A_resampled\n",
    "test_A = x_test_est_B_resampled\n",
    "test_A = x_test_est_C_resampled\n",
    "\n",
    "# Drop all the NaNs\n",
    "test_A = test_A.dropna()\n",
    "test_A = test_A.dropna()\n",
    "test_A = test_A.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_experimental_features(df):\n",
    "    \"\"\"\n",
    "    Experimental feature engineering.\n",
    "    \"\"\"\n",
    "\n",
    "    # Radiation Features\n",
    "    df['total_radiation:W'] = df['direct_rad:W'] + df['diffuse_rad:W']\n",
    "    df['total_radiation_1h:J'] = df['direct_rad_1h:J'] + df['diffuse_rad_1h:J']\n",
    "    df['rad_diff:W'] = df['direct_rad:W'] - df['diffuse_rad:W']\n",
    "    df['rad_diff_1h:J'] = df['direct_rad_1h:J'] - df['diffuse_rad_1h:J']\n",
    "    df['diffuse_direct_ratio'] = df['diffuse_rad:W'] / df['direct_rad:W']\n",
    "\n",
    "    # Temperature and Pressure Features\n",
    "    df['temp_dewpoint_diff'] = df['t_1000hPa:K'] - df['dew_point_2m:K']\n",
    "    df['pressure_gradient'] = df['pressure_100m:hPa'] - df['pressure_50m:hPa']\n",
    "    df['t_1000hPa:C'] = df['t_1000hPa:K'] - 273.15\n",
    "    df['dew_point_2m:C'] = df['dew_point_2m:K'] - 273.15\n",
    "    df['msl_pressure:hPa_scaled'] = MinMaxScaler().fit_transform(df['msl_pressure:hPa'].values.reshape(-1, 1))\n",
    "    df['sfc_pressure:hPa_scaled'] = MinMaxScaler().fit_transform(df['sfc_pressure:hPa'].values.reshape(-1, 1))\n",
    "\n",
    "    # Wind Features\n",
    "    df['wind_vector_magnitude'] = (df['wind_speed_u_10m:ms']**2 + df['wind_speed_v_10m:ms']**2 + df['wind_speed_w_1000hPa:ms']**2)**0.5\n",
    "    df['average_wind_speed'] = (df['wind_speed_10m:ms'] + df['wind_speed_u_10m:ms']) / 2\n",
    "\n",
    "    # Cloud and Snow Features\n",
    "    df['cloud_humidity_product'] = df['total_cloud_cover:p'] * df['absolute_humidity_2m:gm3']\n",
    "    df['snow_accumulation'] = df[['fresh_snow_24h:cm', 'fresh_snow_12h:cm', 'fresh_snow_6h:cm', 'fresh_snow_3h:cm', 'fresh_snow_1h:cm']].sum(axis=1)\n",
    "\n",
    "    # Interaction between radiation and cloud cover\n",
    "    df['radiation_cloud_interaction'] = df['direct_rad:W'] * df['effective_cloud_cover:p']\n",
    "\n",
    "    # Interaction between temperature and radiation (considering that high temperature may reduce efficiency)\n",
    "    df['temp_rad_interaction'] = df['t_1000hPa:K'] * df['total_radiation:W']\n",
    "\n",
    "    # Interaction between wind cooling effect and temperature\n",
    "    df['wind_temp_interaction'] = df['average_wind_speed'] * df['t_1000hPa:K']\n",
    "\n",
    "    # Interaction between humidity and temperature\n",
    "    df['humidity_temp_interaction'] = df['absolute_humidity_2m:gm3'] * df['t_1000hPa:K']\n",
    "\n",
    "    # Interaction between humidity and radiation\n",
    "    df['sun_elevation_direct_rad_interaction'] = df['sun_elevation:d'] * df['direct_rad:W']\n",
    "\n",
    "    # Precipitation Features\n",
    "    df['precip'] = df['precip_5min:mm']*df['precip_type_5min:idx']\n",
    "\n",
    "    # Safeguard in case of inf values\n",
    "    df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_date_features(df):\n",
    "    \"\"\"\n",
    "    Adds 'month', 'year', 'hour' and 'day' columns to the dataframe based on the 'date_forecast' column.\n",
    "    Also adds 'hour_sin' and 'hour_cos' columns for the hour of the day.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if 'date_forecast' exists in the dataframe\n",
    "    if 'date_forecast' in df.columns:\n",
    "        # Convert the 'date_forecast' column to datetime format\n",
    "        df['date_forecast'] = pd.to_datetime(df['date_forecast'])\n",
    "        \n",
    "        # Extract month, year, hour and day\n",
    "        df['month'] = df['date_forecast'].dt.month\n",
    "        df['year'] = df['date_forecast'].dt.year\n",
    "        df['hour'] = df['date_forecast'].dt.hour\n",
    "        df['day'] = df['date_forecast'].dt.day\n",
    "        df['hour_sin'] = np.sin(2*np.pi*df['hour']/24)\n",
    "        df['hour_cos'] = np.cos(2*np.pi*df['hour']/24)\n",
    "\n",
    "    else:\n",
    "        print(\"Warning: 'date_forecast' column not found in the dataframe. No date features added.\")\n",
    "        return df  # Keep the 'date_forecast' column in the dataframe\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding discretized features for the continuous variables to help tree-based models\n",
    "\n",
    "def bin_columns(dataframe, columns_to_bin, n_bins=5):\n",
    "    \"\"\"\n",
    "    Bins the specified columns of the dataframe into equal-sized bins.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframe: pd.DataFrame\n",
    "    - columns_to_bin: list of strings, the names of the columns to bin\n",
    "    - n_bins: int or dict, the number of bins for each column (if int, use the same number for all columns;\n",
    "              if dict, specify individual numbers with column names as keys)\n",
    "    \n",
    "    Returns:\n",
    "    - binned_dataframe: pd.DataFrame, the dataframe with the specified columns binned\n",
    "    \"\"\"\n",
    "    binned_dataframe = dataframe.copy()\n",
    "    \n",
    "    for column in columns_to_bin:\n",
    "        # Determine the number of bins for this column\n",
    "        bins = n_bins if isinstance(n_bins, int) else n_bins.get(column, 5)\n",
    "        \n",
    "        # Create quantile-based bins\n",
    "        binned_dataframe[f'binned_{column}'] = pd.qcut(\n",
    "            binned_dataframe[column],\n",
    "            q=bins,\n",
    "            labels=False,\n",
    "            duplicates='drop'\n",
    "        )\n",
    "        \n",
    "    return binned_dataframe\n",
    "\n",
    "def add_binned_features(df):\n",
    "    columns_to_bin = [\n",
    "        'super_cooled_liquid_water:kgm2',\n",
    "        'ceiling_height_agl:m',\n",
    "        'cloud_base_agl:m'\n",
    "    ]\n",
    "\n",
    "    # Bin the columns\n",
    "    # df = bin_columns(df, columns_to_bin)\n",
    "    df = bin_columns(df, ['effective_cloud_cover:p'], n_bins=2)\n",
    "    df = bin_columns(df, ['ceiling_height_agl:m'], n_bins=3)\n",
    "    df = bin_columns(df, ['average_wind_speed'], n_bins=5)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rate_of_change_features(df, features, second_order=False):\n",
    "    \"\"\"\n",
    "    Adds rate of change columns for specified features in the dataframe.\n",
    "    Assumes the dataframe is time sorted. If second_order is True, it also adds the second order rate of change.\n",
    "    \"\"\"\n",
    "    for feature in features:\n",
    "        rate_column_name = feature + '_rate_of_change'\n",
    "        df[rate_column_name] = df[feature].diff().fillna(0)  # Handle the first diff NaN if required\n",
    "        \n",
    "        if second_order:  # Check if second order difference is required\n",
    "            second_order_column_name = feature + '_rate_of_change_of_change'\n",
    "            df[second_order_column_name] = df[rate_column_name].diff().fillna(0)  # Second order difference\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_rate_of_change_features_to_df(df):\n",
    "    # Define the features for which to calculate rate of change\n",
    "    features_to_diff = [\n",
    "        't_1000hPa:K',\n",
    "        'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad:W',\n",
    "        'effective_cloud_cover:p', 'total_radiation:W'\n",
    "    ]\n",
    "\n",
    "    # Add rate of change features\n",
    "    return add_rate_of_change_features(df, features_to_diff, second_order=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_est_obs_feature(df):\n",
    "    \"\"\"\n",
    "    Adds a column to the dataframe that indicates whether the data is estimated or observed.\n",
    "    \"\"\"\n",
    "    # Add the est_obs feature\n",
    "    if 'date_calc' not in df.columns:\n",
    "        # If 'date_calc' does not exist, create 'observed' column and set to 1\n",
    "        df['observed'] = 1\n",
    "        return df\n",
    "    else:\n",
    "        # If 'date_calc' exists, create a new column and set values to 0\n",
    "        df['observed'] = 0\n",
    "        return df.drop(columns=['date_calc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_constant_regions(dataframe, column_name=\"pv_measurement\", threshold=72):\n",
    "    \"\"\"\n",
    "    Removes rows where the specified column has constant values for more than the given threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the specified column exists in the dataframe\n",
    "    if column_name not in dataframe.columns:\n",
    "        print(f\"Warning: '{column_name}' column not found in the dataframe. No rows removed.\")\n",
    "        return dataframe\n",
    "    \n",
    "    same_as_previous = dataframe[column_name].eq(dataframe[column_name].shift())\n",
    "    group_ids = (~same_as_previous).cumsum()\n",
    "    to_remove = group_ids[same_as_previous].value_counts() > threshold\n",
    "    group_ids_to_remove = to_remove[to_remove].index\n",
    "    \n",
    "    # Drop entire rows that match the conditions\n",
    "    return dataframe.drop(dataframe[group_ids.isin(group_ids_to_remove)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lagged_features(df, features_with__lags, fill_value=None):\n",
    "    \"\"\"\n",
    "    Adds lagged columns for specified features in the dataframe with specific lag periods.\n",
    "    'features_with_specific_lags' is a dictionary with features as keys and specific lag as values.\n",
    "    'fill_value' is what to fill the NaNs with, after shifting.\n",
    "    \"\"\"\n",
    "    for feature, specific_lag in features_with__lags.items():\n",
    "        lag_column_name = f\"{feature}_lag_{specific_lag}\"\n",
    "        df[lag_column_name] = df[feature].shift(specific_lag).fillna(fill_value)\n",
    "    return df\n",
    "\n",
    "def add_lagged_features_to_df(df):\n",
    "    features_with_lags = {\n",
    "        'total_radiation:W': 1,\n",
    "        'total_radiation:W': -1,\n",
    "        'rad_diff:W': 1,\n",
    "        'rad_diff:W': -1,\n",
    "        'total_radiation_1h:J': 1,\n",
    "        'total_radiation_1h:J': -1\n",
    "    }\n",
    "\n",
    "    # Add lagged features for specific lags\n",
    "    return add_lagged_features(df, features_with_lags, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_nan(df):\n",
    "    # Remove the rows where target is nan\n",
    "    try:\n",
    "        df = df[df['pv_measurement'].notna()]\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    # Set all remaining nans to 0\n",
    "    return df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    df = add_experimental_features(df)\n",
    "    df = add_date_features(df)\n",
    "    df = add_binned_features(df)\n",
    "    df = add_rate_of_change_features_to_df(df)\n",
    "    df = add_est_obs_feature(df)\n",
    "    df = remove_constant_regions(df)\n",
    "    df = add_lagged_features_to_df(df)\n",
    "    df = handle_nan(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Preprocess\n",
    "obs_A = preprocessing(obs_A)\n",
    "est_A = preprocessing(est_A)\n",
    "test_A = preprocessing(test_A)\n",
    "\n",
    "obs_B = preprocessing(obs_B)\n",
    "est_B = preprocessing(est_B)\n",
    "test_B = preprocessing(test_A)\n",
    "\n",
    "obs_C = preprocessing(obs_C)\n",
    "est_C = preprocessing(est_C)\n",
    "test_C = preprocessing(test_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seeds used for reproducibility\n",
    "# 32 weights: 0.3, 0.3, 0.4\n",
    "# 24 weights: 0.3, 0.3, 0.4\n",
    "# 33 (without winter months 1 and 12) weights: 0.2, 0.4, 0.4\n",
    "# 11 (without winter months 1, 2 and 11, 12) weights: 0.25, 0.35, 0.4\n",
    "# 5 weights: 0.4, 0.3, 0.3\n",
    "\n",
    "# Best score is the mean prediction of all the 5 seeds mentioned above. The first weight is xgboost, the second is catboost, and the third is autogluon.\n",
    "\n",
    "# Set the random seed\n",
    "np.random.seed(32)\n",
    "\n",
    "# Concatinate\n",
    "A = pd.concat([obs_A, est_A])\n",
    "B = pd.concat([obs_B, est_B])\n",
    "C = pd.concat([obs_C, est_C])\n",
    "\n",
    "# Remove characters unparseable for CatBoost \n",
    "A.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in A.columns]\n",
    "B.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in B.columns]\n",
    "C.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in C.columns]\n",
    "\n",
    "test_A.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_A.columns]\n",
    "test_B.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_B.columns]\n",
    "test_C.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_C.columns]\n",
    "\n",
    "# Getting validation data from summer months, because the test set is from summer months. We experimentet with excluding winter months\n",
    "# from the training data here.\n",
    "\n",
    "# Step 1: Filter A to include only months from March to October\n",
    "A = A[A['date_forecast'].dt.month.isin([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])]\n",
    "\n",
    "# Step 2: Identify unique days within May, June, and July\n",
    "summer_months = A[A['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "\n",
    "# Step 3: Sample these days for val_A\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_A = A[A['date_forecast'].dt.date.isin(sampled_days)]\n",
    "\n",
    "# Step 4: Define train_A as the remaining data\n",
    "train_A = A[~A['date_forecast'].dt.date.isin(sampled_days)]\n",
    "\n",
    "# Prepare your features and target variables\n",
    "X_train_A = train_A.drop(columns='pv_measurement')\n",
    "y_train_A = train_A['pv_measurement']\n",
    "X_val_A = val_A.drop(columns='pv_measurement')\n",
    "y_val_A = val_A['pv_measurement']\n",
    "\n",
    "# Repeat for B and C\n",
    "B = B[B['date_forecast'].dt.month.isin([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])]\n",
    "summer_months = B[B['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_B = B[B['date_forecast'].dt.date.isin(sampled_days)]\n",
    "train_B = B[~B['date_forecast'].dt.date.isin(sampled_days)]\n",
    "X_train_B = train_B.drop(columns='pv_measurement')\n",
    "y_train_B = train_B['pv_measurement']\n",
    "X_val_B = val_B.drop(columns='pv_measurement')\n",
    "y_val_B = val_B['pv_measurement']\n",
    "\n",
    "C = C[C['date_forecast'].dt.month.isin([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])]\n",
    "summer_months = C[C['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_C = C[C['date_forecast'].dt.date.isin(sampled_days)]\n",
    "train_C = C[~C['date_forecast'].dt.date.isin(sampled_days)]\n",
    "X_train_C = train_C.drop(columns='pv_measurement')\n",
    "y_train_C = train_C['pv_measurement']\n",
    "X_val_C = val_C.drop(columns='pv_measurement')\n",
    "y_val_C = val_C['pv_measurement']\n",
    "\n",
    "# Drop date_forecast\n",
    "train_A = train_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "train_B = train_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "train_C = train_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_A = val_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_B = val_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_C = val_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_A = X_train_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_B = X_train_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_C = X_train_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_A = X_val_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_B = X_val_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_C = X_val_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "test_A = test_A.drop(columns=['date_forecast'])\n",
    "test_B = test_B.drop(columns=['date_forecast'])\n",
    "test_C = test_C.drop(columns=['date_forecast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auto_A = TabularDataset(train_A)\n",
    "val_auto_A = TabularDataset(val_A)\n",
    "\n",
    "train_auto_B = TabularDataset(train_B)\n",
    "val_auto_B = TabularDataset(val_B)\n",
    "\n",
    "train_auto_C = TabularDataset(train_C)\n",
    "val_auto_C = TabularDataset(val_C)\n",
    "\n",
    "auto_label = 'pv_measurement'\n",
    "\n",
    "# Set the parameters for the XGBoost models\n",
    "params_xgb_A = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.4, \n",
    "    'learning_rate': 0.012, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 0.8, \n",
    "    'reg_lambda': 0.8, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1,\n",
    "    'num_parallel_tree': 2\n",
    "}\n",
    "\n",
    "params_xgb_B = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.8, \n",
    "    'learning_rate': 0.008, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 1, \n",
    "    'reg_lambda': 3, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "params_xgb_C = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.8, \n",
    "    'learning_rate': 0.008, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 1, \n",
    "    'reg_lambda': 3, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "xgb_A = xgb.XGBRegressor(**params_xgb_A)\n",
    "xgb_B = xgb.XGBRegressor(**params_xgb_B)\n",
    "xgb_C = xgb.XGBRegressor(**params_xgb_C)\n",
    "\n",
    "cat_A = CatBoostRegressor(\n",
    "    iterations=5000,         # The number of trees to build\n",
    "    #learning_rate=0.09,     # The learning rate\n",
    "    #depth=10,               # Depth of the tree\n",
    "    loss_function='MAE',     # Loss function to be optimized. RMSE is common for regression.\n",
    "    eval_metric='MAE',       # Evaluation metric for the validation set\n",
    "    #random_seed=42,         # Seed for reproducibility\n",
    "    #verbose=100             # Frequency of logging the training process\n",
    ")\n",
    "\n",
    "cat_B = CatBoostRegressor(\n",
    "    iterations=5000,\n",
    "    #learning_rate=0.09,\n",
    "    #depth=10,\n",
    "    loss_function='MAE',\n",
    "    eval_metric='MAE',\n",
    "    #random_seed=42,\n",
    "    #verbose=100\n",
    ")\n",
    "\n",
    "cat_C = CatBoostRegressor(\n",
    "    iterations=5000,\n",
    "    #learning_rate=0.09,\n",
    "    #depth=10,\n",
    "    loss_function='MAE',\n",
    "    eval_metric='MAE',\n",
    "    #random_seed=42,\n",
    "    #verbose=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for the XGBoost models. We got them to work the best when having fewer columns\n",
    "xgb_columns = [\n",
    "    'total_radiationW',\n",
    "    'snow_accumulation',\n",
    "    'super_cooled_liquid_waterkgm2',\n",
    "    'average_wind_speed',\n",
    "    'sun_elevationd',\n",
    "    'sun_azimuthd',\n",
    "    'clear_sky_radW',\n",
    "    'month',\n",
    "    't_1000hPaC',\n",
    "    'msl_pressurehPa_scaled',\n",
    "    'rain_waterkgm2',\n",
    "    'cloud_base_aglm',\n",
    "    'effective_cloud_coverp',\n",
    "    'dew_or_rimeidx'\n",
    "]\n",
    "\n",
    "X_train_xgb_A = train_A[xgb_columns]\n",
    "y_train_xgb_A = train_A['pv_measurement']\n",
    "X_test_xgb_A = test_A[xgb_columns]\n",
    "\n",
    "X_train_xgb_B = train_B[xgb_columns]\n",
    "y_train_xgb_B = train_B['pv_measurement']\n",
    "X_test_xgb_B = test_A[xgb_columns]\n",
    "\n",
    "X_train_xgb_C = train_C[xgb_columns]\n",
    "y_train_xgb_C = train_C['pv_measurement']\n",
    "X_test_xgb_C = test_A[xgb_columns]\n",
    "\n",
    "# Train the XGBoost models\n",
    "xgb_A.fit(\n",
    "    X=X_train_xgb_A, y=y_train_xgb_A,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "xgb_B.fit(\n",
    "    X=X_train_xgb_B, y=y_train_xgb_B,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "xgb_C.fit(\n",
    "    X=X_train_xgb_C, y=y_train_xgb_C,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CatBoost models\n",
    "cat_A.fit(\n",
    "    X_train_A, y_train_A,\n",
    "    eval_set=(X_val_A, y_val_A),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "cat_B.fit(\n",
    "    X_train_B, y_train_B,\n",
    "    eval_set=(X_val_B, y_val_B),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "cat_C.fit(\n",
    "    X_train_C, y_train_C,\n",
    "    eval_set=(X_val_C, y_val_C),\n",
    "    use_best_model=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_A = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_A, \n",
    "                                                                                   presets='medium_quality', \n",
    "                                                                                   tuning_data=val_auto_A, \n",
    "                                                                                   use_bag_holdout=True, \n",
    "                                                                                   ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "auto_B = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_B,\n",
    "                                                                                      presets='medium_quality',\n",
    "                                                                                      tuning_data=val_auto_B,\n",
    "                                                                                      use_bag_holdout=True,\n",
    "                                                                                      ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "auto_C = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_C,\n",
    "                                                                                        presets='medium_quality',\n",
    "                                                                                        tuning_data=val_auto_C,\n",
    "                                                                                        use_bag_holdout=True,\n",
    "                                                                                        ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_weight = 0.4\n",
    "cat_weight = 0.3\n",
    "auto_weight = 0.3\n",
    "\n",
    "pred_xgb_A = xgb_A.predict(X_test_xgb_C)\n",
    "pred_xgb_B = xgb_B.predict(X_test_xgb_C)\n",
    "pred_xgb_C = xgb_C.predict(X_test_xgb_C)\n",
    "\n",
    "pred_auto_A = auto_A.predict(test_A)\n",
    "pred_auto_B = auto_B.predict(test_B)\n",
    "pred_auto_C = auto_C.predict(test_C)\n",
    "\n",
    "pred_cat_A = cat_A.predict(test_A)\n",
    "pred_cat_B = cat_B.predict(test_B)\n",
    "pred_cat_C = cat_C.predict(test_C)\n",
    "\n",
    "# Ensemble that seemed the best after some experimentation\n",
    "pred_A = (pred_xgb_A*xgb_weight + pred_cat_A*cat_weight + pred_auto_A*auto_weight)\n",
    "pred_B = (pred_xgb_B*xgb_weight + pred_cat_B*cat_weight + pred_auto_B*auto_weight)\n",
    "pred_C = (pred_xgb_C*xgb_weight + pred_cat_C*cat_weight + pred_auto_C*auto_weight)\n",
    "\n",
    "pred_A = np.clip(pred_A, 0, None)\n",
    "pred_B = np.clip(pred_B, 0, None)\n",
    "pred_C = np.clip(pred_C, 0, None)\n",
    "\n",
    "# Concatenate predictions\n",
    "predictions = np.concatenate([pred_A, pred_B, pred_C])\n",
    "\n",
    "# Save predictions\n",
    "predictions_1 = predictions\n",
    "\n",
    "# Create an id array\n",
    "ids = np.arange(0, len(predictions_1))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'prediction': predictions_1\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('predictions_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence 2\n",
    "\n",
    "%reset -f\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostRegressor\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def add_experimental_features(df):\n",
    "    \"\"\"\n",
    "    Experimental feature engineering.\n",
    "    \"\"\"\n",
    "\n",
    "    # Radiation Features\n",
    "    df['total_radiation:W'] = df['direct_rad:W'] + df['diffuse_rad:W']\n",
    "    df['total_radiation_1h:J'] = df['direct_rad_1h:J'] + df['diffuse_rad_1h:J']\n",
    "    df['rad_diff:W'] = df['direct_rad:W'] - df['diffuse_rad:W']\n",
    "    df['rad_diff_1h:J'] = df['direct_rad_1h:J'] - df['diffuse_rad_1h:J']\n",
    "    df['diffuse_direct_ratio'] = df['diffuse_rad:W'] / df['direct_rad:W']\n",
    "\n",
    "    # Temperature and Pressure Features\n",
    "    df['temp_dewpoint_diff'] = df['t_1000hPa:K'] - df['dew_point_2m:K']\n",
    "    df['pressure_gradient'] = df['pressure_100m:hPa'] - df['pressure_50m:hPa']\n",
    "    df['t_1000hPa:C'] = df['t_1000hPa:K'] - 273.15\n",
    "    df['dew_point_2m:C'] = df['dew_point_2m:K'] - 273.15\n",
    "    df['msl_pressure:hPa_scaled'] = MinMaxScaler().fit_transform(df['msl_pressure:hPa'].values.reshape(-1, 1))\n",
    "    df['sfc_pressure:hPa_scaled'] = MinMaxScaler().fit_transform(df['sfc_pressure:hPa'].values.reshape(-1, 1))\n",
    "\n",
    "    # Wind Features\n",
    "    df['wind_vector_magnitude'] = (df['wind_speed_u_10m:ms']**2 + df['wind_speed_v_10m:ms']**2 + df['wind_speed_w_1000hPa:ms']**2)**0.5\n",
    "    df['average_wind_speed'] = (df['wind_speed_10m:ms'] + df['wind_speed_u_10m:ms']) / 2\n",
    "\n",
    "    # Cloud and Snow Features\n",
    "    df['cloud_humidity_product'] = df['total_cloud_cover:p'] * df['absolute_humidity_2m:gm3']\n",
    "    df['snow_accumulation'] = df[['fresh_snow_24h:cm', 'fresh_snow_12h:cm', 'fresh_snow_6h:cm', 'fresh_snow_3h:cm', 'fresh_snow_1h:cm']].sum(axis=1)\n",
    "\n",
    "    # Interaction between radiation and cloud cover\n",
    "    df['radiation_cloud_interaction'] = df['direct_rad:W'] * df['effective_cloud_cover:p']\n",
    "\n",
    "    # Interaction between temperature and radiation (considering that high temperature may reduce efficiency)\n",
    "    df['temp_rad_interaction'] = df['t_1000hPa:K'] * df['total_radiation:W']\n",
    "\n",
    "    # Interaction between wind cooling effect and temperature\n",
    "    df['wind_temp_interaction'] = df['average_wind_speed'] * df['t_1000hPa:K']\n",
    "\n",
    "    # Interaction between humidity and temperature\n",
    "    df['humidity_temp_interaction'] = df['absolute_humidity_2m:gm3'] * df['t_1000hPa:K']\n",
    "\n",
    "    # Interaction between humidity and radiation\n",
    "    df['sun_elevation_direct_rad_interaction'] = df['sun_elevation:d'] * df['direct_rad:W']\n",
    "\n",
    "    # Precipitation Features\n",
    "    df['precip'] = df['precip_5min:mm']*df['precip_type_5min:idx']\n",
    "\n",
    "    # Safeguard in case of inf values\n",
    "    df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_date_features(df):\n",
    "    \"\"\"\n",
    "    Adds 'month', 'year', 'hour' and 'day' columns to the dataframe based on the 'date_forecast' column.\n",
    "    Also adds 'hour_sin' and 'hour_cos' columns for the hour of the day.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if 'date_forecast' exists in the dataframe\n",
    "    if 'date_forecast' in df.columns:\n",
    "        # Convert the 'date_forecast' column to datetime format\n",
    "        df['date_forecast'] = pd.to_datetime(df['date_forecast'])\n",
    "        \n",
    "        # Extract month, year, hour and day\n",
    "        df['month'] = df['date_forecast'].dt.month\n",
    "        df['year'] = df['date_forecast'].dt.year\n",
    "        df['hour'] = df['date_forecast'].dt.hour\n",
    "        df['day'] = df['date_forecast'].dt.day\n",
    "        df['hour_sin'] = np.sin(2*np.pi*df['hour']/24)\n",
    "        df['hour_cos'] = np.cos(2*np.pi*df['hour']/24)\n",
    "\n",
    "    else:\n",
    "        print(\"Warning: 'date_forecast' column not found in the dataframe. No date features added.\")\n",
    "        return df  # Keep the 'date_forecast' column in the dataframe\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Adding discretized features for the continuous variables to help tree-based models\n",
    "\n",
    "def bin_columns(dataframe, columns_to_bin, n_bins=5):\n",
    "    \"\"\"\n",
    "    Bins the specified columns of the dataframe into equal-sized bins.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframe: pd.DataFrame\n",
    "    - columns_to_bin: list of strings, the names of the columns to bin\n",
    "    - n_bins: int or dict, the number of bins for each column (if int, use the same number for all columns;\n",
    "              if dict, specify individual numbers with column names as keys)\n",
    "    \n",
    "    Returns:\n",
    "    - binned_dataframe: pd.DataFrame, the dataframe with the specified columns binned\n",
    "    \"\"\"\n",
    "    binned_dataframe = dataframe.copy()\n",
    "    \n",
    "    for column in columns_to_bin:\n",
    "        # Determine the number of bins for this column\n",
    "        bins = n_bins if isinstance(n_bins, int) else n_bins.get(column, 5)\n",
    "        \n",
    "        # Create quantile-based bins\n",
    "        binned_dataframe[f'binned_{column}'] = pd.qcut(\n",
    "            binned_dataframe[column],\n",
    "            q=bins,\n",
    "            labels=False,\n",
    "            duplicates='drop'\n",
    "        )\n",
    "        \n",
    "    return binned_dataframe\n",
    "\n",
    "def add_binned_features(df):\n",
    "    columns_to_bin = [\n",
    "        'super_cooled_liquid_water:kgm2',\n",
    "        'ceiling_height_agl:m',\n",
    "        'cloud_base_agl:m'\n",
    "    ]\n",
    "\n",
    "    # Bin the columns\n",
    "    # df = bin_columns(df, columns_to_bin)\n",
    "    df = bin_columns(df, ['effective_cloud_cover:p'], n_bins=2)\n",
    "    df = bin_columns(df, ['ceiling_height_agl:m'], n_bins=3)\n",
    "    df = bin_columns(df, ['average_wind_speed'], n_bins=5)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_rate_of_change_features(df, features, second_order=False):\n",
    "    \"\"\"\n",
    "    Adds rate of change columns for specified features in the dataframe.\n",
    "    Assumes the dataframe is time sorted. If second_order is True, it also adds the second order rate of change.\n",
    "    \"\"\"\n",
    "    for feature in features:\n",
    "        rate_column_name = feature + '_rate_of_change'\n",
    "        df[rate_column_name] = df[feature].diff().fillna(0)  # Handle the first diff NaN if required\n",
    "        \n",
    "        if second_order:  # Check if second order difference is required\n",
    "            second_order_column_name = feature + '_rate_of_change_of_change'\n",
    "            df[second_order_column_name] = df[rate_column_name].diff().fillna(0)  # Second order difference\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_rate_of_change_features_to_df(df):\n",
    "    # Define the features for which to calculate rate of change\n",
    "    features_to_diff = [\n",
    "        't_1000hPa:K',\n",
    "        'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad:W',\n",
    "        'effective_cloud_cover:p', 'total_radiation:W'\n",
    "    ]\n",
    "\n",
    "    # Add rate of change features\n",
    "    return add_rate_of_change_features(df, features_to_diff, second_order=False)\n",
    "\n",
    "def add_est_obs_feature(df):\n",
    "    \"\"\"\n",
    "    Adds a column to the dataframe that indicates whether the data is estimated or observed.\n",
    "    \"\"\"\n",
    "    # Add the est_obs feature\n",
    "    if 'date_calc' not in df.columns:\n",
    "        # If 'date_calc' does not exist, create 'observed' column and set to 1\n",
    "        df['observed'] = 1\n",
    "        return df\n",
    "    else:\n",
    "        # If 'date_calc' exists, create a new column and set values to 0\n",
    "        df['observed'] = 0\n",
    "        return df.drop(columns=['date_calc'])\n",
    "    \n",
    "def remove_constant_regions(dataframe, column_name=\"pv_measurement\", threshold=72):\n",
    "    \"\"\"\n",
    "    Removes rows where the specified column has constant values for more than the given threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the specified column exists in the dataframe\n",
    "    if column_name not in dataframe.columns:\n",
    "        print(f\"Warning: '{column_name}' column not found in the dataframe. No rows removed.\")\n",
    "        return dataframe\n",
    "    \n",
    "    same_as_previous = dataframe[column_name].eq(dataframe[column_name].shift())\n",
    "    group_ids = (~same_as_previous).cumsum()\n",
    "    to_remove = group_ids[same_as_previous].value_counts() > threshold\n",
    "    group_ids_to_remove = to_remove[to_remove].index\n",
    "    \n",
    "    # Drop entire rows that match the conditions\n",
    "    return dataframe.drop(dataframe[group_ids.isin(group_ids_to_remove)].index)\n",
    "\n",
    "def add_lagged_features(df, features_with__lags, fill_value=None):\n",
    "    \"\"\"\n",
    "    Adds lagged columns for specified features in the dataframe with specific lag periods.\n",
    "    'features_with_specific_lags' is a dictionary with features as keys and specific lag as values.\n",
    "    'fill_value' is what to fill the NaNs with, after shifting.\n",
    "    \"\"\"\n",
    "    for feature, specific_lag in features_with__lags.items():\n",
    "        lag_column_name = f\"{feature}_lag_{specific_lag}\"\n",
    "        df[lag_column_name] = df[feature].shift(specific_lag).fillna(fill_value)\n",
    "    return df\n",
    "\n",
    "def add_lagged_features_to_df(df):\n",
    "    features_with_lags = {\n",
    "        'total_radiation:W': 1,\n",
    "        'total_radiation:W': -1,\n",
    "        'rad_diff:W': 1,\n",
    "        'rad_diff:W': -1,\n",
    "        'total_radiation_1h:J': 1,\n",
    "        'total_radiation_1h:J': -1\n",
    "    }\n",
    "\n",
    "    # Add lagged features for specific lags\n",
    "    return add_lagged_features(df, features_with_lags, fill_value=0)\n",
    "\n",
    "def handle_nan(df):\n",
    "    # Remove the rows where target is nan\n",
    "    try:\n",
    "        df = df[df['pv_measurement'].notna()]\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    # Set all remaining nans to 0\n",
    "    return df.fillna(0)\n",
    "\n",
    "# Aggregate the data to hourly with some aggregation methods for each column\n",
    "aggregation_methods = {\n",
    "    'date_forecast': 'first',\n",
    "    'diffuse_rad:W': 'sum',\n",
    "    'direct_rad:W': 'last',\n",
    "    'clear_sky_rad:W': 'sum',\n",
    "    'diffuse_rad_1h:J': 'last',\n",
    "    'direct_rad_1h:J': 'last',\n",
    "    'clear_sky_energy_1h:J': 'last',\n",
    "    'absolute_humidity_2m:gm3': 'mean',\n",
    "    'air_density_2m:kgm3': 'mean',\n",
    "    'ceiling_height_agl:m': 'max',\n",
    "    'cloud_base_agl:m': 'mean',\n",
    "    'dew_or_rime:idx': 'min',\n",
    "    'dew_point_2m:K': 'mean',\n",
    "    'effective_cloud_cover:p': 'sum',\n",
    "    'elevation:m': 'first',\n",
    "    'fresh_snow_12h:cm': 'max',\n",
    "    'fresh_snow_1h:cm': 'sum',\n",
    "    'fresh_snow_24h:cm': 'max',\n",
    "    'fresh_snow_3h:cm': 'max',\n",
    "    'fresh_snow_6h:cm': 'max',\n",
    "    'is_day:idx': 'max',\n",
    "    'is_in_shadow:idx': 'max',\n",
    "    'msl_pressure:hPa': 'mean',\n",
    "    'precip_5min:mm': 'sum',\n",
    "    'precip_type_5min:idx': 'sum',\n",
    "    'pressure_100m:hPa': 'mean',\n",
    "    'pressure_50m:hPa': 'mean',\n",
    "    'prob_rime:p': 'max',\n",
    "    'rain_water:kgm2': 'sum',\n",
    "    'relative_humidity_1000hPa:p': 'mean',\n",
    "    'sfc_pressure:hPa': 'mean',\n",
    "    'snow_density:kgm3': 'mean',\n",
    "    'snow_depth:cm': 'max',\n",
    "    'snow_drift:idx': 'max',\n",
    "    'snow_melt_10min:mm': 'sum',\n",
    "    'snow_water:kgm2': 'sum',\n",
    "    'sun_azimuth:d': 'first',\n",
    "    'sun_elevation:d': 'sum',\n",
    "    'super_cooled_liquid_water:kgm2': 'sum',\n",
    "    't_1000hPa:K': 'mean',\n",
    "    'total_cloud_cover:p': 'mean',\n",
    "    'visibility:m': 'mean',\n",
    "    'wind_speed_10m:ms': 'mean',\n",
    "    'wind_speed_u_10m:ms': 'mean',\n",
    "    'wind_speed_v_10m:ms': 'mean',\n",
    "    'wind_speed_w_1000hPa:ms': 'mean',\n",
    "    'cloud_base_agl:m': 'max',\n",
    "    'snow_density:kgm3': 'mean'\n",
    "}\n",
    "\n",
    "# Read in the data\n",
    "x_target_A = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "x_train_obs_A = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "x_train_est_A = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "x_test_est_A = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "x_target_B = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "x_train_obs_B = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "x_train_est_B = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "x_test_est_B = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "x_target_C = pd.read_parquet('./data/C/train_targets.parquet')\n",
    "x_train_obs_C = pd.read_parquet('./data/C/X_train_observed.parquet')\n",
    "x_train_est_C = pd.read_parquet('./data/C/X_train_estimated.parquet')\n",
    "x_test_est_C = pd.read_parquet('./data/C/X_test_estimated.parquet')\n",
    "\n",
    "# Rename time to date_forecast in target\n",
    "x_target_A.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "x_target_B.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "x_target_C.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "\n",
    "# Fix missing data for test set. Assumin NaN means 0 in these categories\n",
    "x_test_est_A['effective_cloud_cover:p'] = x_test_est_A['effective_cloud_cover:p'].fillna(0)\n",
    "x_test_est_B['effective_cloud_cover:p'] = x_test_est_B['effective_cloud_cover:p'].fillna(0)\n",
    "x_test_est_C['effective_cloud_cover:p'] = x_test_est_C['effective_cloud_cover:p'].fillna(0)\n",
    "\n",
    "x_test_est_A['total_cloud_cover:p'] = x_test_est_A['total_cloud_cover:p'].fillna(0)\n",
    "x_test_est_B['total_cloud_cover:p'] = x_test_est_B['total_cloud_cover:p'].fillna(0)\n",
    "x_test_est_C['total_cloud_cover:p'] = x_test_est_C['total_cloud_cover:p'].fillna(0)\n",
    "\n",
    "x_test_est_A['cloud_base_agl:m'] = x_test_est_A['cloud_base_agl:m'].fillna(0)\n",
    "x_test_est_B['cloud_base_agl:m'] = x_test_est_B['cloud_base_agl:m'].fillna(0)\n",
    "x_test_est_C['cloud_base_agl:m'] = x_test_est_C['cloud_base_agl:m'].fillna(0)\n",
    "\n",
    "x_test_est_A['ceiling_height_agl:m'] = x_test_est_A['ceiling_height_agl:m'].fillna(0)\n",
    "x_test_est_B['ceiling_height_agl:m'] = x_test_est_B['ceiling_height_agl:m'].fillna(0)\n",
    "x_test_est_C['ceiling_height_agl:m'] = x_test_est_C['ceiling_height_agl:m'].fillna(0)\n",
    "\n",
    "x_test_est_A['snow_density:kgm3'] = x_test_est_A['snow_density:kgm3'].fillna(0)\n",
    "x_test_est_B['snow_density:kgm3'] = x_test_est_B['snow_density:kgm3'].fillna(0)\n",
    "x_test_est_C['snow_density:kgm3'] = x_test_est_C['snow_density:kgm3'].fillna(0)\n",
    "\n",
    "x_test_est_A['snow_drift:idx'] = x_test_est_A['snow_drift:idx'].fillna(0)\n",
    "x_test_est_B['snow_drift:idx'] = x_test_est_B['snow_drift:idx'].fillna(0)\n",
    "x_test_est_C['snow_drift:idx'] = x_test_est_C['snow_drift:idx'].fillna(0)\n",
    "\n",
    "# Resample\n",
    "x_train_obs_A_resampled = x_train_obs_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_A_resampled = x_train_est_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_A_resampled = x_test_est_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "x_train_obs_B_resampled = x_train_obs_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_B_resampled = x_train_est_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_B_resampled = x_test_est_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "x_train_obs_C_resampled = x_train_obs_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_C_resampled = x_train_est_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_C_resampled = x_test_est_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "# Merge\n",
    "split_value = x_train_est_A['date_forecast'].iloc[0]\n",
    "split_index = x_target_A[x_target_A['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_A = x_target_A.iloc[:split_index]\n",
    "x_target_est_A = x_target_A.iloc[split_index:]\n",
    "\n",
    "obs_A = x_train_obs_A_resampled.merge(x_target_obs_A, left_index=True, right_on='date_forecast')\n",
    "est_A = x_train_est_A_resampled.merge(x_target_est_A, left_index=True, right_on='date_forecast')\n",
    "\n",
    "split_value = x_train_est_B['date_forecast'].iloc[0]\n",
    "split_index = x_target_B[x_target_B['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_B = x_target_B.iloc[:split_index]\n",
    "x_target_est_B = x_target_B.iloc[split_index:]\n",
    "\n",
    "obs_B = x_train_obs_B_resampled.merge(x_target_obs_B, left_index=True, right_on='date_forecast')\n",
    "est_B = x_train_est_B_resampled.merge(x_target_est_B, left_index=True, right_on='date_forecast')\n",
    "\n",
    "split_value = x_train_est_C['date_forecast'].iloc[0]\n",
    "split_index = x_target_C[x_target_C['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_C = x_target_C.iloc[:split_index]\n",
    "x_target_est_C = x_target_C.iloc[split_index:]\n",
    "\n",
    "obs_C = x_train_obs_C_resampled.merge(x_target_obs_C, left_index=True, right_on='date_forecast')\n",
    "est_C = x_train_est_C_resampled.merge(x_target_est_C, left_index=True, right_on='date_forecast')\n",
    "\n",
    "# Keep date_forecast in test dfs\n",
    "test_A = x_test_est_A_resampled\n",
    "test_B = x_test_est_B_resampled\n",
    "test_C = x_test_est_C_resampled\n",
    "\n",
    "# Drop all the NaNs\n",
    "test_A = test_A.dropna()\n",
    "test_B = test_B.dropna()\n",
    "test_C = test_C.dropna()\n",
    "\n",
    "def preprocessing(df):\n",
    "    df = add_experimental_features(df.copy())\n",
    "    df = add_date_features(df.copy())\n",
    "    df = add_binned_features(df.copy())\n",
    "    df = add_rate_of_change_features_to_df(df.copy())\n",
    "    df = add_est_obs_feature(df.copy())\n",
    "    df = remove_constant_regions(df.copy())\n",
    "    df = add_lagged_features_to_df(df.copy())\n",
    "    df = handle_nan(df.copy())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Preprocess\n",
    "obs_A = preprocessing(obs_A.copy())\n",
    "est_A = preprocessing(est_A.copy())\n",
    "test_A = preprocessing(test_A.copy())\n",
    "\n",
    "obs_B = preprocessing(obs_B.copy())\n",
    "est_B = preprocessing(est_B.copy())\n",
    "test_B = preprocessing(test_B.copy())\n",
    "\n",
    "obs_C = preprocessing(obs_C.copy())\n",
    "est_C = preprocessing(est_C.copy())\n",
    "test_C = preprocessing(test_C.copy())\n",
    "\n",
    "# Random seeds used for reproducibility\n",
    "# 32 weights: 0.3, 0.3, 0.4\n",
    "# 24 weights: 0.3, 0.3, 0.4\n",
    "# 33 (without winter months 1 and 12) weights: 0.2, 0.4, 0.4\n",
    "# 11 (without winter months 1, 2 and 11, 12) weights: 0.25, 0.35, 0.4\n",
    "# 5 weights: 0.4, 0.3, 0.3\n",
    "\n",
    "# Best score is the mean prediction of all the 5 seeds mentioned above. The first weight is xgboost, the second is catboost, and the third is autogluon.\n",
    "\n",
    "# Set the random seed\n",
    "np.random.seed(24)\n",
    "\n",
    "# Concatinate\n",
    "A = pd.concat([obs_A, est_A])\n",
    "B = pd.concat([obs_B, est_B])\n",
    "C = pd.concat([obs_C, est_C])\n",
    "\n",
    "# Remove characters unparseable for CatBoost \n",
    "A.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in A.columns]\n",
    "B.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in B.columns]\n",
    "C.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in C.columns]\n",
    "\n",
    "test_A.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_A.columns]\n",
    "test_B.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_B.columns]\n",
    "test_C.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_C.columns]\n",
    "\n",
    "# Getting validation data from summer months, because the test set is from summer months. We experimentet with excluding winter months\n",
    "# from the training data here.\n",
    "\n",
    "# Step 1: Filter A to include only months from March to October\n",
    "A = A[A['date_forecast'].dt.month.isin([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])]\n",
    "\n",
    "# Step 2: Identify unique days within May, June, and July\n",
    "summer_months = A[A['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "\n",
    "# Step 3: Sample these days for val_A\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_A = A[A['date_forecast'].dt.date.isin(sampled_days)]\n",
    "\n",
    "# Step 4: Define train_A as the remaining data\n",
    "train_A = A[~A['date_forecast'].dt.date.isin(sampled_days)]\n",
    "\n",
    "# Prepare your features and target variables\n",
    "X_train_A = train_A.drop(columns='pv_measurement')\n",
    "y_train_A = train_A['pv_measurement']\n",
    "X_val_A = val_A.drop(columns='pv_measurement')\n",
    "y_val_A = val_A['pv_measurement']\n",
    "\n",
    "# Repeat for B and C\n",
    "B = B[B['date_forecast'].dt.month.isin([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])]\n",
    "summer_months = B[B['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_B = B[B['date_forecast'].dt.date.isin(sampled_days)]\n",
    "train_B = B[~B['date_forecast'].dt.date.isin(sampled_days)]\n",
    "X_train_B = train_B.drop(columns='pv_measurement')\n",
    "y_train_B = train_B['pv_measurement']\n",
    "X_val_B = val_B.drop(columns='pv_measurement')\n",
    "y_val_B = val_B['pv_measurement']\n",
    "\n",
    "C = C[C['date_forecast'].dt.month.isin([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])]\n",
    "summer_months = C[C['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_C = C[C['date_forecast'].dt.date.isin(sampled_days)]\n",
    "train_C = C[~C['date_forecast'].dt.date.isin(sampled_days)]\n",
    "X_train_C = train_C.drop(columns='pv_measurement')\n",
    "y_train_C = train_C['pv_measurement']\n",
    "X_val_C = val_C.drop(columns='pv_measurement')\n",
    "y_val_C = val_C['pv_measurement']\n",
    "\n",
    "# Drop date_forecast\n",
    "train_A = train_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "train_B = train_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "train_C = train_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_A = val_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_B = val_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_C = val_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_A = X_train_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_B = X_train_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_C = X_train_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_A = X_val_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_B = X_val_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_C = X_val_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "test_A = test_A.drop(columns=['date_forecast'])\n",
    "test_B = test_B.drop(columns=['date_forecast'])\n",
    "test_C = test_C.drop(columns=['date_forecast'])\n",
    "\n",
    "train_auto_A = TabularDataset(train_A)\n",
    "val_auto_A = TabularDataset(val_A)\n",
    "\n",
    "train_auto_B = TabularDataset(train_B)\n",
    "val_auto_B = TabularDataset(val_B)\n",
    "\n",
    "train_auto_C = TabularDataset(train_C)\n",
    "val_auto_C = TabularDataset(val_C)\n",
    "\n",
    "auto_label = 'pv_measurement'\n",
    "\n",
    "# Set the parameters for the XGBoost models\n",
    "params_xgb_A = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.4, \n",
    "    'learning_rate': 0.012, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 0.8, \n",
    "    'reg_lambda': 0.8, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1,\n",
    "    'num_parallel_tree': 2\n",
    "}\n",
    "\n",
    "params_xgb_B = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.8, \n",
    "    'learning_rate': 0.008, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 1, \n",
    "    'reg_lambda': 3, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "params_xgb_C = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.8, \n",
    "    'learning_rate': 0.008, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 1, \n",
    "    'reg_lambda': 3, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "xgb_A = xgb.XGBRegressor(**params_xgb_A)\n",
    "xgb_B = xgb.XGBRegressor(**params_xgb_B)\n",
    "xgb_C = xgb.XGBRegressor(**params_xgb_C)\n",
    "\n",
    "cat_A = CatBoostRegressor(\n",
    "    iterations=5000,         # The number of trees to build\n",
    "    #learning_rate=0.09,     # The learning rate\n",
    "    #depth=10,               # Depth of the tree\n",
    "    loss_function='MAE',     # Loss function to be optimized. RMSE is common for regression.\n",
    "    eval_metric='MAE',       # Evaluation metric for the validation set\n",
    "    #random_seed=42,         # Seed for reproducibility\n",
    "    #verbose=100             # Frequency of logging the training process\n",
    ")\n",
    "\n",
    "cat_B = CatBoostRegressor(\n",
    "    iterations=5000,\n",
    "    #learning_rate=0.09,\n",
    "    #depth=10,\n",
    "    loss_function='MAE',\n",
    "    eval_metric='MAE',\n",
    "    #random_seed=42,\n",
    "    #verbose=100\n",
    ")\n",
    "\n",
    "cat_C = CatBoostRegressor(\n",
    "    iterations=5000,\n",
    "    #learning_rate=0.09,\n",
    "    #depth=10,\n",
    "    loss_function='MAE',\n",
    "    eval_metric='MAE',\n",
    "    #random_seed=42,\n",
    "    #verbose=100\n",
    ")\n",
    "\n",
    "# Prepare data for the XGBoost models. We got them to work the best when having fewer columns\n",
    "xgb_columns = [\n",
    "    'total_radiationW',\n",
    "    'snow_accumulation',\n",
    "    'super_cooled_liquid_waterkgm2',\n",
    "    'average_wind_speed',\n",
    "    'sun_elevationd',\n",
    "    'sun_azimuthd',\n",
    "    'clear_sky_radW',\n",
    "    'month',\n",
    "    't_1000hPaC',\n",
    "    'msl_pressurehPa_scaled',\n",
    "    'rain_waterkgm2',\n",
    "    'cloud_base_aglm',\n",
    "    'effective_cloud_coverp',\n",
    "    'dew_or_rimeidx'\n",
    "]\n",
    "\n",
    "X_train_xgb_A = train_A[xgb_columns]\n",
    "y_train_xgb_A = train_A['pv_measurement']\n",
    "X_test_xgb_A = test_A[xgb_columns]\n",
    "\n",
    "X_train_xgb_B = train_B[xgb_columns]\n",
    "y_train_xgb_B = train_B['pv_measurement']\n",
    "X_test_xgb_B = test_B[xgb_columns]\n",
    "\n",
    "X_train_xgb_C = train_C[xgb_columns]\n",
    "y_train_xgb_C = train_C['pv_measurement']\n",
    "X_test_xgb_C = test_C[xgb_columns]\n",
    "\n",
    "# Train the XGBoost models\n",
    "xgb_A.fit(\n",
    "    X=X_train_xgb_A, y=y_train_xgb_A,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "xgb_B.fit(\n",
    "    X=X_train_xgb_B, y=y_train_xgb_B,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "xgb_C.fit(\n",
    "    X=X_train_xgb_C, y=y_train_xgb_C,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Train the CatBoost models\n",
    "cat_A.fit(\n",
    "    X_train_A, y_train_A,\n",
    "    eval_set=(X_val_A, y_val_A),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "cat_B.fit(\n",
    "    X_train_B, y_train_B,\n",
    "    eval_set=(X_val_B, y_val_B),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "cat_C.fit(\n",
    "    X_train_C, y_train_C,\n",
    "    eval_set=(X_val_C, y_val_C),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "auto_A = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_A, \n",
    "                                                                                   presets='medium_quality', \n",
    "                                                                                   tuning_data=val_auto_A, \n",
    "                                                                                   use_bag_holdout=True, \n",
    "                                                                                   ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "auto_B = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_B,\n",
    "                                                                                      presets='medium_quality',\n",
    "                                                                                      tuning_data=val_auto_B,\n",
    "                                                                                      use_bag_holdout=True,\n",
    "                                                                                      ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "auto_C = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_C,\n",
    "                                                                                        presets='medium_quality',\n",
    "                                                                                        tuning_data=val_auto_C,\n",
    "                                                                                        use_bag_holdout=True,\n",
    "                                                                                        ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "xgb_weight = 0.4\n",
    "cat_weight = 0.3\n",
    "auto_weight = 0.3\n",
    "\n",
    "pred_xgb_A = xgb_A.predict(X_test_xgb_C)\n",
    "pred_xgb_B = xgb_B.predict(X_test_xgb_C)\n",
    "pred_xgb_C = xgb_C.predict(X_test_xgb_C)\n",
    "\n",
    "pred_auto_A = auto_A.predict(test_A)\n",
    "pred_auto_B = auto_B.predict(test_B)\n",
    "pred_auto_C = auto_C.predict(test_C)\n",
    "\n",
    "pred_cat_A = cat_A.predict(test_A)\n",
    "pred_cat_B = cat_B.predict(test_B)\n",
    "pred_cat_C = cat_C.predict(test_C)\n",
    "\n",
    "# Ensemble that seemed the best after some experimentation\n",
    "pred_A = (pred_xgb_A*xgb_weight + pred_cat_A*cat_weight + pred_auto_A*auto_weight)\n",
    "pred_B = (pred_xgb_B*xgb_weight + pred_cat_B*cat_weight + pred_auto_B*auto_weight)\n",
    "pred_C = (pred_xgb_C*xgb_weight + pred_cat_C*cat_weight + pred_auto_C*auto_weight)\n",
    "\n",
    "pred_A = np.clip(pred_A, 0, None)\n",
    "pred_B = np.clip(pred_B, 0, None)\n",
    "pred_C = np.clip(pred_C, 0, None)\n",
    "\n",
    "# Concatenate predictions\n",
    "predictions = np.concatenate([pred_A, pred_B, pred_C])\n",
    "\n",
    "# Save predictions\n",
    "predictions_2 = predictions\n",
    "\n",
    "# Create an id array\n",
    "ids = np.arange(0, len(predictions_2))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'prediction': predictions_2\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('predictions_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence 3\n",
    "\n",
    "%reset -f\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostRegressor\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def add_experimental_features(df):\n",
    "    \"\"\"\n",
    "    Experimental feature engineering.\n",
    "    \"\"\"\n",
    "\n",
    "    # Radiation Features\n",
    "    df['total_radiation:W'] = df['direct_rad:W'] + df['diffuse_rad:W']\n",
    "    df['total_radiation_1h:J'] = df['direct_rad_1h:J'] + df['diffuse_rad_1h:J']\n",
    "    df['rad_diff:W'] = df['direct_rad:W'] - df['diffuse_rad:W']\n",
    "    df['rad_diff_1h:J'] = df['direct_rad_1h:J'] - df['diffuse_rad_1h:J']\n",
    "    df['diffuse_direct_ratio'] = df['diffuse_rad:W'] / df['direct_rad:W']\n",
    "\n",
    "    # Temperature and Pressure Features\n",
    "    df['temp_dewpoint_diff'] = df['t_1000hPa:K'] - df['dew_point_2m:K']\n",
    "    df['pressure_gradient'] = df['pressure_100m:hPa'] - df['pressure_50m:hPa']\n",
    "    df['t_1000hPa:C'] = df['t_1000hPa:K'] - 273.15\n",
    "    df['dew_point_2m:C'] = df['dew_point_2m:K'] - 273.15\n",
    "    df['msl_pressure:hPa_scaled'] = MinMaxScaler().fit_transform(df['msl_pressure:hPa'].values.reshape(-1, 1))\n",
    "    df['sfc_pressure:hPa_scaled'] = MinMaxScaler().fit_transform(df['sfc_pressure:hPa'].values.reshape(-1, 1))\n",
    "\n",
    "    # Wind Features\n",
    "    df['wind_vector_magnitude'] = (df['wind_speed_u_10m:ms']**2 + df['wind_speed_v_10m:ms']**2 + df['wind_speed_w_1000hPa:ms']**2)**0.5\n",
    "    df['average_wind_speed'] = (df['wind_speed_10m:ms'] + df['wind_speed_u_10m:ms']) / 2\n",
    "\n",
    "    # Cloud and Snow Features\n",
    "    df['cloud_humidity_product'] = df['total_cloud_cover:p'] * df['absolute_humidity_2m:gm3']\n",
    "    df['snow_accumulation'] = df[['fresh_snow_24h:cm', 'fresh_snow_12h:cm', 'fresh_snow_6h:cm', 'fresh_snow_3h:cm', 'fresh_snow_1h:cm']].sum(axis=1)\n",
    "\n",
    "    # Interaction between radiation and cloud cover\n",
    "    df['radiation_cloud_interaction'] = df['direct_rad:W'] * df['effective_cloud_cover:p']\n",
    "\n",
    "    # Interaction between temperature and radiation (considering that high temperature may reduce efficiency)\n",
    "    df['temp_rad_interaction'] = df['t_1000hPa:K'] * df['total_radiation:W']\n",
    "\n",
    "    # Interaction between wind cooling effect and temperature\n",
    "    df['wind_temp_interaction'] = df['average_wind_speed'] * df['t_1000hPa:K']\n",
    "\n",
    "    # Interaction between humidity and temperature\n",
    "    df['humidity_temp_interaction'] = df['absolute_humidity_2m:gm3'] * df['t_1000hPa:K']\n",
    "\n",
    "    # Interaction between humidity and radiation\n",
    "    df['sun_elevation_direct_rad_interaction'] = df['sun_elevation:d'] * df['direct_rad:W']\n",
    "\n",
    "    # Precipitation Features\n",
    "    df['precip'] = df['precip_5min:mm']*df['precip_type_5min:idx']\n",
    "\n",
    "    # Safeguard in case of inf values\n",
    "    df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_date_features(df):\n",
    "    \"\"\"\n",
    "    Adds 'month', 'year', 'hour' and 'day' columns to the dataframe based on the 'date_forecast' column.\n",
    "    Also adds 'hour_sin' and 'hour_cos' columns for the hour of the day.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if 'date_forecast' exists in the dataframe\n",
    "    if 'date_forecast' in df.columns:\n",
    "        # Convert the 'date_forecast' column to datetime format\n",
    "        df['date_forecast'] = pd.to_datetime(df['date_forecast'])\n",
    "        \n",
    "        # Extract month, year, hour and day\n",
    "        df['month'] = df['date_forecast'].dt.month\n",
    "        df['year'] = df['date_forecast'].dt.year\n",
    "        df['hour'] = df['date_forecast'].dt.hour\n",
    "        df['day'] = df['date_forecast'].dt.day\n",
    "        df['hour_sin'] = np.sin(2*np.pi*df['hour']/24)\n",
    "        df['hour_cos'] = np.cos(2*np.pi*df['hour']/24)\n",
    "\n",
    "    else:\n",
    "        print(\"Warning: 'date_forecast' column not found in the dataframe. No date features added.\")\n",
    "        return df  # Keep the 'date_forecast' column in the dataframe\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Adding discretized features for the continuous variables to help tree-based models\n",
    "\n",
    "def bin_columns(dataframe, columns_to_bin, n_bins=5):\n",
    "    \"\"\"\n",
    "    Bins the specified columns of the dataframe into equal-sized bins.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframe: pd.DataFrame\n",
    "    - columns_to_bin: list of strings, the names of the columns to bin\n",
    "    - n_bins: int or dict, the number of bins for each column (if int, use the same number for all columns;\n",
    "              if dict, specify individual numbers with column names as keys)\n",
    "    \n",
    "    Returns:\n",
    "    - binned_dataframe: pd.DataFrame, the dataframe with the specified columns binned\n",
    "    \"\"\"\n",
    "    binned_dataframe = dataframe.copy()\n",
    "    \n",
    "    for column in columns_to_bin:\n",
    "        # Determine the number of bins for this column\n",
    "        bins = n_bins if isinstance(n_bins, int) else n_bins.get(column, 5)\n",
    "        \n",
    "        # Create quantile-based bins\n",
    "        binned_dataframe[f'binned_{column}'] = pd.qcut(\n",
    "            binned_dataframe[column],\n",
    "            q=bins,\n",
    "            labels=False,\n",
    "            duplicates='drop'\n",
    "        )\n",
    "        \n",
    "    return binned_dataframe\n",
    "\n",
    "def add_binned_features(df):\n",
    "    columns_to_bin = [\n",
    "        'super_cooled_liquid_water:kgm2',\n",
    "        'ceiling_height_agl:m',\n",
    "        'cloud_base_agl:m'\n",
    "    ]\n",
    "\n",
    "    # Bin the columns\n",
    "    # df = bin_columns(df, columns_to_bin)\n",
    "    df = bin_columns(df, ['effective_cloud_cover:p'], n_bins=2)\n",
    "    df = bin_columns(df, ['ceiling_height_agl:m'], n_bins=3)\n",
    "    df = bin_columns(df, ['average_wind_speed'], n_bins=5)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_rate_of_change_features(df, features, second_order=False):\n",
    "    \"\"\"\n",
    "    Adds rate of change columns for specified features in the dataframe.\n",
    "    Assumes the dataframe is time sorted. If second_order is True, it also adds the second order rate of change.\n",
    "    \"\"\"\n",
    "    for feature in features:\n",
    "        rate_column_name = feature + '_rate_of_change'\n",
    "        df[rate_column_name] = df[feature].diff().fillna(0)  # Handle the first diff NaN if required\n",
    "        \n",
    "        if second_order:  # Check if second order difference is required\n",
    "            second_order_column_name = feature + '_rate_of_change_of_change'\n",
    "            df[second_order_column_name] = df[rate_column_name].diff().fillna(0)  # Second order difference\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_rate_of_change_features_to_df(df):\n",
    "    # Define the features for which to calculate rate of change\n",
    "    features_to_diff = [\n",
    "        't_1000hPa:K',\n",
    "        'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad:W',\n",
    "        'effective_cloud_cover:p', 'total_radiation:W'\n",
    "    ]\n",
    "\n",
    "    # Add rate of change features\n",
    "    return add_rate_of_change_features(df, features_to_diff, second_order=False)\n",
    "\n",
    "def add_est_obs_feature(df):\n",
    "    \"\"\"\n",
    "    Adds a column to the dataframe that indicates whether the data is estimated or observed.\n",
    "    \"\"\"\n",
    "    # Add the est_obs feature\n",
    "    if 'date_calc' not in df.columns:\n",
    "        # If 'date_calc' does not exist, create 'observed' column and set to 1\n",
    "        df['observed'] = 1\n",
    "        return df\n",
    "    else:\n",
    "        # If 'date_calc' exists, create a new column and set values to 0\n",
    "        df['observed'] = 0\n",
    "        return df.drop(columns=['date_calc'])\n",
    "    \n",
    "def remove_constant_regions(dataframe, column_name=\"pv_measurement\", threshold=72):\n",
    "    \"\"\"\n",
    "    Removes rows where the specified column has constant values for more than the given threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the specified column exists in the dataframe\n",
    "    if column_name not in dataframe.columns:\n",
    "        print(f\"Warning: '{column_name}' column not found in the dataframe. No rows removed.\")\n",
    "        return dataframe\n",
    "    \n",
    "    same_as_previous = dataframe[column_name].eq(dataframe[column_name].shift())\n",
    "    group_ids = (~same_as_previous).cumsum()\n",
    "    to_remove = group_ids[same_as_previous].value_counts() > threshold\n",
    "    group_ids_to_remove = to_remove[to_remove].index\n",
    "    \n",
    "    # Drop entire rows that match the conditions\n",
    "    return dataframe.drop(dataframe[group_ids.isin(group_ids_to_remove)].index)\n",
    "\n",
    "def add_lagged_features(df, features_with__lags, fill_value=None):\n",
    "    \"\"\"\n",
    "    Adds lagged columns for specified features in the dataframe with specific lag periods.\n",
    "    'features_with_specific_lags' is a dictionary with features as keys and specific lag as values.\n",
    "    'fill_value' is what to fill the NaNs with, after shifting.\n",
    "    \"\"\"\n",
    "    for feature, specific_lag in features_with__lags.items():\n",
    "        lag_column_name = f\"{feature}_lag_{specific_lag}\"\n",
    "        df[lag_column_name] = df[feature].shift(specific_lag).fillna(fill_value)\n",
    "    return df\n",
    "\n",
    "def add_lagged_features_to_df(df):\n",
    "    features_with_lags = {\n",
    "        'total_radiation:W': 1,\n",
    "        'total_radiation:W': -1,\n",
    "        'rad_diff:W': 1,\n",
    "        'rad_diff:W': -1,\n",
    "        'total_radiation_1h:J': 1,\n",
    "        'total_radiation_1h:J': -1\n",
    "    }\n",
    "\n",
    "    # Add lagged features for specific lags\n",
    "    return add_lagged_features(df, features_with_lags, fill_value=0)\n",
    "\n",
    "def handle_nan(df):\n",
    "    # Remove the rows where target is nan\n",
    "    try:\n",
    "        df = df[df['pv_measurement'].notna()]\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    # Set all remaining nans to 0\n",
    "    return df.fillna(0)\n",
    "\n",
    "# Aggregate the data to hourly with some aggregation methods for each column\n",
    "aggregation_methods = {\n",
    "    'date_forecast': 'first',\n",
    "    'diffuse_rad:W': 'sum',\n",
    "    'direct_rad:W': 'last',\n",
    "    'clear_sky_rad:W': 'sum',\n",
    "    'diffuse_rad_1h:J': 'last',\n",
    "    'direct_rad_1h:J': 'last',\n",
    "    'clear_sky_energy_1h:J': 'last',\n",
    "    'absolute_humidity_2m:gm3': 'mean',\n",
    "    'air_density_2m:kgm3': 'mean',\n",
    "    'ceiling_height_agl:m': 'max',\n",
    "    'cloud_base_agl:m': 'mean',\n",
    "    'dew_or_rime:idx': 'min',\n",
    "    'dew_point_2m:K': 'mean',\n",
    "    'effective_cloud_cover:p': 'sum',\n",
    "    'elevation:m': 'first',\n",
    "    'fresh_snow_12h:cm': 'max',\n",
    "    'fresh_snow_1h:cm': 'sum',\n",
    "    'fresh_snow_24h:cm': 'max',\n",
    "    'fresh_snow_3h:cm': 'max',\n",
    "    'fresh_snow_6h:cm': 'max',\n",
    "    'is_day:idx': 'max',\n",
    "    'is_in_shadow:idx': 'max',\n",
    "    'msl_pressure:hPa': 'mean',\n",
    "    'precip_5min:mm': 'sum',\n",
    "    'precip_type_5min:idx': 'sum',\n",
    "    'pressure_100m:hPa': 'mean',\n",
    "    'pressure_50m:hPa': 'mean',\n",
    "    'prob_rime:p': 'max',\n",
    "    'rain_water:kgm2': 'sum',\n",
    "    'relative_humidity_1000hPa:p': 'mean',\n",
    "    'sfc_pressure:hPa': 'mean',\n",
    "    'snow_density:kgm3': 'mean',\n",
    "    'snow_depth:cm': 'max',\n",
    "    'snow_drift:idx': 'max',\n",
    "    'snow_melt_10min:mm': 'sum',\n",
    "    'snow_water:kgm2': 'sum',\n",
    "    'sun_azimuth:d': 'first',\n",
    "    'sun_elevation:d': 'sum',\n",
    "    'super_cooled_liquid_water:kgm2': 'sum',\n",
    "    't_1000hPa:K': 'mean',\n",
    "    'total_cloud_cover:p': 'mean',\n",
    "    'visibility:m': 'mean',\n",
    "    'wind_speed_10m:ms': 'mean',\n",
    "    'wind_speed_u_10m:ms': 'mean',\n",
    "    'wind_speed_v_10m:ms': 'mean',\n",
    "    'wind_speed_w_1000hPa:ms': 'mean',\n",
    "    'cloud_base_agl:m': 'max',\n",
    "    'snow_density:kgm3': 'mean'\n",
    "}\n",
    "\n",
    "# Read in the data\n",
    "x_target_A = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "x_train_obs_A = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "x_train_est_A = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "x_test_est_A = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "x_target_B = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "x_train_obs_B = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "x_train_est_B = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "x_test_est_B = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "x_target_C = pd.read_parquet('./data/C/train_targets.parquet')\n",
    "x_train_obs_C = pd.read_parquet('./data/C/X_train_observed.parquet')\n",
    "x_train_est_C = pd.read_parquet('./data/C/X_train_estimated.parquet')\n",
    "x_test_est_C = pd.read_parquet('./data/C/X_test_estimated.parquet')\n",
    "\n",
    "# Rename time to date_forecast in target\n",
    "x_target_A.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "x_target_B.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "x_target_C.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "\n",
    "# Fix missing data for test set. Assumin NaN means 0 in these categories\n",
    "x_test_est_A['effective_cloud_cover:p'] = x_test_est_A['effective_cloud_cover:p'].fillna(0)\n",
    "x_test_est_B['effective_cloud_cover:p'] = x_test_est_B['effective_cloud_cover:p'].fillna(0)\n",
    "x_test_est_C['effective_cloud_cover:p'] = x_test_est_C['effective_cloud_cover:p'].fillna(0)\n",
    "\n",
    "x_test_est_A['total_cloud_cover:p'] = x_test_est_A['total_cloud_cover:p'].fillna(0)\n",
    "x_test_est_B['total_cloud_cover:p'] = x_test_est_B['total_cloud_cover:p'].fillna(0)\n",
    "x_test_est_C['total_cloud_cover:p'] = x_test_est_C['total_cloud_cover:p'].fillna(0)\n",
    "\n",
    "x_test_est_A['cloud_base_agl:m'] = x_test_est_A['cloud_base_agl:m'].fillna(0)\n",
    "x_test_est_B['cloud_base_agl:m'] = x_test_est_B['cloud_base_agl:m'].fillna(0)\n",
    "x_test_est_C['cloud_base_agl:m'] = x_test_est_C['cloud_base_agl:m'].fillna(0)\n",
    "\n",
    "x_test_est_A['ceiling_height_agl:m'] = x_test_est_A['ceiling_height_agl:m'].fillna(0)\n",
    "x_test_est_B['ceiling_height_agl:m'] = x_test_est_B['ceiling_height_agl:m'].fillna(0)\n",
    "x_test_est_C['ceiling_height_agl:m'] = x_test_est_C['ceiling_height_agl:m'].fillna(0)\n",
    "\n",
    "x_test_est_A['snow_density:kgm3'] = x_test_est_A['snow_density:kgm3'].fillna(0)\n",
    "x_test_est_B['snow_density:kgm3'] = x_test_est_B['snow_density:kgm3'].fillna(0)\n",
    "x_test_est_C['snow_density:kgm3'] = x_test_est_C['snow_density:kgm3'].fillna(0)\n",
    "\n",
    "x_test_est_A['snow_drift:idx'] = x_test_est_A['snow_drift:idx'].fillna(0)\n",
    "x_test_est_B['snow_drift:idx'] = x_test_est_B['snow_drift:idx'].fillna(0)\n",
    "x_test_est_C['snow_drift:idx'] = x_test_est_C['snow_drift:idx'].fillna(0)\n",
    "\n",
    "# Resample\n",
    "x_train_obs_A_resampled = x_train_obs_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_A_resampled = x_train_est_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_A_resampled = x_test_est_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "x_train_obs_B_resampled = x_train_obs_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_B_resampled = x_train_est_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_B_resampled = x_test_est_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "x_train_obs_C_resampled = x_train_obs_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_C_resampled = x_train_est_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_C_resampled = x_test_est_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "# Merge\n",
    "split_value = x_train_est_A['date_forecast'].iloc[0]\n",
    "split_index = x_target_A[x_target_A['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_A = x_target_A.iloc[:split_index]\n",
    "x_target_est_A = x_target_A.iloc[split_index:]\n",
    "\n",
    "obs_A = x_train_obs_A_resampled.merge(x_target_obs_A, left_index=True, right_on='date_forecast')\n",
    "est_A = x_train_est_A_resampled.merge(x_target_est_A, left_index=True, right_on='date_forecast')\n",
    "\n",
    "split_value = x_train_est_B['date_forecast'].iloc[0]\n",
    "split_index = x_target_B[x_target_B['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_B = x_target_B.iloc[:split_index]\n",
    "x_target_est_B = x_target_B.iloc[split_index:]\n",
    "\n",
    "obs_B = x_train_obs_B_resampled.merge(x_target_obs_B, left_index=True, right_on='date_forecast')\n",
    "est_B = x_train_est_B_resampled.merge(x_target_est_B, left_index=True, right_on='date_forecast')\n",
    "\n",
    "split_value = x_train_est_C['date_forecast'].iloc[0]\n",
    "split_index = x_target_C[x_target_C['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_C = x_target_C.iloc[:split_index]\n",
    "x_target_est_C = x_target_C.iloc[split_index:]\n",
    "\n",
    "obs_C = x_train_obs_C_resampled.merge(x_target_obs_C, left_index=True, right_on='date_forecast')\n",
    "est_C = x_train_est_C_resampled.merge(x_target_est_C, left_index=True, right_on='date_forecast')\n",
    "\n",
    "# Keep date_forecast in test dfs\n",
    "test_A = x_test_est_A_resampled\n",
    "test_B = x_test_est_B_resampled\n",
    "test_C = x_test_est_C_resampled\n",
    "\n",
    "# Drop all the NaNs\n",
    "test_A = test_A.dropna()\n",
    "test_B = test_B.dropna()\n",
    "test_C = test_C.dropna()\n",
    "\n",
    "def preprocessing(df):\n",
    "    df = add_experimental_features(df.copy())\n",
    "    df = add_date_features(df.copy())\n",
    "    df = add_binned_features(df.copy())\n",
    "    df = add_rate_of_change_features_to_df(df.copy())\n",
    "    df = add_est_obs_feature(df.copy())\n",
    "    df = remove_constant_regions(df.copy())\n",
    "    df = add_lagged_features_to_df(df.copy())\n",
    "    df = handle_nan(df.copy())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Preprocess\n",
    "obs_A = preprocessing(obs_A.copy())\n",
    "est_A = preprocessing(est_A.copy())\n",
    "test_A = preprocessing(test_A.copy())\n",
    "\n",
    "obs_B = preprocessing(obs_B.copy())\n",
    "est_B = preprocessing(est_B.copy())\n",
    "test_B = preprocessing(test_B.copy())\n",
    "\n",
    "obs_C = preprocessing(obs_C.copy())\n",
    "est_C = preprocessing(est_C.copy())\n",
    "test_C = preprocessing(test_C.copy())\n",
    "\n",
    "# Random seeds used for reproducibility\n",
    "# 32 weights: 0.3, 0.3, 0.4\n",
    "# 24 weights: 0.3, 0.3, 0.4\n",
    "# 33 (without winter months 1 and 12) weights: 0.2, 0.4, 0.4\n",
    "# 11 (without winter months 1, 2 and 11, 12) weights: 0.25, 0.35, 0.4\n",
    "# 5 weights: 0.4, 0.3, 0.3\n",
    "\n",
    "# Best score is the mean prediction of all the 5 seeds mentioned above. The first weight is xgboost, the second is catboost, and the third is autogluon.\n",
    "\n",
    "# Set the random seed\n",
    "np.random.seed(33)\n",
    "\n",
    "# Concatinate\n",
    "A = pd.concat([obs_A, est_A])\n",
    "B = pd.concat([obs_B, est_B])\n",
    "C = pd.concat([obs_C, est_C])\n",
    "\n",
    "# Remove characters unparseable for CatBoost \n",
    "A.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in A.columns]\n",
    "B.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in B.columns]\n",
    "C.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in C.columns]\n",
    "\n",
    "test_A.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_A.columns]\n",
    "test_B.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_B.columns]\n",
    "test_C.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_C.columns]\n",
    "\n",
    "# Getting validation data from summer months, because the test set is from summer months. We experimentet with excluding winter months\n",
    "# from the training data here.\n",
    "\n",
    "# Step 1: Filter A to include only months from March to October\n",
    "A = A[A['date_forecast'].dt.month.isin([2, 3, 4, 5, 6, 7, 8, 9, 10, 11])]\n",
    "\n",
    "# Step 2: Identify unique days within May, June, and July\n",
    "summer_months = A[A['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "\n",
    "# Step 3: Sample these days for val_A\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_A = A[A['date_forecast'].dt.date.isin(sampled_days)]\n",
    "\n",
    "# Step 4: Define train_A as the remaining data\n",
    "train_A = A[~A['date_forecast'].dt.date.isin(sampled_days)]\n",
    "\n",
    "# Prepare your features and target variables\n",
    "X_train_A = train_A.drop(columns='pv_measurement')\n",
    "y_train_A = train_A['pv_measurement']\n",
    "X_val_A = val_A.drop(columns='pv_measurement')\n",
    "y_val_A = val_A['pv_measurement']\n",
    "\n",
    "# Repeat for B and C\n",
    "B = B[B['date_forecast'].dt.month.isin([2, 3, 4, 5, 6, 7, 8, 9, 10, 11])]\n",
    "summer_months = B[B['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_B = B[B['date_forecast'].dt.date.isin(sampled_days)]\n",
    "train_B = B[~B['date_forecast'].dt.date.isin(sampled_days)]\n",
    "X_train_B = train_B.drop(columns='pv_measurement')\n",
    "y_train_B = train_B['pv_measurement']\n",
    "X_val_B = val_B.drop(columns='pv_measurement')\n",
    "y_val_B = val_B['pv_measurement']\n",
    "\n",
    "C = C[C['date_forecast'].dt.month.isin([2, 3, 4, 5, 6, 7, 8, 9, 10, 11])]\n",
    "summer_months = C[C['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_C = C[C['date_forecast'].dt.date.isin(sampled_days)]\n",
    "train_C = C[~C['date_forecast'].dt.date.isin(sampled_days)]\n",
    "X_train_C = train_C.drop(columns='pv_measurement')\n",
    "y_train_C = train_C['pv_measurement']\n",
    "X_val_C = val_C.drop(columns='pv_measurement')\n",
    "y_val_C = val_C['pv_measurement']\n",
    "\n",
    "# Drop date_forecast\n",
    "train_A = train_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "train_B = train_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "train_C = train_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_A = val_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_B = val_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_C = val_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_A = X_train_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_B = X_train_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_C = X_train_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_A = X_val_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_B = X_val_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_C = X_val_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "test_A = test_A.drop(columns=['date_forecast'])\n",
    "test_B = test_B.drop(columns=['date_forecast'])\n",
    "test_C = test_C.drop(columns=['date_forecast'])\n",
    "\n",
    "train_auto_A = TabularDataset(train_A)\n",
    "val_auto_A = TabularDataset(val_A)\n",
    "\n",
    "train_auto_B = TabularDataset(train_B)\n",
    "val_auto_B = TabularDataset(val_B)\n",
    "\n",
    "train_auto_C = TabularDataset(train_C)\n",
    "val_auto_C = TabularDataset(val_C)\n",
    "\n",
    "auto_label = 'pv_measurement'\n",
    "\n",
    "# Set the parameters for the XGBoost models\n",
    "params_xgb_A = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.4, \n",
    "    'learning_rate': 0.012, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 0.8, \n",
    "    'reg_lambda': 0.8, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1,\n",
    "    'num_parallel_tree': 2\n",
    "}\n",
    "\n",
    "params_xgb_B = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.8, \n",
    "    'learning_rate': 0.008, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 1, \n",
    "    'reg_lambda': 3, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "params_xgb_C = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.8, \n",
    "    'learning_rate': 0.008, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 1, \n",
    "    'reg_lambda': 3, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "xgb_A = xgb.XGBRegressor(**params_xgb_A)\n",
    "xgb_B = xgb.XGBRegressor(**params_xgb_B)\n",
    "xgb_C = xgb.XGBRegressor(**params_xgb_C)\n",
    "\n",
    "cat_A = CatBoostRegressor(\n",
    "    iterations=5000,         # The number of trees to build\n",
    "    #learning_rate=0.09,     # The learning rate\n",
    "    #depth=10,               # Depth of the tree\n",
    "    loss_function='MAE',     # Loss function to be optimized. RMSE is common for regression.\n",
    "    eval_metric='MAE',       # Evaluation metric for the validation set\n",
    "    #random_seed=42,         # Seed for reproducibility\n",
    "    #verbose=100             # Frequency of logging the training process\n",
    ")\n",
    "\n",
    "cat_B = CatBoostRegressor(\n",
    "    iterations=5000,\n",
    "    #learning_rate=0.09,\n",
    "    #depth=10,\n",
    "    loss_function='MAE',\n",
    "    eval_metric='MAE',\n",
    "    #random_seed=42,\n",
    "    #verbose=100\n",
    ")\n",
    "\n",
    "cat_C = CatBoostRegressor(\n",
    "    iterations=5000,\n",
    "    #learning_rate=0.09,\n",
    "    #depth=10,\n",
    "    loss_function='MAE',\n",
    "    eval_metric='MAE',\n",
    "    #random_seed=42,\n",
    "    #verbose=100\n",
    ")\n",
    "\n",
    "# Prepare data for the XGBoost models. We got them to work the best when having fewer columns\n",
    "xgb_columns = [\n",
    "    'total_radiationW',\n",
    "    'snow_accumulation',\n",
    "    'super_cooled_liquid_waterkgm2',\n",
    "    'average_wind_speed',\n",
    "    'sun_elevationd',\n",
    "    'sun_azimuthd',\n",
    "    'clear_sky_radW',\n",
    "    'month',\n",
    "    't_1000hPaC',\n",
    "    'msl_pressurehPa_scaled',\n",
    "    'rain_waterkgm2',\n",
    "    'cloud_base_aglm',\n",
    "    'effective_cloud_coverp',\n",
    "    'dew_or_rimeidx'\n",
    "]\n",
    "print(train_A.columns)\n",
    "\n",
    "X_train_xgb_A = train_A[xgb_columns]\n",
    "y_train_xgb_A = train_A['pv_measurement']\n",
    "X_test_xgb_A = test_A[xgb_columns]\n",
    "\n",
    "X_train_xgb_B = train_B[xgb_columns]\n",
    "y_train_xgb_B = train_B['pv_measurement']\n",
    "X_test_xgb_B = test_B[xgb_columns]\n",
    "\n",
    "X_train_xgb_C = train_C[xgb_columns]\n",
    "y_train_xgb_C = train_C['pv_measurement']\n",
    "X_test_xgb_C = test_C[xgb_columns]\n",
    "\n",
    "# Train the XGBoost models\n",
    "xgb_A.fit(\n",
    "    X=X_train_xgb_A, y=y_train_xgb_A,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "xgb_B.fit(\n",
    "    X=X_train_xgb_B, y=y_train_xgb_B,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "xgb_C.fit(\n",
    "    X=X_train_xgb_C, y=y_train_xgb_C,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Train the CatBoost models\n",
    "cat_A.fit(\n",
    "    X_train_A, y_train_A,\n",
    "    eval_set=(X_val_A, y_val_A),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "cat_B.fit(\n",
    "    X_train_B, y_train_B,\n",
    "    eval_set=(X_val_B, y_val_B),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "cat_C.fit(\n",
    "    X_train_C, y_train_C,\n",
    "    eval_set=(X_val_C, y_val_C),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "auto_A = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_A, \n",
    "                                                                                   presets='medium_quality', \n",
    "                                                                                   tuning_data=val_auto_A, \n",
    "                                                                                   use_bag_holdout=True, \n",
    "                                                                                   ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "auto_B = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_B,\n",
    "                                                                                      presets='medium_quality',\n",
    "                                                                                      tuning_data=val_auto_B,\n",
    "                                                                                      use_bag_holdout=True,\n",
    "                                                                                      ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "auto_C = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_C,\n",
    "                                                                                        presets='medium_quality',\n",
    "                                                                                        tuning_data=val_auto_C,\n",
    "                                                                                        use_bag_holdout=True,\n",
    "                                                                                        ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "xgb_weight = 0.2\n",
    "cat_weight = 0.4\n",
    "auto_weight = 0.4\n",
    "\n",
    "pred_xgb_A = xgb_A.predict(X_test_xgb_C)\n",
    "pred_xgb_B = xgb_B.predict(X_test_xgb_C)\n",
    "pred_xgb_C = xgb_C.predict(X_test_xgb_C)\n",
    "\n",
    "pred_auto_A = auto_A.predict(test_A)\n",
    "pred_auto_B = auto_B.predict(test_B)\n",
    "pred_auto_C = auto_C.predict(test_C)\n",
    "\n",
    "pred_cat_A = cat_A.predict(test_A)\n",
    "pred_cat_B = cat_B.predict(test_B)\n",
    "pred_cat_C = cat_C.predict(test_C)\n",
    "\n",
    "# Ensemble that seemed the best after some experimentation\n",
    "pred_A = (pred_xgb_A*xgb_weight + pred_cat_A*cat_weight + pred_auto_A*auto_weight)\n",
    "pred_B = (pred_xgb_B*xgb_weight + pred_cat_B*cat_weight + pred_auto_B*auto_weight)\n",
    "pred_C = (pred_xgb_C*xgb_weight + pred_cat_C*cat_weight + pred_auto_C*auto_weight)\n",
    "\n",
    "pred_A = np.clip(pred_A, 0, None)\n",
    "pred_B = np.clip(pred_B, 0, None)\n",
    "pred_C = np.clip(pred_C, 0, None)\n",
    "\n",
    "# Concatenate predictions\n",
    "predictions = np.concatenate([pred_A, pred_B, pred_C])\n",
    "\n",
    "# Save predictions\n",
    "predictions_3 = predictions\n",
    "\n",
    "# Create an id array\n",
    "ids = np.arange(0, len(predictions_3))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'prediction': predictions_3\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('predictions_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence 4\n",
    "\n",
    "%reset -f\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostRegressor\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def add_experimental_features(df):\n",
    "    \"\"\"\n",
    "    Experimental feature engineering.\n",
    "    \"\"\"\n",
    "\n",
    "    # Radiation Features\n",
    "    df['total_radiation:W'] = df['direct_rad:W'] + df['diffuse_rad:W']\n",
    "    df['total_radiation_1h:J'] = df['direct_rad_1h:J'] + df['diffuse_rad_1h:J']\n",
    "    df['rad_diff:W'] = df['direct_rad:W'] - df['diffuse_rad:W']\n",
    "    df['rad_diff_1h:J'] = df['direct_rad_1h:J'] - df['diffuse_rad_1h:J']\n",
    "    df['diffuse_direct_ratio'] = df['diffuse_rad:W'] / df['direct_rad:W']\n",
    "\n",
    "    # Temperature and Pressure Features\n",
    "    df['temp_dewpoint_diff'] = df['t_1000hPa:K'] - df['dew_point_2m:K']\n",
    "    df['pressure_gradient'] = df['pressure_100m:hPa'] - df['pressure_50m:hPa']\n",
    "    df['t_1000hPa:C'] = df['t_1000hPa:K'] - 273.15\n",
    "    df['dew_point_2m:C'] = df['dew_point_2m:K'] - 273.15\n",
    "    df['msl_pressure:hPa_scaled'] = MinMaxScaler().fit_transform(df['msl_pressure:hPa'].values.reshape(-1, 1))\n",
    "    df['sfc_pressure:hPa_scaled'] = MinMaxScaler().fit_transform(df['sfc_pressure:hPa'].values.reshape(-1, 1))\n",
    "\n",
    "    # Wind Features\n",
    "    df['wind_vector_magnitude'] = (df['wind_speed_u_10m:ms']**2 + df['wind_speed_v_10m:ms']**2 + df['wind_speed_w_1000hPa:ms']**2)**0.5\n",
    "    df['average_wind_speed'] = (df['wind_speed_10m:ms'] + df['wind_speed_u_10m:ms']) / 2\n",
    "\n",
    "    # Cloud and Snow Features\n",
    "    df['cloud_humidity_product'] = df['total_cloud_cover:p'] * df['absolute_humidity_2m:gm3']\n",
    "    df['snow_accumulation'] = df[['fresh_snow_24h:cm', 'fresh_snow_12h:cm', 'fresh_snow_6h:cm', 'fresh_snow_3h:cm', 'fresh_snow_1h:cm']].sum(axis=1)\n",
    "\n",
    "    # Interaction between radiation and cloud cover\n",
    "    df['radiation_cloud_interaction'] = df['direct_rad:W'] * df['effective_cloud_cover:p']\n",
    "\n",
    "    # Interaction between temperature and radiation (considering that high temperature may reduce efficiency)\n",
    "    df['temp_rad_interaction'] = df['t_1000hPa:K'] * df['total_radiation:W']\n",
    "\n",
    "    # Interaction between wind cooling effect and temperature\n",
    "    df['wind_temp_interaction'] = df['average_wind_speed'] * df['t_1000hPa:K']\n",
    "\n",
    "    # Interaction between humidity and temperature\n",
    "    df['humidity_temp_interaction'] = df['absolute_humidity_2m:gm3'] * df['t_1000hPa:K']\n",
    "\n",
    "    # Interaction between humidity and radiation\n",
    "    df['sun_elevation_direct_rad_interaction'] = df['sun_elevation:d'] * df['direct_rad:W']\n",
    "\n",
    "    # Precipitation Features\n",
    "    df['precip'] = df['precip_5min:mm']*df['precip_type_5min:idx']\n",
    "\n",
    "    # Safeguard in case of inf values\n",
    "    df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_date_features(df):\n",
    "    \"\"\"\n",
    "    Adds 'month', 'year', 'hour' and 'day' columns to the dataframe based on the 'date_forecast' column.\n",
    "    Also adds 'hour_sin' and 'hour_cos' columns for the hour of the day.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if 'date_forecast' exists in the dataframe\n",
    "    if 'date_forecast' in df.columns:\n",
    "        # Convert the 'date_forecast' column to datetime format\n",
    "        df['date_forecast'] = pd.to_datetime(df['date_forecast'])\n",
    "        \n",
    "        # Extract month, year, hour and day\n",
    "        df['month'] = df['date_forecast'].dt.month\n",
    "        df['year'] = df['date_forecast'].dt.year\n",
    "        df['hour'] = df['date_forecast'].dt.hour\n",
    "        df['day'] = df['date_forecast'].dt.day\n",
    "        df['hour_sin'] = np.sin(2*np.pi*df['hour']/24)\n",
    "        df['hour_cos'] = np.cos(2*np.pi*df['hour']/24)\n",
    "\n",
    "    else:\n",
    "        print(\"Warning: 'date_forecast' column not found in the dataframe. No date features added.\")\n",
    "        return df  # Keep the 'date_forecast' column in the dataframe\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Adding discretized features for the continuous variables to help tree-based models\n",
    "\n",
    "def bin_columns(dataframe, columns_to_bin, n_bins=5):\n",
    "    \"\"\"\n",
    "    Bins the specified columns of the dataframe into equal-sized bins.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframe: pd.DataFrame\n",
    "    - columns_to_bin: list of strings, the names of the columns to bin\n",
    "    - n_bins: int or dict, the number of bins for each column (if int, use the same number for all columns;\n",
    "              if dict, specify individual numbers with column names as keys)\n",
    "    \n",
    "    Returns:\n",
    "    - binned_dataframe: pd.DataFrame, the dataframe with the specified columns binned\n",
    "    \"\"\"\n",
    "    binned_dataframe = dataframe.copy()\n",
    "    \n",
    "    for column in columns_to_bin:\n",
    "        # Determine the number of bins for this column\n",
    "        bins = n_bins if isinstance(n_bins, int) else n_bins.get(column, 5)\n",
    "        \n",
    "        # Create quantile-based bins\n",
    "        binned_dataframe[f'binned_{column}'] = pd.qcut(\n",
    "            binned_dataframe[column],\n",
    "            q=bins,\n",
    "            labels=False,\n",
    "            duplicates='drop'\n",
    "        )\n",
    "        \n",
    "    return binned_dataframe\n",
    "\n",
    "def add_binned_features(df):\n",
    "    columns_to_bin = [\n",
    "        'super_cooled_liquid_water:kgm2',\n",
    "        'ceiling_height_agl:m',\n",
    "        'cloud_base_agl:m'\n",
    "    ]\n",
    "\n",
    "    # Bin the columns\n",
    "    # df = bin_columns(df, columns_to_bin)\n",
    "    df = bin_columns(df, ['effective_cloud_cover:p'], n_bins=2)\n",
    "    df = bin_columns(df, ['ceiling_height_agl:m'], n_bins=3)\n",
    "    df = bin_columns(df, ['average_wind_speed'], n_bins=5)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_rate_of_change_features(df, features, second_order=False):\n",
    "    \"\"\"\n",
    "    Adds rate of change columns for specified features in the dataframe.\n",
    "    Assumes the dataframe is time sorted. If second_order is True, it also adds the second order rate of change.\n",
    "    \"\"\"\n",
    "    for feature in features:\n",
    "        rate_column_name = feature + '_rate_of_change'\n",
    "        df[rate_column_name] = df[feature].diff().fillna(0)  # Handle the first diff NaN if required\n",
    "        \n",
    "        if second_order:  # Check if second order difference is required\n",
    "            second_order_column_name = feature + '_rate_of_change_of_change'\n",
    "            df[second_order_column_name] = df[rate_column_name].diff().fillna(0)  # Second order difference\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_rate_of_change_features_to_df(df):\n",
    "    # Define the features for which to calculate rate of change\n",
    "    features_to_diff = [\n",
    "        't_1000hPa:K',\n",
    "        'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad:W',\n",
    "        'effective_cloud_cover:p', 'total_radiation:W'\n",
    "    ]\n",
    "\n",
    "    # Add rate of change features\n",
    "    return add_rate_of_change_features(df, features_to_diff, second_order=False)\n",
    "\n",
    "def add_est_obs_feature(df):\n",
    "    \"\"\"\n",
    "    Adds a column to the dataframe that indicates whether the data is estimated or observed.\n",
    "    \"\"\"\n",
    "    # Add the est_obs feature\n",
    "    if 'date_calc' not in df.columns:\n",
    "        # If 'date_calc' does not exist, create 'observed' column and set to 1\n",
    "        df['observed'] = 1\n",
    "        return df\n",
    "    else:\n",
    "        # If 'date_calc' exists, create a new column and set values to 0\n",
    "        df['observed'] = 0\n",
    "        return df.drop(columns=['date_calc'])\n",
    "    \n",
    "def remove_constant_regions(dataframe, column_name=\"pv_measurement\", threshold=72):\n",
    "    \"\"\"\n",
    "    Removes rows where the specified column has constant values for more than the given threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the specified column exists in the dataframe\n",
    "    if column_name not in dataframe.columns:\n",
    "        print(f\"Warning: '{column_name}' column not found in the dataframe. No rows removed.\")\n",
    "        return dataframe\n",
    "    \n",
    "    same_as_previous = dataframe[column_name].eq(dataframe[column_name].shift())\n",
    "    group_ids = (~same_as_previous).cumsum()\n",
    "    to_remove = group_ids[same_as_previous].value_counts() > threshold\n",
    "    group_ids_to_remove = to_remove[to_remove].index\n",
    "    \n",
    "    # Drop entire rows that match the conditions\n",
    "    return dataframe.drop(dataframe[group_ids.isin(group_ids_to_remove)].index)\n",
    "\n",
    "def add_lagged_features(df, features_with__lags, fill_value=None):\n",
    "    \"\"\"\n",
    "    Adds lagged columns for specified features in the dataframe with specific lag periods.\n",
    "    'features_with_specific_lags' is a dictionary with features as keys and specific lag as values.\n",
    "    'fill_value' is what to fill the NaNs with, after shifting.\n",
    "    \"\"\"\n",
    "    for feature, specific_lag in features_with__lags.items():\n",
    "        lag_column_name = f\"{feature}_lag_{specific_lag}\"\n",
    "        df[lag_column_name] = df[feature].shift(specific_lag).fillna(fill_value)\n",
    "    return df\n",
    "\n",
    "def add_lagged_features_to_df(df):\n",
    "    features_with_lags = {\n",
    "        'total_radiation:W': 1,\n",
    "        'total_radiation:W': -1,\n",
    "        'rad_diff:W': 1,\n",
    "        'rad_diff:W': -1,\n",
    "        'total_radiation_1h:J': 1,\n",
    "        'total_radiation_1h:J': -1\n",
    "    }\n",
    "\n",
    "    # Add lagged features for specific lags\n",
    "    return add_lagged_features(df, features_with_lags, fill_value=0)\n",
    "\n",
    "def handle_nan(df):\n",
    "    # Remove the rows where target is nan\n",
    "    try:\n",
    "        df = df[df['pv_measurement'].notna()]\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    # Set all remaining nans to 0\n",
    "    return df.fillna(0)\n",
    "\n",
    "# Aggregate the data to hourly with some aggregation methods for each column\n",
    "aggregation_methods = {\n",
    "    'date_forecast': 'first',\n",
    "    'diffuse_rad:W': 'sum',\n",
    "    'direct_rad:W': 'last',\n",
    "    'clear_sky_rad:W': 'sum',\n",
    "    'diffuse_rad_1h:J': 'last',\n",
    "    'direct_rad_1h:J': 'last',\n",
    "    'clear_sky_energy_1h:J': 'last',\n",
    "    'absolute_humidity_2m:gm3': 'mean',\n",
    "    'air_density_2m:kgm3': 'mean',\n",
    "    'ceiling_height_agl:m': 'max',\n",
    "    'cloud_base_agl:m': 'mean',\n",
    "    'dew_or_rime:idx': 'min',\n",
    "    'dew_point_2m:K': 'mean',\n",
    "    'effective_cloud_cover:p': 'sum',\n",
    "    'elevation:m': 'first',\n",
    "    'fresh_snow_12h:cm': 'max',\n",
    "    'fresh_snow_1h:cm': 'sum',\n",
    "    'fresh_snow_24h:cm': 'max',\n",
    "    'fresh_snow_3h:cm': 'max',\n",
    "    'fresh_snow_6h:cm': 'max',\n",
    "    'is_day:idx': 'max',\n",
    "    'is_in_shadow:idx': 'max',\n",
    "    'msl_pressure:hPa': 'mean',\n",
    "    'precip_5min:mm': 'sum',\n",
    "    'precip_type_5min:idx': 'sum',\n",
    "    'pressure_100m:hPa': 'mean',\n",
    "    'pressure_50m:hPa': 'mean',\n",
    "    'prob_rime:p': 'max',\n",
    "    'rain_water:kgm2': 'sum',\n",
    "    'relative_humidity_1000hPa:p': 'mean',\n",
    "    'sfc_pressure:hPa': 'mean',\n",
    "    'snow_density:kgm3': 'mean',\n",
    "    'snow_depth:cm': 'max',\n",
    "    'snow_drift:idx': 'max',\n",
    "    'snow_melt_10min:mm': 'sum',\n",
    "    'snow_water:kgm2': 'sum',\n",
    "    'sun_azimuth:d': 'first',\n",
    "    'sun_elevation:d': 'sum',\n",
    "    'super_cooled_liquid_water:kgm2': 'sum',\n",
    "    't_1000hPa:K': 'mean',\n",
    "    'total_cloud_cover:p': 'mean',\n",
    "    'visibility:m': 'mean',\n",
    "    'wind_speed_10m:ms': 'mean',\n",
    "    'wind_speed_u_10m:ms': 'mean',\n",
    "    'wind_speed_v_10m:ms': 'mean',\n",
    "    'wind_speed_w_1000hPa:ms': 'mean',\n",
    "    'cloud_base_agl:m': 'max',\n",
    "    'snow_density:kgm3': 'mean'\n",
    "}\n",
    "\n",
    "# Read in the data\n",
    "x_target_A = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "x_train_obs_A = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "x_train_est_A = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "x_test_est_A = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "x_target_B = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "x_train_obs_B = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "x_train_est_B = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "x_test_est_B = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "x_target_C = pd.read_parquet('./data/C/train_targets.parquet')\n",
    "x_train_obs_C = pd.read_parquet('./data/C/X_train_observed.parquet')\n",
    "x_train_est_C = pd.read_parquet('./data/C/X_train_estimated.parquet')\n",
    "x_test_est_C = pd.read_parquet('./data/C/X_test_estimated.parquet')\n",
    "\n",
    "# Rename time to date_forecast in target\n",
    "x_target_A.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "x_target_B.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "x_target_C.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "\n",
    "# Fix missing data for test set. Assumin NaN means 0 in these categories\n",
    "x_test_est_A['effective_cloud_cover:p'] = x_test_est_A['effective_cloud_cover:p'].fillna(0)\n",
    "x_test_est_B['effective_cloud_cover:p'] = x_test_est_B['effective_cloud_cover:p'].fillna(0)\n",
    "x_test_est_C['effective_cloud_cover:p'] = x_test_est_C['effective_cloud_cover:p'].fillna(0)\n",
    "\n",
    "x_test_est_A['total_cloud_cover:p'] = x_test_est_A['total_cloud_cover:p'].fillna(0)\n",
    "x_test_est_B['total_cloud_cover:p'] = x_test_est_B['total_cloud_cover:p'].fillna(0)\n",
    "x_test_est_C['total_cloud_cover:p'] = x_test_est_C['total_cloud_cover:p'].fillna(0)\n",
    "\n",
    "x_test_est_A['cloud_base_agl:m'] = x_test_est_A['cloud_base_agl:m'].fillna(0)\n",
    "x_test_est_B['cloud_base_agl:m'] = x_test_est_B['cloud_base_agl:m'].fillna(0)\n",
    "x_test_est_C['cloud_base_agl:m'] = x_test_est_C['cloud_base_agl:m'].fillna(0)\n",
    "\n",
    "x_test_est_A['ceiling_height_agl:m'] = x_test_est_A['ceiling_height_agl:m'].fillna(0)\n",
    "x_test_est_B['ceiling_height_agl:m'] = x_test_est_B['ceiling_height_agl:m'].fillna(0)\n",
    "x_test_est_C['ceiling_height_agl:m'] = x_test_est_C['ceiling_height_agl:m'].fillna(0)\n",
    "\n",
    "x_test_est_A['snow_density:kgm3'] = x_test_est_A['snow_density:kgm3'].fillna(0)\n",
    "x_test_est_B['snow_density:kgm3'] = x_test_est_B['snow_density:kgm3'].fillna(0)\n",
    "x_test_est_C['snow_density:kgm3'] = x_test_est_C['snow_density:kgm3'].fillna(0)\n",
    "\n",
    "x_test_est_A['snow_drift:idx'] = x_test_est_A['snow_drift:idx'].fillna(0)\n",
    "x_test_est_B['snow_drift:idx'] = x_test_est_B['snow_drift:idx'].fillna(0)\n",
    "x_test_est_C['snow_drift:idx'] = x_test_est_C['snow_drift:idx'].fillna(0)\n",
    "\n",
    "# Resample\n",
    "x_train_obs_A_resampled = x_train_obs_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_A_resampled = x_train_est_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_A_resampled = x_test_est_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "x_train_obs_B_resampled = x_train_obs_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_B_resampled = x_train_est_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_B_resampled = x_test_est_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "x_train_obs_C_resampled = x_train_obs_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_C_resampled = x_train_est_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_C_resampled = x_test_est_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "# Merge\n",
    "split_value = x_train_est_A['date_forecast'].iloc[0]\n",
    "split_index = x_target_A[x_target_A['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_A = x_target_A.iloc[:split_index]\n",
    "x_target_est_A = x_target_A.iloc[split_index:]\n",
    "\n",
    "obs_A = x_train_obs_A_resampled.merge(x_target_obs_A, left_index=True, right_on='date_forecast')\n",
    "est_A = x_train_est_A_resampled.merge(x_target_est_A, left_index=True, right_on='date_forecast')\n",
    "\n",
    "split_value = x_train_est_B['date_forecast'].iloc[0]\n",
    "split_index = x_target_B[x_target_B['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_B = x_target_B.iloc[:split_index]\n",
    "x_target_est_B = x_target_B.iloc[split_index:]\n",
    "\n",
    "obs_B = x_train_obs_B_resampled.merge(x_target_obs_B, left_index=True, right_on='date_forecast')\n",
    "est_B = x_train_est_B_resampled.merge(x_target_est_B, left_index=True, right_on='date_forecast')\n",
    "\n",
    "split_value = x_train_est_C['date_forecast'].iloc[0]\n",
    "split_index = x_target_C[x_target_C['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_C = x_target_C.iloc[:split_index]\n",
    "x_target_est_C = x_target_C.iloc[split_index:]\n",
    "\n",
    "obs_C = x_train_obs_C_resampled.merge(x_target_obs_C, left_index=True, right_on='date_forecast')\n",
    "est_C = x_train_est_C_resampled.merge(x_target_est_C, left_index=True, right_on='date_forecast')\n",
    "\n",
    "# Keep date_forecast in test dfs\n",
    "test_A = x_test_est_A_resampled\n",
    "test_B = x_test_est_B_resampled\n",
    "test_C = x_test_est_C_resampled\n",
    "\n",
    "# Drop all the NaNs\n",
    "test_A = test_A.dropna()\n",
    "test_B = test_B.dropna()\n",
    "test_C = test_C.dropna()\n",
    "\n",
    "def preprocessing(df):\n",
    "    df = add_experimental_features(df.copy())\n",
    "    df = add_date_features(df.copy())\n",
    "    df = add_binned_features(df.copy())\n",
    "    df = add_rate_of_change_features_to_df(df.copy())\n",
    "    df = add_est_obs_feature(df.copy())\n",
    "    df = remove_constant_regions(df.copy())\n",
    "    df = add_lagged_features_to_df(df.copy())\n",
    "    df = handle_nan(df.copy())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Preprocess\n",
    "obs_A = preprocessing(obs_A.copy())\n",
    "est_A = preprocessing(est_A.copy())\n",
    "test_A = preprocessing(test_A.copy())\n",
    "\n",
    "obs_B = preprocessing(obs_B.copy())\n",
    "est_B = preprocessing(est_B.copy())\n",
    "test_B = preprocessing(test_B.copy())\n",
    "\n",
    "obs_C = preprocessing(obs_C.copy())\n",
    "est_C = preprocessing(est_C.copy())\n",
    "test_C = preprocessing(test_C.copy())\n",
    "\n",
    "# Random seeds used for reproducibility\n",
    "# 32 weights: 0.3, 0.3, 0.4\n",
    "# 24 weights: 0.3, 0.3, 0.4\n",
    "# 33 (without winter months 1 and 12) weights: 0.2, 0.4, 0.4\n",
    "# 11 (without winter months 1, 2 and 11, 12) weights: 0.25, 0.35, 0.4\n",
    "# 5 weights: 0.4, 0.3, 0.3\n",
    "\n",
    "# Best score is the mean prediction of all the 5 seeds mentioned above. The first weight is xgboost, the second is catboost, and the third is autogluon.\n",
    "\n",
    "# Set the random seed\n",
    "np.random.seed(11)\n",
    "\n",
    "# Concatinate\n",
    "A = pd.concat([obs_A, est_A])\n",
    "B = pd.concat([obs_B, est_B])\n",
    "C = pd.concat([obs_C, est_C])\n",
    "\n",
    "# Remove characters unparseable for CatBoost \n",
    "A.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in A.columns]\n",
    "B.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in B.columns]\n",
    "C.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in C.columns]\n",
    "\n",
    "test_A.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_A.columns]\n",
    "test_B.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_B.columns]\n",
    "test_C.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_C.columns]\n",
    "\n",
    "# Getting validation data from summer months, because the test set is from summer months. We experimentet with excluding winter months\n",
    "# from the training data here.\n",
    "\n",
    "# Step 1: Filter A to include only months from March to October\n",
    "A = A[A['date_forecast'].dt.month.isin([3, 4, 5, 6, 7, 8, 9, 10])]\n",
    "\n",
    "# Step 2: Identify unique days within May, June, and July\n",
    "summer_months = A[A['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "\n",
    "# Step 3: Sample these days for val_A\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_A = A[A['date_forecast'].dt.date.isin(sampled_days)]\n",
    "\n",
    "# Step 4: Define train_A as the remaining data\n",
    "train_A = A[~A['date_forecast'].dt.date.isin(sampled_days)]\n",
    "\n",
    "# Prepare your features and target variables\n",
    "X_train_A = train_A.drop(columns='pv_measurement')\n",
    "y_train_A = train_A['pv_measurement']\n",
    "X_val_A = val_A.drop(columns='pv_measurement')\n",
    "y_val_A = val_A['pv_measurement']\n",
    "\n",
    "# Repeat for B and C\n",
    "B = B[B['date_forecast'].dt.month.isin([3, 4, 5, 6, 7, 8, 9, 10])]\n",
    "summer_months = B[B['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_B = B[B['date_forecast'].dt.date.isin(sampled_days)]\n",
    "train_B = B[~B['date_forecast'].dt.date.isin(sampled_days)]\n",
    "X_train_B = train_B.drop(columns='pv_measurement')\n",
    "y_train_B = train_B['pv_measurement']\n",
    "X_val_B = val_B.drop(columns='pv_measurement')\n",
    "y_val_B = val_B['pv_measurement']\n",
    "\n",
    "C = C[C['date_forecast'].dt.month.isin([3, 4, 5, 6, 7, 8, 9, 10])]\n",
    "summer_months = C[C['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_C = C[C['date_forecast'].dt.date.isin(sampled_days)]\n",
    "train_C = C[~C['date_forecast'].dt.date.isin(sampled_days)]\n",
    "X_train_C = train_C.drop(columns='pv_measurement')\n",
    "y_train_C = train_C['pv_measurement']\n",
    "X_val_C = val_C.drop(columns='pv_measurement')\n",
    "y_val_C = val_C['pv_measurement']\n",
    "\n",
    "# Drop date_forecast\n",
    "train_A = train_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "train_B = train_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "train_C = train_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_A = val_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_B = val_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_C = val_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_A = X_train_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_B = X_train_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_C = X_train_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_A = X_val_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_B = X_val_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_C = X_val_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "test_A = test_A.drop(columns=['date_forecast'])\n",
    "test_B = test_B.drop(columns=['date_forecast'])\n",
    "test_C = test_C.drop(columns=['date_forecast'])\n",
    "\n",
    "train_auto_A = TabularDataset(train_A)\n",
    "val_auto_A = TabularDataset(val_A)\n",
    "\n",
    "train_auto_B = TabularDataset(train_B)\n",
    "val_auto_B = TabularDataset(val_B)\n",
    "\n",
    "train_auto_C = TabularDataset(train_C)\n",
    "val_auto_C = TabularDataset(val_C)\n",
    "\n",
    "auto_label = 'pv_measurement'\n",
    "\n",
    "# Set the parameters for the XGBoost models\n",
    "params_xgb_A = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.4, \n",
    "    'learning_rate': 0.012, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 0.8, \n",
    "    'reg_lambda': 0.8, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1,\n",
    "    'num_parallel_tree': 2\n",
    "}\n",
    "\n",
    "params_xgb_B = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.8, \n",
    "    'learning_rate': 0.008, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 1, \n",
    "    'reg_lambda': 3, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "params_xgb_C = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.8, \n",
    "    'learning_rate': 0.008, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 1, \n",
    "    'reg_lambda': 3, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "xgb_A = xgb.XGBRegressor(**params_xgb_A)\n",
    "xgb_B = xgb.XGBRegressor(**params_xgb_B)\n",
    "xgb_C = xgb.XGBRegressor(**params_xgb_C)\n",
    "\n",
    "cat_A = CatBoostRegressor(\n",
    "    iterations=5000,         # The number of trees to build\n",
    "    #learning_rate=0.09,     # The learning rate\n",
    "    #depth=10,               # Depth of the tree\n",
    "    loss_function='MAE',     # Loss function to be optimized. RMSE is common for regression.\n",
    "    eval_metric='MAE',       # Evaluation metric for the validation set\n",
    "    #random_seed=42,         # Seed for reproducibility\n",
    "    #verbose=100             # Frequency of logging the training process\n",
    ")\n",
    "\n",
    "cat_B = CatBoostRegressor(\n",
    "    iterations=5000,\n",
    "    #learning_rate=0.09,\n",
    "    #depth=10,\n",
    "    loss_function='MAE',\n",
    "    eval_metric='MAE',\n",
    "    #random_seed=42,\n",
    "    #verbose=100\n",
    ")\n",
    "\n",
    "cat_C = CatBoostRegressor(\n",
    "    iterations=5000,\n",
    "    #learning_rate=0.09,\n",
    "    #depth=10,\n",
    "    loss_function='MAE',\n",
    "    eval_metric='MAE',\n",
    "    #random_seed=42,\n",
    "    #verbose=100\n",
    ")\n",
    "\n",
    "# Prepare data for the XGBoost models. We got them to work the best when having fewer columns\n",
    "xgb_columns = [\n",
    "    'total_radiationW',\n",
    "    'snow_accumulation',\n",
    "    'super_cooled_liquid_waterkgm2',\n",
    "    'average_wind_speed',\n",
    "    'sun_elevationd',\n",
    "    'sun_azimuthd',\n",
    "    'clear_sky_radW',\n",
    "    'month',\n",
    "    't_1000hPaC',\n",
    "    'msl_pressurehPa_scaled',\n",
    "    'rain_waterkgm2',\n",
    "    'cloud_base_aglm',\n",
    "    'effective_cloud_coverp',\n",
    "    'dew_or_rimeidx'\n",
    "]\n",
    "print(train_A.columns)\n",
    "\n",
    "X_train_xgb_A = train_A[xgb_columns]\n",
    "y_train_xgb_A = train_A['pv_measurement']\n",
    "X_test_xgb_A = test_A[xgb_columns]\n",
    "\n",
    "X_train_xgb_B = train_B[xgb_columns]\n",
    "y_train_xgb_B = train_B['pv_measurement']\n",
    "X_test_xgb_B = test_B[xgb_columns]\n",
    "\n",
    "X_train_xgb_C = train_C[xgb_columns]\n",
    "y_train_xgb_C = train_C['pv_measurement']\n",
    "X_test_xgb_C = test_C[xgb_columns]\n",
    "\n",
    "# Train the XGBoost models\n",
    "xgb_A.fit(\n",
    "    X=X_train_xgb_A, y=y_train_xgb_A,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "xgb_B.fit(\n",
    "    X=X_train_xgb_B, y=y_train_xgb_B,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "xgb_C.fit(\n",
    "    X=X_train_xgb_C, y=y_train_xgb_C,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Train the CatBoost models\n",
    "cat_A.fit(\n",
    "    X_train_A, y_train_A,\n",
    "    eval_set=(X_val_A, y_val_A),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "cat_B.fit(\n",
    "    X_train_B, y_train_B,\n",
    "    eval_set=(X_val_B, y_val_B),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "cat_C.fit(\n",
    "    X_train_C, y_train_C,\n",
    "    eval_set=(X_val_C, y_val_C),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "auto_A = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_A, \n",
    "                                                                                   presets='medium_quality', \n",
    "                                                                                   tuning_data=val_auto_A, \n",
    "                                                                                   use_bag_holdout=True, \n",
    "                                                                                   ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "auto_B = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_B,\n",
    "                                                                                      presets='medium_quality',\n",
    "                                                                                      tuning_data=val_auto_B,\n",
    "                                                                                      use_bag_holdout=True,\n",
    "                                                                                      ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "auto_C = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_C,\n",
    "                                                                                        presets='medium_quality',\n",
    "                                                                                        tuning_data=val_auto_C,\n",
    "                                                                                        use_bag_holdout=True,\n",
    "                                                                                        ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "xgb_weight = 0.25\n",
    "cat_weight = 0.35\n",
    "auto_weight = 0.4\n",
    "\n",
    "pred_xgb_A = xgb_A.predict(X_test_xgb_C)\n",
    "pred_xgb_B = xgb_B.predict(X_test_xgb_C)\n",
    "pred_xgb_C = xgb_C.predict(X_test_xgb_C)\n",
    "\n",
    "pred_auto_A = auto_A.predict(test_A)\n",
    "pred_auto_B = auto_B.predict(test_B)\n",
    "pred_auto_C = auto_C.predict(test_C)\n",
    "\n",
    "pred_cat_A = cat_A.predict(test_A)\n",
    "pred_cat_B = cat_B.predict(test_B)\n",
    "pred_cat_C = cat_C.predict(test_C)\n",
    "\n",
    "# Ensemble that seemed the best after some experimentation\n",
    "pred_A = (pred_xgb_A*xgb_weight + pred_cat_A*cat_weight + pred_auto_A*auto_weight)\n",
    "pred_B = (pred_xgb_B*xgb_weight + pred_cat_B*cat_weight + pred_auto_B*auto_weight)\n",
    "pred_C = (pred_xgb_C*xgb_weight + pred_cat_C*cat_weight + pred_auto_C*auto_weight)\n",
    "\n",
    "pred_A = np.clip(pred_A, 0, None)\n",
    "pred_B = np.clip(pred_B, 0, None)\n",
    "pred_C = np.clip(pred_C, 0, None)\n",
    "\n",
    "# Concatenate predictions\n",
    "predictions = np.concatenate([pred_A, pred_B, pred_C])\n",
    "\n",
    "# Save predictions\n",
    "predictions_4 = predictions\n",
    "\n",
    "# Create an id array\n",
    "ids = np.arange(0, len(predictions_4))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'prediction': predictions_4\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('predictions_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence 5\n",
    "\n",
    "%reset -f\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostRegressor\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def add_experimental_features(df):\n",
    "    \"\"\"\n",
    "    Experimental feature engineering.\n",
    "    \"\"\"\n",
    "\n",
    "    # Radiation Features\n",
    "    df['total_radiation:W'] = df['direct_rad:W'] + df['diffuse_rad:W']\n",
    "    df['total_radiation_1h:J'] = df['direct_rad_1h:J'] + df['diffuse_rad_1h:J']\n",
    "    df['rad_diff:W'] = df['direct_rad:W'] - df['diffuse_rad:W']\n",
    "    df['rad_diff_1h:J'] = df['direct_rad_1h:J'] - df['diffuse_rad_1h:J']\n",
    "    df['diffuse_direct_ratio'] = df['diffuse_rad:W'] / df['direct_rad:W']\n",
    "\n",
    "    # Temperature and Pressure Features\n",
    "    df['temp_dewpoint_diff'] = df['t_1000hPa:K'] - df['dew_point_2m:K']\n",
    "    df['pressure_gradient'] = df['pressure_100m:hPa'] - df['pressure_50m:hPa']\n",
    "    df['t_1000hPa:C'] = df['t_1000hPa:K'] - 273.15\n",
    "    df['dew_point_2m:C'] = df['dew_point_2m:K'] - 273.15\n",
    "    df['msl_pressure:hPa_scaled'] = MinMaxScaler().fit_transform(df['msl_pressure:hPa'].values.reshape(-1, 1))\n",
    "    df['sfc_pressure:hPa_scaled'] = MinMaxScaler().fit_transform(df['sfc_pressure:hPa'].values.reshape(-1, 1))\n",
    "\n",
    "    # Wind Features\n",
    "    df['wind_vector_magnitude'] = (df['wind_speed_u_10m:ms']**2 + df['wind_speed_v_10m:ms']**2 + df['wind_speed_w_1000hPa:ms']**2)**0.5\n",
    "    df['average_wind_speed'] = (df['wind_speed_10m:ms'] + df['wind_speed_u_10m:ms']) / 2\n",
    "\n",
    "    # Cloud and Snow Features\n",
    "    df['cloud_humidity_product'] = df['total_cloud_cover:p'] * df['absolute_humidity_2m:gm3']\n",
    "    df['snow_accumulation'] = df[['fresh_snow_24h:cm', 'fresh_snow_12h:cm', 'fresh_snow_6h:cm', 'fresh_snow_3h:cm', 'fresh_snow_1h:cm']].sum(axis=1)\n",
    "\n",
    "    # Interaction between radiation and cloud cover\n",
    "    df['radiation_cloud_interaction'] = df['direct_rad:W'] * df['effective_cloud_cover:p']\n",
    "\n",
    "    # Interaction between temperature and radiation (considering that high temperature may reduce efficiency)\n",
    "    df['temp_rad_interaction'] = df['t_1000hPa:K'] * df['total_radiation:W']\n",
    "\n",
    "    # Interaction between wind cooling effect and temperature\n",
    "    df['wind_temp_interaction'] = df['average_wind_speed'] * df['t_1000hPa:K']\n",
    "\n",
    "    # Interaction between humidity and temperature\n",
    "    df['humidity_temp_interaction'] = df['absolute_humidity_2m:gm3'] * df['t_1000hPa:K']\n",
    "\n",
    "    # Interaction between humidity and radiation\n",
    "    df['sun_elevation_direct_rad_interaction'] = df['sun_elevation:d'] * df['direct_rad:W']\n",
    "\n",
    "    # Precipitation Features\n",
    "    df['precip'] = df['precip_5min:mm']*df['precip_type_5min:idx']\n",
    "\n",
    "    # Safeguard in case of inf values\n",
    "    df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_date_features(df):\n",
    "    \"\"\"\n",
    "    Adds 'month', 'year', 'hour' and 'day' columns to the dataframe based on the 'date_forecast' column.\n",
    "    Also adds 'hour_sin' and 'hour_cos' columns for the hour of the day.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if 'date_forecast' exists in the dataframe\n",
    "    if 'date_forecast' in df.columns:\n",
    "        # Convert the 'date_forecast' column to datetime format\n",
    "        df['date_forecast'] = pd.to_datetime(df['date_forecast'])\n",
    "        \n",
    "        # Extract month, year, hour and day\n",
    "        df['month'] = df['date_forecast'].dt.month\n",
    "        df['year'] = df['date_forecast'].dt.year\n",
    "        df['hour'] = df['date_forecast'].dt.hour\n",
    "        df['day'] = df['date_forecast'].dt.day\n",
    "        df['hour_sin'] = np.sin(2*np.pi*df['hour']/24)\n",
    "        df['hour_cos'] = np.cos(2*np.pi*df['hour']/24)\n",
    "\n",
    "    else:\n",
    "        print(\"Warning: 'date_forecast' column not found in the dataframe. No date features added.\")\n",
    "        return df  # Keep the 'date_forecast' column in the dataframe\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Adding discretized features for the continuous variables to help tree-based models\n",
    "\n",
    "def bin_columns(dataframe, columns_to_bin, n_bins=5):\n",
    "    \"\"\"\n",
    "    Bins the specified columns of the dataframe into equal-sized bins.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframe: pd.DataFrame\n",
    "    - columns_to_bin: list of strings, the names of the columns to bin\n",
    "    - n_bins: int or dict, the number of bins for each column (if int, use the same number for all columns;\n",
    "              if dict, specify individual numbers with column names as keys)\n",
    "    \n",
    "    Returns:\n",
    "    - binned_dataframe: pd.DataFrame, the dataframe with the specified columns binned\n",
    "    \"\"\"\n",
    "    binned_dataframe = dataframe.copy()\n",
    "    \n",
    "    for column in columns_to_bin:\n",
    "        # Determine the number of bins for this column\n",
    "        bins = n_bins if isinstance(n_bins, int) else n_bins.get(column, 5)\n",
    "        \n",
    "        # Create quantile-based bins\n",
    "        binned_dataframe[f'binned_{column}'] = pd.qcut(\n",
    "            binned_dataframe[column],\n",
    "            q=bins,\n",
    "            labels=False,\n",
    "            duplicates='drop'\n",
    "        )\n",
    "        \n",
    "    return binned_dataframe\n",
    "\n",
    "def add_binned_features(df):\n",
    "    columns_to_bin = [\n",
    "        'super_cooled_liquid_water:kgm2',\n",
    "        'ceiling_height_agl:m',\n",
    "        'cloud_base_agl:m'\n",
    "    ]\n",
    "\n",
    "    # Bin the columns\n",
    "    # df = bin_columns(df, columns_to_bin)\n",
    "    df = bin_columns(df, ['effective_cloud_cover:p'], n_bins=2)\n",
    "    df = bin_columns(df, ['ceiling_height_agl:m'], n_bins=3)\n",
    "    df = bin_columns(df, ['average_wind_speed'], n_bins=5)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_rate_of_change_features(df, features, second_order=False):\n",
    "    \"\"\"\n",
    "    Adds rate of change columns for specified features in the dataframe.\n",
    "    Assumes the dataframe is time sorted. If second_order is True, it also adds the second order rate of change.\n",
    "    \"\"\"\n",
    "    for feature in features:\n",
    "        rate_column_name = feature + '_rate_of_change'\n",
    "        df[rate_column_name] = df[feature].diff().fillna(0)  # Handle the first diff NaN if required\n",
    "        \n",
    "        if second_order:  # Check if second order difference is required\n",
    "            second_order_column_name = feature + '_rate_of_change_of_change'\n",
    "            df[second_order_column_name] = df[rate_column_name].diff().fillna(0)  # Second order difference\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_rate_of_change_features_to_df(df):\n",
    "    # Define the features for which to calculate rate of change\n",
    "    features_to_diff = [\n",
    "        't_1000hPa:K',\n",
    "        'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad:W',\n",
    "        'effective_cloud_cover:p', 'total_radiation:W'\n",
    "    ]\n",
    "\n",
    "    # Add rate of change features\n",
    "    return add_rate_of_change_features(df, features_to_diff, second_order=False)\n",
    "\n",
    "def add_est_obs_feature(df):\n",
    "    \"\"\"\n",
    "    Adds a column to the dataframe that indicates whether the data is estimated or observed.\n",
    "    \"\"\"\n",
    "    # Add the est_obs feature\n",
    "    if 'date_calc' not in df.columns:\n",
    "        # If 'date_calc' does not exist, create 'observed' column and set to 1\n",
    "        df['observed'] = 1\n",
    "        return df\n",
    "    else:\n",
    "        # If 'date_calc' exists, create a new column and set values to 0\n",
    "        df['observed'] = 0\n",
    "        return df.drop(columns=['date_calc'])\n",
    "    \n",
    "def remove_constant_regions(dataframe, column_name=\"pv_measurement\", threshold=72):\n",
    "    \"\"\"\n",
    "    Removes rows where the specified column has constant values for more than the given threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the specified column exists in the dataframe\n",
    "    if column_name not in dataframe.columns:\n",
    "        print(f\"Warning: '{column_name}' column not found in the dataframe. No rows removed.\")\n",
    "        return dataframe\n",
    "    \n",
    "    same_as_previous = dataframe[column_name].eq(dataframe[column_name].shift())\n",
    "    group_ids = (~same_as_previous).cumsum()\n",
    "    to_remove = group_ids[same_as_previous].value_counts() > threshold\n",
    "    group_ids_to_remove = to_remove[to_remove].index\n",
    "    \n",
    "    # Drop entire rows that match the conditions\n",
    "    return dataframe.drop(dataframe[group_ids.isin(group_ids_to_remove)].index)\n",
    "\n",
    "def add_lagged_features(df, features_with__lags, fill_value=None):\n",
    "    \"\"\"\n",
    "    Adds lagged columns for specified features in the dataframe with specific lag periods.\n",
    "    'features_with_specific_lags' is a dictionary with features as keys and specific lag as values.\n",
    "    'fill_value' is what to fill the NaNs with, after shifting.\n",
    "    \"\"\"\n",
    "    for feature, specific_lag in features_with__lags.items():\n",
    "        lag_column_name = f\"{feature}_lag_{specific_lag}\"\n",
    "        df[lag_column_name] = df[feature].shift(specific_lag).fillna(fill_value)\n",
    "    return df\n",
    "\n",
    "def add_lagged_features_to_df(df):\n",
    "    features_with_lags = {\n",
    "        'total_radiation:W': 1,\n",
    "        'total_radiation:W': -1,\n",
    "        'rad_diff:W': 1,\n",
    "        'rad_diff:W': -1,\n",
    "        'total_radiation_1h:J': 1,\n",
    "        'total_radiation_1h:J': -1\n",
    "    }\n",
    "\n",
    "    # Add lagged features for specific lags\n",
    "    return add_lagged_features(df, features_with_lags, fill_value=0)\n",
    "\n",
    "def handle_nan(df):\n",
    "    # Remove the rows where target is nan\n",
    "    try:\n",
    "        df = df[df['pv_measurement'].notna()]\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    # Set all remaining nans to 0\n",
    "    return df.fillna(0)\n",
    "\n",
    "# Aggregate the data to hourly with some aggregation methods for each column\n",
    "aggregation_methods = {\n",
    "    'date_forecast': 'first',\n",
    "    'diffuse_rad:W': 'sum',\n",
    "    'direct_rad:W': 'last',\n",
    "    'clear_sky_rad:W': 'sum',\n",
    "    'diffuse_rad_1h:J': 'last',\n",
    "    'direct_rad_1h:J': 'last',\n",
    "    'clear_sky_energy_1h:J': 'last',\n",
    "    'absolute_humidity_2m:gm3': 'mean',\n",
    "    'air_density_2m:kgm3': 'mean',\n",
    "    'ceiling_height_agl:m': 'max',\n",
    "    'cloud_base_agl:m': 'mean',\n",
    "    'dew_or_rime:idx': 'min',\n",
    "    'dew_point_2m:K': 'mean',\n",
    "    'effective_cloud_cover:p': 'sum',\n",
    "    'elevation:m': 'first',\n",
    "    'fresh_snow_12h:cm': 'max',\n",
    "    'fresh_snow_1h:cm': 'sum',\n",
    "    'fresh_snow_24h:cm': 'max',\n",
    "    'fresh_snow_3h:cm': 'max',\n",
    "    'fresh_snow_6h:cm': 'max',\n",
    "    'is_day:idx': 'max',\n",
    "    'is_in_shadow:idx': 'max',\n",
    "    'msl_pressure:hPa': 'mean',\n",
    "    'precip_5min:mm': 'sum',\n",
    "    'precip_type_5min:idx': 'sum',\n",
    "    'pressure_100m:hPa': 'mean',\n",
    "    'pressure_50m:hPa': 'mean',\n",
    "    'prob_rime:p': 'max',\n",
    "    'rain_water:kgm2': 'sum',\n",
    "    'relative_humidity_1000hPa:p': 'mean',\n",
    "    'sfc_pressure:hPa': 'mean',\n",
    "    'snow_density:kgm3': 'mean',\n",
    "    'snow_depth:cm': 'max',\n",
    "    'snow_drift:idx': 'max',\n",
    "    'snow_melt_10min:mm': 'sum',\n",
    "    'snow_water:kgm2': 'sum',\n",
    "    'sun_azimuth:d': 'first',\n",
    "    'sun_elevation:d': 'sum',\n",
    "    'super_cooled_liquid_water:kgm2': 'sum',\n",
    "    't_1000hPa:K': 'mean',\n",
    "    'total_cloud_cover:p': 'mean',\n",
    "    'visibility:m': 'mean',\n",
    "    'wind_speed_10m:ms': 'mean',\n",
    "    'wind_speed_u_10m:ms': 'mean',\n",
    "    'wind_speed_v_10m:ms': 'mean',\n",
    "    'wind_speed_w_1000hPa:ms': 'mean',\n",
    "    'cloud_base_agl:m': 'max',\n",
    "    'snow_density:kgm3': 'mean'\n",
    "}\n",
    "\n",
    "# Read in the data\n",
    "x_target_A = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "x_train_obs_A = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "x_train_est_A = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "x_test_est_A = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "x_target_B = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "x_train_obs_B = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "x_train_est_B = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "x_test_est_B = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "x_target_C = pd.read_parquet('./data/C/train_targets.parquet')\n",
    "x_train_obs_C = pd.read_parquet('./data/C/X_train_observed.parquet')\n",
    "x_train_est_C = pd.read_parquet('./data/C/X_train_estimated.parquet')\n",
    "x_test_est_C = pd.read_parquet('./data/C/X_test_estimated.parquet')\n",
    "\n",
    "# Rename time to date_forecast in target\n",
    "x_target_A.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "x_target_B.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "x_target_C.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "\n",
    "# Fix missing data for test set. Assumin NaN means 0 in these categories\n",
    "x_test_est_A['effective_cloud_cover:p'] = x_test_est_A['effective_cloud_cover:p'].fillna(0)\n",
    "x_test_est_B['effective_cloud_cover:p'] = x_test_est_B['effective_cloud_cover:p'].fillna(0)\n",
    "x_test_est_C['effective_cloud_cover:p'] = x_test_est_C['effective_cloud_cover:p'].fillna(0)\n",
    "\n",
    "x_test_est_A['total_cloud_cover:p'] = x_test_est_A['total_cloud_cover:p'].fillna(0)\n",
    "x_test_est_B['total_cloud_cover:p'] = x_test_est_B['total_cloud_cover:p'].fillna(0)\n",
    "x_test_est_C['total_cloud_cover:p'] = x_test_est_C['total_cloud_cover:p'].fillna(0)\n",
    "\n",
    "x_test_est_A['cloud_base_agl:m'] = x_test_est_A['cloud_base_agl:m'].fillna(0)\n",
    "x_test_est_B['cloud_base_agl:m'] = x_test_est_B['cloud_base_agl:m'].fillna(0)\n",
    "x_test_est_C['cloud_base_agl:m'] = x_test_est_C['cloud_base_agl:m'].fillna(0)\n",
    "\n",
    "x_test_est_A['ceiling_height_agl:m'] = x_test_est_A['ceiling_height_agl:m'].fillna(0)\n",
    "x_test_est_B['ceiling_height_agl:m'] = x_test_est_B['ceiling_height_agl:m'].fillna(0)\n",
    "x_test_est_C['ceiling_height_agl:m'] = x_test_est_C['ceiling_height_agl:m'].fillna(0)\n",
    "\n",
    "x_test_est_A['snow_density:kgm3'] = x_test_est_A['snow_density:kgm3'].fillna(0)\n",
    "x_test_est_B['snow_density:kgm3'] = x_test_est_B['snow_density:kgm3'].fillna(0)\n",
    "x_test_est_C['snow_density:kgm3'] = x_test_est_C['snow_density:kgm3'].fillna(0)\n",
    "\n",
    "x_test_est_A['snow_drift:idx'] = x_test_est_A['snow_drift:idx'].fillna(0)\n",
    "x_test_est_B['snow_drift:idx'] = x_test_est_B['snow_drift:idx'].fillna(0)\n",
    "x_test_est_C['snow_drift:idx'] = x_test_est_C['snow_drift:idx'].fillna(0)\n",
    "\n",
    "# Resample\n",
    "x_train_obs_A_resampled = x_train_obs_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_A_resampled = x_train_est_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_A_resampled = x_test_est_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "x_train_obs_B_resampled = x_train_obs_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_B_resampled = x_train_est_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_B_resampled = x_test_est_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "x_train_obs_C_resampled = x_train_obs_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_C_resampled = x_train_est_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_C_resampled = x_test_est_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "# Merge\n",
    "split_value = x_train_est_A['date_forecast'].iloc[0]\n",
    "split_index = x_target_A[x_target_A['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_A = x_target_A.iloc[:split_index]\n",
    "x_target_est_A = x_target_A.iloc[split_index:]\n",
    "\n",
    "obs_A = x_train_obs_A_resampled.merge(x_target_obs_A, left_index=True, right_on='date_forecast')\n",
    "est_A = x_train_est_A_resampled.merge(x_target_est_A, left_index=True, right_on='date_forecast')\n",
    "\n",
    "split_value = x_train_est_B['date_forecast'].iloc[0]\n",
    "split_index = x_target_B[x_target_B['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_B = x_target_B.iloc[:split_index]\n",
    "x_target_est_B = x_target_B.iloc[split_index:]\n",
    "\n",
    "obs_B = x_train_obs_B_resampled.merge(x_target_obs_B, left_index=True, right_on='date_forecast')\n",
    "est_B = x_train_est_B_resampled.merge(x_target_est_B, left_index=True, right_on='date_forecast')\n",
    "\n",
    "split_value = x_train_est_C['date_forecast'].iloc[0]\n",
    "split_index = x_target_C[x_target_C['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_C = x_target_C.iloc[:split_index]\n",
    "x_target_est_C = x_target_C.iloc[split_index:]\n",
    "\n",
    "obs_C = x_train_obs_C_resampled.merge(x_target_obs_C, left_index=True, right_on='date_forecast')\n",
    "est_C = x_train_est_C_resampled.merge(x_target_est_C, left_index=True, right_on='date_forecast')\n",
    "\n",
    "# Keep date_forecast in test dfs\n",
    "test_A = x_test_est_A_resampled\n",
    "test_B = x_test_est_B_resampled\n",
    "test_C = x_test_est_C_resampled\n",
    "\n",
    "# Drop all the NaNs\n",
    "test_A = test_A.dropna()\n",
    "test_B = test_B.dropna()\n",
    "test_C = test_C.dropna()\n",
    "\n",
    "def preprocessing(df):\n",
    "    df = add_experimental_features(df.copy())\n",
    "    df = add_date_features(df.copy())\n",
    "    df = add_binned_features(df.copy())\n",
    "    df = add_rate_of_change_features_to_df(df.copy())\n",
    "    df = add_est_obs_feature(df.copy())\n",
    "    df = remove_constant_regions(df.copy())\n",
    "    df = add_lagged_features_to_df(df.copy())\n",
    "    df = handle_nan(df.copy())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Preprocess\n",
    "obs_A = preprocessing(obs_A.copy())\n",
    "est_A = preprocessing(est_A.copy())\n",
    "test_A = preprocessing(test_A.copy())\n",
    "\n",
    "obs_B = preprocessing(obs_B.copy())\n",
    "est_B = preprocessing(est_B.copy())\n",
    "test_B = preprocessing(test_B.copy())\n",
    "\n",
    "obs_C = preprocessing(obs_C.copy())\n",
    "est_C = preprocessing(est_C.copy())\n",
    "test_C = preprocessing(test_C.copy())\n",
    "\n",
    "# Random seeds used for reproducibility\n",
    "# 32 weights: 0.3, 0.3, 0.4\n",
    "# 24 weights: 0.3, 0.3, 0.4\n",
    "# 33 (without winter months 1 and 12) weights: 0.2, 0.4, 0.4\n",
    "# 11 (without winter months 1, 2 and 11, 12) weights: 0.25, 0.35, 0.4\n",
    "# 5 weights: 0.4, 0.3, 0.3\n",
    "\n",
    "# Best score is the mean prediction of all the 5 seeds mentioned above. The first weight is xgboost, the second is catboost, and the third is autogluon.\n",
    "\n",
    "# Set the random seed\n",
    "np.random.seed(5)\n",
    "\n",
    "# Concatinate\n",
    "A = pd.concat([obs_A, est_A])\n",
    "B = pd.concat([obs_B, est_B])\n",
    "C = pd.concat([obs_C, est_C])\n",
    "\n",
    "# Remove characters unparseable for CatBoost \n",
    "A.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in A.columns]\n",
    "B.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in B.columns]\n",
    "C.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in C.columns]\n",
    "\n",
    "test_A.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_A.columns]\n",
    "test_B.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_B.columns]\n",
    "test_C.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_C.columns]\n",
    "\n",
    "# Getting validation data from summer months, because the test set is from summer months. We experimentet with excluding winter months\n",
    "# from the training data here.\n",
    "\n",
    "# Step 1: Filter A to include only months from March to October\n",
    "A = A[A['date_forecast'].dt.month.isin([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])]\n",
    "\n",
    "# Step 2: Identify unique days within May, June, and July\n",
    "summer_months = A[A['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "\n",
    "# Step 3: Sample these days for val_A\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_A = A[A['date_forecast'].dt.date.isin(sampled_days)]\n",
    "\n",
    "# Step 4: Define train_A as the remaining data\n",
    "train_A = A[~A['date_forecast'].dt.date.isin(sampled_days)]\n",
    "\n",
    "# Prepare your features and target variables\n",
    "X_train_A = train_A.drop(columns='pv_measurement')\n",
    "y_train_A = train_A['pv_measurement']\n",
    "X_val_A = val_A.drop(columns='pv_measurement')\n",
    "y_val_A = val_A['pv_measurement']\n",
    "\n",
    "# Repeat for B and C\n",
    "B = B[B['date_forecast'].dt.month.isin([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])]\n",
    "summer_months = B[B['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_B = B[B['date_forecast'].dt.date.isin(sampled_days)]\n",
    "train_B = B[~B['date_forecast'].dt.date.isin(sampled_days)]\n",
    "X_train_B = train_B.drop(columns='pv_measurement')\n",
    "y_train_B = train_B['pv_measurement']\n",
    "X_val_B = val_B.drop(columns='pv_measurement')\n",
    "y_val_B = val_B['pv_measurement']\n",
    "\n",
    "C = C[C['date_forecast'].dt.month.isin([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])]\n",
    "summer_months = C[C['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_C = C[C['date_forecast'].dt.date.isin(sampled_days)]\n",
    "train_C = C[~C['date_forecast'].dt.date.isin(sampled_days)]\n",
    "X_train_C = train_C.drop(columns='pv_measurement')\n",
    "y_train_C = train_C['pv_measurement']\n",
    "X_val_C = val_C.drop(columns='pv_measurement')\n",
    "y_val_C = val_C['pv_measurement']\n",
    "\n",
    "# Drop date_forecast\n",
    "train_A = train_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "train_B = train_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "train_C = train_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_A = val_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_B = val_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_C = val_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_A = X_train_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_B = X_train_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_C = X_train_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_A = X_val_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_B = X_val_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_C = X_val_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "test_A = test_A.drop(columns=['date_forecast'])\n",
    "test_B = test_B.drop(columns=['date_forecast'])\n",
    "test_C = test_C.drop(columns=['date_forecast'])\n",
    "\n",
    "train_auto_A = TabularDataset(train_A)\n",
    "val_auto_A = TabularDataset(val_A)\n",
    "\n",
    "train_auto_B = TabularDataset(train_B)\n",
    "val_auto_B = TabularDataset(val_B)\n",
    "\n",
    "train_auto_C = TabularDataset(train_C)\n",
    "val_auto_C = TabularDataset(val_C)\n",
    "\n",
    "auto_label = 'pv_measurement'\n",
    "\n",
    "# Set the parameters for the XGBoost models\n",
    "params_xgb_A = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.4, \n",
    "    'learning_rate': 0.012, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 0.8, \n",
    "    'reg_lambda': 0.8, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1,\n",
    "    'num_parallel_tree': 2\n",
    "}\n",
    "\n",
    "params_xgb_B = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.8, \n",
    "    'learning_rate': 0.008, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 1, \n",
    "    'reg_lambda': 3, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "params_xgb_C = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.8, \n",
    "    'learning_rate': 0.008, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 1, \n",
    "    'reg_lambda': 3, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "xgb_A = xgb.XGBRegressor(**params_xgb_A)\n",
    "xgb_B = xgb.XGBRegressor(**params_xgb_B)\n",
    "xgb_C = xgb.XGBRegressor(**params_xgb_C)\n",
    "\n",
    "cat_A = CatBoostRegressor(\n",
    "    iterations=5000,         # The number of trees to build\n",
    "    #learning_rate=0.09,     # The learning rate\n",
    "    #depth=10,               # Depth of the tree\n",
    "    loss_function='MAE',     # Loss function to be optimized. RMSE is common for regression.\n",
    "    eval_metric='MAE',       # Evaluation metric for the validation set\n",
    "    #random_seed=42,         # Seed for reproducibility\n",
    "    #verbose=100             # Frequency of logging the training process\n",
    ")\n",
    "\n",
    "cat_B = CatBoostRegressor(\n",
    "    iterations=5000,\n",
    "    #learning_rate=0.09,\n",
    "    #depth=10,\n",
    "    loss_function='MAE',\n",
    "    eval_metric='MAE',\n",
    "    #random_seed=42,\n",
    "    #verbose=100\n",
    ")\n",
    "\n",
    "cat_C = CatBoostRegressor(\n",
    "    iterations=5000,\n",
    "    #learning_rate=0.09,\n",
    "    #depth=10,\n",
    "    loss_function='MAE',\n",
    "    eval_metric='MAE',\n",
    "    #random_seed=42,\n",
    "    #verbose=100\n",
    ")\n",
    "\n",
    "# Prepare data for the XGBoost models. We got them to work the best when having fewer columns\n",
    "xgb_columns = [\n",
    "    'total_radiationW',\n",
    "    'snow_accumulation',\n",
    "    'super_cooled_liquid_waterkgm2',\n",
    "    'average_wind_speed',\n",
    "    'sun_elevationd',\n",
    "    'sun_azimuthd',\n",
    "    'clear_sky_radW',\n",
    "    'month',\n",
    "    't_1000hPaC',\n",
    "    'msl_pressurehPa_scaled',\n",
    "    'rain_waterkgm2',\n",
    "    'cloud_base_aglm',\n",
    "    'effective_cloud_coverp',\n",
    "    'dew_or_rimeidx'\n",
    "]\n",
    "print(train_A.columns)\n",
    "\n",
    "X_train_xgb_A = train_A[xgb_columns]\n",
    "y_train_xgb_A = train_A['pv_measurement']\n",
    "X_test_xgb_A = test_A[xgb_columns]\n",
    "\n",
    "X_train_xgb_B = train_B[xgb_columns]\n",
    "y_train_xgb_B = train_B['pv_measurement']\n",
    "X_test_xgb_B = test_B[xgb_columns]\n",
    "\n",
    "X_train_xgb_C = train_C[xgb_columns]\n",
    "y_train_xgb_C = train_C['pv_measurement']\n",
    "X_test_xgb_C = test_C[xgb_columns]\n",
    "\n",
    "# Train the XGBoost models\n",
    "xgb_A.fit(\n",
    "    X=X_train_xgb_A, y=y_train_xgb_A,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "xgb_B.fit(\n",
    "    X=X_train_xgb_B, y=y_train_xgb_B,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "xgb_C.fit(\n",
    "    X=X_train_xgb_C, y=y_train_xgb_C,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Train the CatBoost models\n",
    "cat_A.fit(\n",
    "    X_train_A, y_train_A,\n",
    "    eval_set=(X_val_A, y_val_A),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "cat_B.fit(\n",
    "    X_train_B, y_train_B,\n",
    "    eval_set=(X_val_B, y_val_B),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "cat_C.fit(\n",
    "    X_train_C, y_train_C,\n",
    "    eval_set=(X_val_C, y_val_C),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "auto_A = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_A, \n",
    "                                                                                   presets='medium_quality', \n",
    "                                                                                   tuning_data=val_auto_A, \n",
    "                                                                                   use_bag_holdout=True, \n",
    "                                                                                   ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "auto_B = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_B,\n",
    "                                                                                      presets='medium_quality',\n",
    "                                                                                      tuning_data=val_auto_B,\n",
    "                                                                                      use_bag_holdout=True,\n",
    "                                                                                      ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "auto_C = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_C,\n",
    "                                                                                        presets='medium_quality',\n",
    "                                                                                        tuning_data=val_auto_C,\n",
    "                                                                                        use_bag_holdout=True,\n",
    "                                                                                        ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "xgb_weight = 0.4\n",
    "cat_weight = 0.3\n",
    "auto_weight = 0.3\n",
    "\n",
    "pred_xgb_A = xgb_A.predict(X_test_xgb_C)\n",
    "pred_xgb_B = xgb_B.predict(X_test_xgb_C)\n",
    "pred_xgb_C = xgb_C.predict(X_test_xgb_C)\n",
    "\n",
    "pred_auto_A = auto_A.predict(test_A)\n",
    "pred_auto_B = auto_B.predict(test_B)\n",
    "pred_auto_C = auto_C.predict(test_C)\n",
    "\n",
    "pred_cat_A = cat_A.predict(test_A)\n",
    "pred_cat_B = cat_B.predict(test_B)\n",
    "pred_cat_C = cat_C.predict(test_C)\n",
    "\n",
    "# Ensemble that seemed the best after some experimentation\n",
    "pred_A = (pred_xgb_A*xgb_weight + pred_cat_A*cat_weight + pred_auto_A*auto_weight)\n",
    "pred_B = (pred_xgb_B*xgb_weight + pred_cat_B*cat_weight + pred_auto_B*auto_weight)\n",
    "pred_C = (pred_xgb_C*xgb_weight + pred_cat_C*cat_weight + pred_auto_C*auto_weight)\n",
    "\n",
    "pred_A = np.clip(pred_A, 0, None)\n",
    "pred_B = np.clip(pred_B, 0, None)\n",
    "pred_C = np.clip(pred_C, 0, None)\n",
    "\n",
    "# Concatenate predictions\n",
    "predictions = np.concatenate([pred_A, pred_B, pred_C])\n",
    "\n",
    "# Save predictions\n",
    "predictions_5 = predictions\n",
    "\n",
    "# Create an id array\n",
    "ids = np.arange(0, len(predictions_5))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'prediction': predictions_5\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('predictions_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence 6\n",
    "\n",
    "%reset -f\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostRegressor\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def add_experimental_features(df):\n",
    "    \"\"\"\n",
    "    Experimental feature engineering.\n",
    "    \"\"\"\n",
    "\n",
    "    # Radiation Features\n",
    "    df['total_radiation:W'] = df['direct_rad:W'] + df['diffuse_rad:W']\n",
    "    df['total_radiation_1h:J'] = df['direct_rad_1h:J'] + df['diffuse_rad_1h:J']\n",
    "    df['rad_diff:W'] = df['direct_rad:W'] - df['diffuse_rad:W']\n",
    "    df['rad_diff_1h:J'] = df['direct_rad_1h:J'] - df['diffuse_rad_1h:J']\n",
    "    df['diffuse_direct_ratio'] = df['diffuse_rad:W'] / df['direct_rad:W']\n",
    "\n",
    "    # Temperature and Pressure Features\n",
    "    df['temp_dewpoint_diff'] = df['t_1000hPa:K'] - df['dew_point_2m:K']\n",
    "    df['pressure_gradient'] = df['pressure_100m:hPa'] - df['pressure_50m:hPa']\n",
    "    df['t_1000hPa:C'] = df['t_1000hPa:K'] - 273.15\n",
    "    df['dew_point_2m:C'] = df['dew_point_2m:K'] - 273.15\n",
    "    df['msl_pressure:hPa_scaled'] = MinMaxScaler().fit_transform(df['msl_pressure:hPa'].values.reshape(-1, 1))\n",
    "    df['sfc_pressure:hPa_scaled'] = MinMaxScaler().fit_transform(df['sfc_pressure:hPa'].values.reshape(-1, 1))\n",
    "\n",
    "    # Wind Features\n",
    "    df['wind_vector_magnitude'] = (df['wind_speed_u_10m:ms']**2 + df['wind_speed_v_10m:ms']**2 + df['wind_speed_w_1000hPa:ms']**2)**0.5\n",
    "    df['average_wind_speed'] = (df['wind_speed_10m:ms'] + df['wind_speed_u_10m:ms']) / 2\n",
    "\n",
    "    # Cloud and Snow Features\n",
    "    df['cloud_humidity_product'] = df['total_cloud_cover:p'] * df['absolute_humidity_2m:gm3']\n",
    "    df['snow_accumulation'] = df[['fresh_snow_24h:cm', 'fresh_snow_12h:cm', 'fresh_snow_6h:cm', 'fresh_snow_3h:cm', 'fresh_snow_1h:cm']].sum(axis=1)\n",
    "\n",
    "    # Interaction between radiation and cloud cover\n",
    "    df['radiation_cloud_interaction'] = df['direct_rad:W'] * df['effective_cloud_cover:p']\n",
    "\n",
    "    # Interaction between temperature and radiation (considering that high temperature may reduce efficiency)\n",
    "    df['temp_rad_interaction'] = df['t_1000hPa:K'] * df['total_radiation:W']\n",
    "\n",
    "    # Interaction between wind cooling effect and temperature\n",
    "    df['wind_temp_interaction'] = df['average_wind_speed'] * df['t_1000hPa:K']\n",
    "\n",
    "    # Interaction between humidity and temperature\n",
    "    df['humidity_temp_interaction'] = df['absolute_humidity_2m:gm3'] * df['t_1000hPa:K']\n",
    "\n",
    "    # Interaction between humidity and radiation\n",
    "    df['sun_elevation_direct_rad_interaction'] = df['sun_elevation:d'] * df['direct_rad:W']\n",
    "\n",
    "    # Precipitation Features\n",
    "    df['precip'] = df['precip_5min:mm']*df['precip_type_5min:idx']\n",
    "\n",
    "    # Safeguard in case of inf values\n",
    "    df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_date_features(df):\n",
    "    \"\"\"\n",
    "    Adds 'month', 'year', 'hour' and 'day' columns to the dataframe based on the 'date_forecast' column.\n",
    "    Also adds 'hour_sin' and 'hour_cos' columns for the hour of the day.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if 'date_forecast' exists in the dataframe\n",
    "    if 'date_forecast' in df.columns:\n",
    "        # Convert the 'date_forecast' column to datetime format\n",
    "        df['date_forecast'] = pd.to_datetime(df['date_forecast'])\n",
    "        \n",
    "        # Extract month, year, hour and day\n",
    "        df['month'] = df['date_forecast'].dt.month\n",
    "        df['year'] = df['date_forecast'].dt.year\n",
    "        df['hour'] = df['date_forecast'].dt.hour\n",
    "        df['day'] = df['date_forecast'].dt.day\n",
    "        df['hour_sin'] = np.sin(2*np.pi*df['hour']/24)\n",
    "        df['hour_cos'] = np.cos(2*np.pi*df['hour']/24)\n",
    "\n",
    "    else:\n",
    "        print(\"Warning: 'date_forecast' column not found in the dataframe. No date features added.\")\n",
    "        return df  # Keep the 'date_forecast' column in the dataframe\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Adding discretized features for the continuous variables to help tree-based models\n",
    "\n",
    "def bin_columns(dataframe, columns_to_bin, n_bins=5):\n",
    "    \"\"\"\n",
    "    Bins the specified columns of the dataframe into equal-sized bins.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframe: pd.DataFrame\n",
    "    - columns_to_bin: list of strings, the names of the columns to bin\n",
    "    - n_bins: int or dict, the number of bins for each column (if int, use the same number for all columns;\n",
    "              if dict, specify individual numbers with column names as keys)\n",
    "    \n",
    "    Returns:\n",
    "    - binned_dataframe: pd.DataFrame, the dataframe with the specified columns binned\n",
    "    \"\"\"\n",
    "    binned_dataframe = dataframe.copy()\n",
    "    \n",
    "    for column in columns_to_bin:\n",
    "        # Determine the number of bins for this column\n",
    "        bins = n_bins if isinstance(n_bins, int) else n_bins.get(column, 5)\n",
    "        \n",
    "        # Create quantile-based bins\n",
    "        binned_dataframe[f'binned_{column}'] = pd.qcut(\n",
    "            binned_dataframe[column],\n",
    "            q=bins,\n",
    "            labels=False,\n",
    "            duplicates='drop'\n",
    "        )\n",
    "        \n",
    "    return binned_dataframe\n",
    "\n",
    "def add_binned_features(df):\n",
    "    columns_to_bin = [\n",
    "        'super_cooled_liquid_water:kgm2',\n",
    "        'ceiling_height_agl:m',\n",
    "        'cloud_base_agl:m'\n",
    "    ]\n",
    "\n",
    "    # Bin the columns\n",
    "    # df = bin_columns(df, columns_to_bin)\n",
    "    df = bin_columns(df, ['effective_cloud_cover:p'], n_bins=2)\n",
    "    df = bin_columns(df, ['ceiling_height_agl:m'], n_bins=3)\n",
    "    df = bin_columns(df, ['average_wind_speed'], n_bins=5)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_rate_of_change_features(df, features, second_order=False):\n",
    "    \"\"\"\n",
    "    Adds rate of change columns for specified features in the dataframe.\n",
    "    Assumes the dataframe is time sorted. If second_order is True, it also adds the second order rate of change.\n",
    "    \"\"\"\n",
    "    for feature in features:\n",
    "        rate_column_name = feature + '_rate_of_change'\n",
    "        df[rate_column_name] = df[feature].diff().fillna(0)  # Handle the first diff NaN if required\n",
    "        \n",
    "        if second_order:  # Check if second order difference is required\n",
    "            second_order_column_name = feature + '_rate_of_change_of_change'\n",
    "            df[second_order_column_name] = df[rate_column_name].diff().fillna(0)  # Second order difference\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_rate_of_change_features_to_df(df):\n",
    "    # Define the features for which to calculate rate of change\n",
    "    features_to_diff = [\n",
    "        't_1000hPa:K',\n",
    "        'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad:W',\n",
    "        'effective_cloud_cover:p', 'total_radiation:W'\n",
    "    ]\n",
    "\n",
    "    # Add rate of change features\n",
    "    return add_rate_of_change_features(df, features_to_diff, second_order=False)\n",
    "\n",
    "def add_est_obs_feature(df):\n",
    "    \"\"\"\n",
    "    Adds a column to the dataframe that indicates whether the data is estimated or observed.\n",
    "    \"\"\"\n",
    "    # Add the est_obs feature\n",
    "    if 'date_calc' not in df.columns:\n",
    "        # If 'date_calc' does not exist, create 'observed' column and set to 1\n",
    "        df['observed'] = 1\n",
    "        return df\n",
    "    else:\n",
    "        # If 'date_calc' exists, create a new column and set values to 0\n",
    "        df['observed'] = 0\n",
    "        return df.drop(columns=['date_calc'])\n",
    "    \n",
    "def remove_constant_regions(dataframe, column_name=\"pv_measurement\", threshold=72):\n",
    "    \"\"\"\n",
    "    Removes rows where the specified column has constant values for more than the given threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the specified column exists in the dataframe\n",
    "    if column_name not in dataframe.columns:\n",
    "        print(f\"Warning: '{column_name}' column not found in the dataframe. No rows removed.\")\n",
    "        return dataframe\n",
    "    \n",
    "    same_as_previous = dataframe[column_name].eq(dataframe[column_name].shift())\n",
    "    group_ids = (~same_as_previous).cumsum()\n",
    "    to_remove = group_ids[same_as_previous].value_counts() > threshold\n",
    "    group_ids_to_remove = to_remove[to_remove].index\n",
    "    \n",
    "    # Drop entire rows that match the conditions\n",
    "    return dataframe.drop(dataframe[group_ids.isin(group_ids_to_remove)].index)\n",
    "\n",
    "def add_lagged_features(df, features_with__lags, fill_value=None):\n",
    "    \"\"\"\n",
    "    Adds lagged columns for specified features in the dataframe with specific lag periods.\n",
    "    'features_with_specific_lags' is a dictionary with features as keys and specific lag as values.\n",
    "    'fill_value' is what to fill the NaNs with, after shifting.\n",
    "    \"\"\"\n",
    "    for feature, specific_lag in features_with__lags.items():\n",
    "        lag_column_name = f\"{feature}_lag_{specific_lag}\"\n",
    "        df[lag_column_name] = df[feature].shift(specific_lag).fillna(fill_value)\n",
    "    return df\n",
    "\n",
    "def add_lagged_features_to_df(df):\n",
    "    features_with_lags = {\n",
    "        'total_radiation:W': 1,\n",
    "        'total_radiation:W': -1,\n",
    "        'rad_diff:W': 1,\n",
    "        'rad_diff:W': -1,\n",
    "        'total_radiation_1h:J': 1,\n",
    "        'total_radiation_1h:J': -1\n",
    "    }\n",
    "\n",
    "    # Add lagged features for specific lags\n",
    "    return add_lagged_features(df, features_with_lags, fill_value=0)\n",
    "\n",
    "def handle_nan(df):\n",
    "    # Remove the rows where target is nan\n",
    "    try:\n",
    "        df = df[df['pv_measurement'].notna()]\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    # Set all remaining nans to 0\n",
    "    return df.fillna(0)\n",
    "\n",
    "# Aggregate the data to hourly with some aggregation methods for each column\n",
    "aggregation_methods = {\n",
    "    'date_forecast': 'first',\n",
    "    'diffuse_rad:W': 'sum',\n",
    "    'direct_rad:W': 'last',\n",
    "    'clear_sky_rad:W': 'sum',\n",
    "    'diffuse_rad_1h:J': 'last',\n",
    "    'direct_rad_1h:J': 'last',\n",
    "    'clear_sky_energy_1h:J': 'last',\n",
    "    'absolute_humidity_2m:gm3': 'mean',\n",
    "    'air_density_2m:kgm3': 'mean',\n",
    "    'ceiling_height_agl:m': 'max',\n",
    "    'cloud_base_agl:m': 'mean',\n",
    "    'dew_or_rime:idx': 'min',\n",
    "    'dew_point_2m:K': 'mean',\n",
    "    'effective_cloud_cover:p': 'sum',\n",
    "    'elevation:m': 'first',\n",
    "    'fresh_snow_12h:cm': 'max',\n",
    "    'fresh_snow_1h:cm': 'sum',\n",
    "    'fresh_snow_24h:cm': 'max',\n",
    "    'fresh_snow_3h:cm': 'max',\n",
    "    'fresh_snow_6h:cm': 'max',\n",
    "    'is_day:idx': 'max',\n",
    "    'is_in_shadow:idx': 'max',\n",
    "    'msl_pressure:hPa': 'mean',\n",
    "    'precip_5min:mm': 'sum',\n",
    "    'precip_type_5min:idx': 'sum',\n",
    "    'pressure_100m:hPa': 'mean',\n",
    "    'pressure_50m:hPa': 'mean',\n",
    "    'prob_rime:p': 'max',\n",
    "    'rain_water:kgm2': 'sum',\n",
    "    'relative_humidity_1000hPa:p': 'mean',\n",
    "    'sfc_pressure:hPa': 'mean',\n",
    "    'snow_density:kgm3': 'mean',\n",
    "    'snow_depth:cm': 'max',\n",
    "    'snow_drift:idx': 'max',\n",
    "    'snow_melt_10min:mm': 'sum',\n",
    "    'snow_water:kgm2': 'sum',\n",
    "    'sun_azimuth:d': 'first',\n",
    "    'sun_elevation:d': 'sum',\n",
    "    'super_cooled_liquid_water:kgm2': 'sum',\n",
    "    't_1000hPa:K': 'mean',\n",
    "    'total_cloud_cover:p': 'mean',\n",
    "    'visibility:m': 'mean',\n",
    "    'wind_speed_10m:ms': 'mean',\n",
    "    'wind_speed_u_10m:ms': 'mean',\n",
    "    'wind_speed_v_10m:ms': 'mean',\n",
    "    'wind_speed_w_1000hPa:ms': 'mean',\n",
    "    'cloud_base_agl:m': 'max',\n",
    "    'snow_density:kgm3': 'mean'\n",
    "}\n",
    "\n",
    "# Read in the data\n",
    "x_target_A = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "x_train_obs_A = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "x_train_est_A = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "x_test_est_A = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "x_target_B = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "x_train_obs_B = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "x_train_est_B = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "x_test_est_B = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "x_target_C = pd.read_parquet('./data/C/train_targets.parquet')\n",
    "x_train_obs_C = pd.read_parquet('./data/C/X_train_observed.parquet')\n",
    "x_train_est_C = pd.read_parquet('./data/C/X_train_estimated.parquet')\n",
    "x_test_est_C = pd.read_parquet('./data/C/X_test_estimated.parquet')\n",
    "\n",
    "# Rename time to date_forecast in target\n",
    "x_target_A.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "x_target_B.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "x_target_C.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "\n",
    "# Fix missing data for test set. Assumin NaN means 0 in these categories\n",
    "x_test_est_A['effective_cloud_cover:p'] = x_test_est_A['effective_cloud_cover:p'].fillna(0)\n",
    "x_test_est_B['effective_cloud_cover:p'] = x_test_est_B['effective_cloud_cover:p'].fillna(0)\n",
    "x_test_est_C['effective_cloud_cover:p'] = x_test_est_C['effective_cloud_cover:p'].fillna(0)\n",
    "\n",
    "x_test_est_A['total_cloud_cover:p'] = x_test_est_A['total_cloud_cover:p'].fillna(0)\n",
    "x_test_est_B['total_cloud_cover:p'] = x_test_est_B['total_cloud_cover:p'].fillna(0)\n",
    "x_test_est_C['total_cloud_cover:p'] = x_test_est_C['total_cloud_cover:p'].fillna(0)\n",
    "\n",
    "x_test_est_A['cloud_base_agl:m'] = x_test_est_A['cloud_base_agl:m'].fillna(0)\n",
    "x_test_est_B['cloud_base_agl:m'] = x_test_est_B['cloud_base_agl:m'].fillna(0)\n",
    "x_test_est_C['cloud_base_agl:m'] = x_test_est_C['cloud_base_agl:m'].fillna(0)\n",
    "\n",
    "x_test_est_A['ceiling_height_agl:m'] = x_test_est_A['ceiling_height_agl:m'].fillna(0)\n",
    "x_test_est_B['ceiling_height_agl:m'] = x_test_est_B['ceiling_height_agl:m'].fillna(0)\n",
    "x_test_est_C['ceiling_height_agl:m'] = x_test_est_C['ceiling_height_agl:m'].fillna(0)\n",
    "\n",
    "x_test_est_A['snow_density:kgm3'] = x_test_est_A['snow_density:kgm3'].fillna(0)\n",
    "x_test_est_B['snow_density:kgm3'] = x_test_est_B['snow_density:kgm3'].fillna(0)\n",
    "x_test_est_C['snow_density:kgm3'] = x_test_est_C['snow_density:kgm3'].fillna(0)\n",
    "\n",
    "x_test_est_A['snow_drift:idx'] = x_test_est_A['snow_drift:idx'].fillna(0)\n",
    "x_test_est_B['snow_drift:idx'] = x_test_est_B['snow_drift:idx'].fillna(0)\n",
    "x_test_est_C['snow_drift:idx'] = x_test_est_C['snow_drift:idx'].fillna(0)\n",
    "\n",
    "# Resample\n",
    "x_train_obs_A_resampled = x_train_obs_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_A_resampled = x_train_est_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_A_resampled = x_test_est_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "x_train_obs_B_resampled = x_train_obs_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_B_resampled = x_train_est_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_B_resampled = x_test_est_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "x_train_obs_C_resampled = x_train_obs_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_C_resampled = x_train_est_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_C_resampled = x_test_est_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "# Merge\n",
    "split_value = x_train_est_A['date_forecast'].iloc[0]\n",
    "split_index = x_target_A[x_target_A['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_A = x_target_A.iloc[:split_index]\n",
    "x_target_est_A = x_target_A.iloc[split_index:]\n",
    "\n",
    "obs_A = x_train_obs_A_resampled.merge(x_target_obs_A, left_index=True, right_on='date_forecast')\n",
    "est_A = x_train_est_A_resampled.merge(x_target_est_A, left_index=True, right_on='date_forecast')\n",
    "\n",
    "split_value = x_train_est_B['date_forecast'].iloc[0]\n",
    "split_index = x_target_B[x_target_B['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_B = x_target_B.iloc[:split_index]\n",
    "x_target_est_B = x_target_B.iloc[split_index:]\n",
    "\n",
    "obs_B = x_train_obs_B_resampled.merge(x_target_obs_B, left_index=True, right_on='date_forecast')\n",
    "est_B = x_train_est_B_resampled.merge(x_target_est_B, left_index=True, right_on='date_forecast')\n",
    "\n",
    "split_value = x_train_est_C['date_forecast'].iloc[0]\n",
    "split_index = x_target_C[x_target_C['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_C = x_target_C.iloc[:split_index]\n",
    "x_target_est_C = x_target_C.iloc[split_index:]\n",
    "\n",
    "obs_C = x_train_obs_C_resampled.merge(x_target_obs_C, left_index=True, right_on='date_forecast')\n",
    "est_C = x_train_est_C_resampled.merge(x_target_est_C, left_index=True, right_on='date_forecast')\n",
    "\n",
    "# Keep date_forecast in test dfs\n",
    "test_A = x_test_est_A_resampled\n",
    "test_B = x_test_est_B_resampled\n",
    "test_C = x_test_est_C_resampled\n",
    "\n",
    "# Drop all the NaNs\n",
    "test_A = test_A.dropna()\n",
    "test_B = test_B.dropna()\n",
    "test_C = test_C.dropna()\n",
    "\n",
    "def preprocessing(df):\n",
    "    df = add_experimental_features(df.copy())\n",
    "    df = add_date_features(df.copy())\n",
    "    df = add_binned_features(df.copy())\n",
    "    df = add_rate_of_change_features_to_df(df.copy())\n",
    "    df = add_est_obs_feature(df.copy())\n",
    "    df = remove_constant_regions(df.copy())\n",
    "    df = add_lagged_features_to_df(df.copy())\n",
    "    df = handle_nan(df.copy())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Preprocess\n",
    "obs_A = preprocessing(obs_A.copy())\n",
    "est_A = preprocessing(est_A.copy())\n",
    "test_A = preprocessing(test_A.copy())\n",
    "\n",
    "obs_B = preprocessing(obs_B.copy())\n",
    "est_B = preprocessing(est_B.copy())\n",
    "test_B = preprocessing(test_B.copy())\n",
    "\n",
    "obs_C = preprocessing(obs_C.copy())\n",
    "est_C = preprocessing(est_C.copy())\n",
    "test_C = preprocessing(test_C.copy())\n",
    "\n",
    "# Random seeds used for reproducibility\n",
    "# 32 weights: 0.3, 0.3, 0.4\n",
    "# 24 weights: 0.3, 0.3, 0.4\n",
    "# 33 (without winter months 1 and 12) weights: 0.2, 0.4, 0.4\n",
    "# 11 (without winter months 1, 2 and 11, 12) weights: 0.25, 0.35, 0.4\n",
    "# 5 weights: 0.4, 0.3, 0.3\n",
    "# 6 weights 0.3, 0.4, 0.3\n",
    "\n",
    "# Best score is the mean prediction of all the seeds mentioned above. The first weight is xgboost, the second is catboost, and the third is autogluon.\n",
    "\n",
    "# Set the random seed\n",
    "np.random.seed(6)\n",
    "\n",
    "# Concatinate\n",
    "A = pd.concat([obs_A, est_A])\n",
    "B = pd.concat([obs_B, est_B])\n",
    "C = pd.concat([obs_C, est_C])\n",
    "\n",
    "# Remove characters unparseable for CatBoost \n",
    "A.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in A.columns]\n",
    "B.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in B.columns]\n",
    "C.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in C.columns]\n",
    "\n",
    "test_A.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_A.columns]\n",
    "test_B.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_B.columns]\n",
    "test_C.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_C.columns]\n",
    "\n",
    "# Getting validation data from summer months, because the test set is from summer months. We experimentet with excluding winter months\n",
    "# from the training data here.\n",
    "\n",
    "# Step 1: Filter A to include only months from March to October\n",
    "A = A[A['date_forecast'].dt.month.isin([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])]\n",
    "\n",
    "# Step 2: Identify unique days within May, June, and July\n",
    "summer_months = A[A['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "\n",
    "# Step 3: Sample these days for val_A\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_A = A[A['date_forecast'].dt.date.isin(sampled_days)]\n",
    "\n",
    "# Step 4: Define train_A as the remaining data\n",
    "train_A = A[~A['date_forecast'].dt.date.isin(sampled_days)]\n",
    "\n",
    "# Prepare your features and target variables\n",
    "X_train_A = train_A.drop(columns='pv_measurement')\n",
    "y_train_A = train_A['pv_measurement']\n",
    "X_val_A = val_A.drop(columns='pv_measurement')\n",
    "y_val_A = val_A['pv_measurement']\n",
    "\n",
    "# Repeat for B and C\n",
    "B = B[B['date_forecast'].dt.month.isin([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])]\n",
    "summer_months = B[B['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_B = B[B['date_forecast'].dt.date.isin(sampled_days)]\n",
    "train_B = B[~B['date_forecast'].dt.date.isin(sampled_days)]\n",
    "X_train_B = train_B.drop(columns='pv_measurement')\n",
    "y_train_B = train_B['pv_measurement']\n",
    "X_val_B = val_B.drop(columns='pv_measurement')\n",
    "y_val_B = val_B['pv_measurement']\n",
    "\n",
    "C = C[C['date_forecast'].dt.month.isin([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])]\n",
    "summer_months = C[C['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_C = C[C['date_forecast'].dt.date.isin(sampled_days)]\n",
    "train_C = C[~C['date_forecast'].dt.date.isin(sampled_days)]\n",
    "X_train_C = train_C.drop(columns='pv_measurement')\n",
    "y_train_C = train_C['pv_measurement']\n",
    "X_val_C = val_C.drop(columns='pv_measurement')\n",
    "y_val_C = val_C['pv_measurement']\n",
    "\n",
    "# Drop date_forecast\n",
    "train_A = train_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "train_B = train_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "train_C = train_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_A = val_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_B = val_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_C = val_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_A = X_train_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_B = X_train_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_C = X_train_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_A = X_val_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_B = X_val_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_C = X_val_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "test_A = test_A.drop(columns=['date_forecast'])\n",
    "test_B = test_B.drop(columns=['date_forecast'])\n",
    "test_C = test_C.drop(columns=['date_forecast'])\n",
    "\n",
    "train_auto_A = TabularDataset(train_A)\n",
    "val_auto_A = TabularDataset(val_A)\n",
    "\n",
    "train_auto_B = TabularDataset(train_B)\n",
    "val_auto_B = TabularDataset(val_B)\n",
    "\n",
    "train_auto_C = TabularDataset(train_C)\n",
    "val_auto_C = TabularDataset(val_C)\n",
    "\n",
    "auto_label = 'pv_measurement'\n",
    "\n",
    "# Set the parameters for the XGBoost models\n",
    "params_xgb_A = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.4, \n",
    "    'learning_rate': 0.012, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 0.8, \n",
    "    'reg_lambda': 0.8, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1,\n",
    "    'num_parallel_tree': 2\n",
    "}\n",
    "\n",
    "params_xgb_B = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.8, \n",
    "    'learning_rate': 0.008, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 1, \n",
    "    'reg_lambda': 3, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "params_xgb_C = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.8, \n",
    "    'learning_rate': 0.008, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 1, \n",
    "    'reg_lambda': 3, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "xgb_A = xgb.XGBRegressor(**params_xgb_A)\n",
    "xgb_B = xgb.XGBRegressor(**params_xgb_B)\n",
    "xgb_C = xgb.XGBRegressor(**params_xgb_C)\n",
    "\n",
    "cat_A = CatBoostRegressor(\n",
    "    iterations=5000,         # The number of trees to build\n",
    "    #learning_rate=0.09,     # The learning rate\n",
    "    #depth=10,               # Depth of the tree\n",
    "    loss_function='MAE',     # Loss function to be optimized. RMSE is common for regression.\n",
    "    eval_metric='MAE',       # Evaluation metric for the validation set\n",
    "    #random_seed=42,         # Seed for reproducibility\n",
    "    #verbose=100             # Frequency of logging the training process\n",
    ")\n",
    "\n",
    "cat_B = CatBoostRegressor(\n",
    "    iterations=5000,\n",
    "    #learning_rate=0.09,\n",
    "    #depth=10,\n",
    "    loss_function='MAE',\n",
    "    eval_metric='MAE',\n",
    "    #random_seed=42,\n",
    "    #verbose=100\n",
    ")\n",
    "\n",
    "cat_C = CatBoostRegressor(\n",
    "    iterations=5000,\n",
    "    #learning_rate=0.09,\n",
    "    #depth=10,\n",
    "    loss_function='MAE',\n",
    "    eval_metric='MAE',\n",
    "    #random_seed=42,\n",
    "    #verbose=100\n",
    ")\n",
    "\n",
    "# Prepare data for the XGBoost models. We got them to work the best when having fewer columns\n",
    "xgb_columns = [\n",
    "    'total_radiationW',\n",
    "    'snow_accumulation',\n",
    "    'super_cooled_liquid_waterkgm2',\n",
    "    'average_wind_speed',\n",
    "    'sun_elevationd',\n",
    "    'sun_azimuthd',\n",
    "    'clear_sky_radW',\n",
    "    'month',\n",
    "    't_1000hPaC',\n",
    "    'msl_pressurehPa_scaled',\n",
    "    'rain_waterkgm2',\n",
    "    'cloud_base_aglm',\n",
    "    'effective_cloud_coverp',\n",
    "    'dew_or_rimeidx'\n",
    "]\n",
    "print(train_A.columns)\n",
    "\n",
    "X_train_xgb_A = train_A[xgb_columns]\n",
    "y_train_xgb_A = train_A['pv_measurement']\n",
    "X_test_xgb_A = test_A[xgb_columns]\n",
    "\n",
    "X_train_xgb_B = train_B[xgb_columns]\n",
    "y_train_xgb_B = train_B['pv_measurement']\n",
    "X_test_xgb_B = test_B[xgb_columns]\n",
    "\n",
    "X_train_xgb_C = train_C[xgb_columns]\n",
    "y_train_xgb_C = train_C['pv_measurement']\n",
    "X_test_xgb_C = test_C[xgb_columns]\n",
    "\n",
    "# Train the XGBoost models\n",
    "xgb_A.fit(\n",
    "    X=X_train_xgb_A, y=y_train_xgb_A,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "xgb_B.fit(\n",
    "    X=X_train_xgb_B, y=y_train_xgb_B,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "xgb_C.fit(\n",
    "    X=X_train_xgb_C, y=y_train_xgb_C,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Train the CatBoost models\n",
    "cat_A.fit(\n",
    "    X_train_A, y_train_A,\n",
    "    eval_set=(X_val_A, y_val_A),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "cat_B.fit(\n",
    "    X_train_B, y_train_B,\n",
    "    eval_set=(X_val_B, y_val_B),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "cat_C.fit(\n",
    "    X_train_C, y_train_C,\n",
    "    eval_set=(X_val_C, y_val_C),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "auto_A = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_A, \n",
    "                                                                                   presets='medium_quality', \n",
    "                                                                                   tuning_data=val_auto_A, \n",
    "                                                                                   use_bag_holdout=True, \n",
    "                                                                                   ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "auto_B = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_B,\n",
    "                                                                                      presets='medium_quality',\n",
    "                                                                                      tuning_data=val_auto_B,\n",
    "                                                                                      use_bag_holdout=True,\n",
    "                                                                                      ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "auto_C = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_C,\n",
    "                                                                                        presets='medium_quality',\n",
    "                                                                                        tuning_data=val_auto_C,\n",
    "                                                                                        use_bag_holdout=True,\n",
    "                                                                                        ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "xgb_weight = 0.3\n",
    "cat_weight = 0.4\n",
    "auto_weight = 0.3\n",
    "\n",
    "pred_xgb_A = xgb_A.predict(X_test_xgb_C)\n",
    "pred_xgb_B = xgb_B.predict(X_test_xgb_C)\n",
    "pred_xgb_C = xgb_C.predict(X_test_xgb_C)\n",
    "\n",
    "pred_auto_A = auto_A.predict(test_A)\n",
    "pred_auto_B = auto_B.predict(test_B)\n",
    "pred_auto_C = auto_C.predict(test_C)\n",
    "\n",
    "pred_cat_A = cat_A.predict(test_A)\n",
    "pred_cat_B = cat_B.predict(test_B)\n",
    "pred_cat_C = cat_C.predict(test_C)\n",
    "\n",
    "# Ensemble that seemed the best after some experimentation\n",
    "pred_A = (pred_xgb_A*xgb_weight + pred_cat_A*cat_weight + pred_auto_A*auto_weight)\n",
    "pred_B = (pred_xgb_B*xgb_weight + pred_cat_B*cat_weight + pred_auto_B*auto_weight)\n",
    "pred_C = (pred_xgb_C*xgb_weight + pred_cat_C*cat_weight + pred_auto_C*auto_weight)\n",
    "\n",
    "pred_A = np.clip(pred_A, 0, None)\n",
    "pred_B = np.clip(pred_B, 0, None)\n",
    "pred_C = np.clip(pred_C, 0, None)\n",
    "\n",
    "# Concatenate predictions\n",
    "predictions = np.concatenate([pred_A, pred_B, pred_C])\n",
    "\n",
    "# Save predictions\n",
    "predictions_6 = predictions\n",
    "\n",
    "# Create an id array\n",
    "ids = np.arange(0, len(predictions_6))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'prediction': predictions_6\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('predictions_6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence 7\n",
    "\n",
    "%reset -f\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostRegressor\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def add_experimental_features(df):\n",
    "    \"\"\"\n",
    "    Experimental feature engineering.\n",
    "    \"\"\"\n",
    "\n",
    "    # Radiation Features\n",
    "    df['total_radiation:W'] = df['direct_rad:W'] + df['diffuse_rad:W']\n",
    "    df['total_radiation_1h:J'] = df['direct_rad_1h:J'] + df['diffuse_rad_1h:J']\n",
    "    df['rad_diff:W'] = df['direct_rad:W'] - df['diffuse_rad:W']\n",
    "    df['rad_diff_1h:J'] = df['direct_rad_1h:J'] - df['diffuse_rad_1h:J']\n",
    "    df['diffuse_direct_ratio'] = df['diffuse_rad:W'] / df['direct_rad:W']\n",
    "\n",
    "    # Temperature and Pressure Features\n",
    "    df['temp_dewpoint_diff'] = df['t_1000hPa:K'] - df['dew_point_2m:K']\n",
    "    df['pressure_gradient'] = df['pressure_100m:hPa'] - df['pressure_50m:hPa']\n",
    "    df['t_1000hPa:C'] = df['t_1000hPa:K'] - 273.15\n",
    "    df['dew_point_2m:C'] = df['dew_point_2m:K'] - 273.15\n",
    "    df['msl_pressure:hPa_scaled'] = MinMaxScaler().fit_transform(df['msl_pressure:hPa'].values.reshape(-1, 1))\n",
    "    df['sfc_pressure:hPa_scaled'] = MinMaxScaler().fit_transform(df['sfc_pressure:hPa'].values.reshape(-1, 1))\n",
    "\n",
    "    # Wind Features\n",
    "    df['wind_vector_magnitude'] = (df['wind_speed_u_10m:ms']**2 + df['wind_speed_v_10m:ms']**2 + df['wind_speed_w_1000hPa:ms']**2)**0.5\n",
    "    df['average_wind_speed'] = (df['wind_speed_10m:ms'] + df['wind_speed_u_10m:ms']) / 2\n",
    "\n",
    "    # Cloud and Snow Features\n",
    "    df['cloud_humidity_product'] = df['total_cloud_cover:p'] * df['absolute_humidity_2m:gm3']\n",
    "    df['snow_accumulation'] = df[['fresh_snow_24h:cm', 'fresh_snow_12h:cm', 'fresh_snow_6h:cm', 'fresh_snow_3h:cm', 'fresh_snow_1h:cm']].sum(axis=1)\n",
    "\n",
    "    # Interaction between radiation and cloud cover\n",
    "    df['radiation_cloud_interaction'] = df['direct_rad:W'] * df['effective_cloud_cover:p']\n",
    "\n",
    "    # Interaction between temperature and radiation (considering that high temperature may reduce efficiency)\n",
    "    df['temp_rad_interaction'] = df['t_1000hPa:K'] * df['total_radiation:W']\n",
    "\n",
    "    # Interaction between wind cooling effect and temperature\n",
    "    df['wind_temp_interaction'] = df['average_wind_speed'] * df['t_1000hPa:K']\n",
    "\n",
    "    # Interaction between humidity and temperature\n",
    "    df['humidity_temp_interaction'] = df['absolute_humidity_2m:gm3'] * df['t_1000hPa:K']\n",
    "\n",
    "    # Interaction between humidity and radiation\n",
    "    df['sun_elevation_direct_rad_interaction'] = df['sun_elevation:d'] * df['direct_rad:W']\n",
    "\n",
    "    # Precipitation Features\n",
    "    df['precip'] = df['precip_5min:mm']*df['precip_type_5min:idx']\n",
    "\n",
    "    # Safeguard in case of inf values\n",
    "    df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_date_features(df):\n",
    "    \"\"\"\n",
    "    Adds 'month', 'year', 'hour' and 'day' columns to the dataframe based on the 'date_forecast' column.\n",
    "    Also adds 'hour_sin' and 'hour_cos' columns for the hour of the day.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if 'date_forecast' exists in the dataframe\n",
    "    if 'date_forecast' in df.columns:\n",
    "        # Convert the 'date_forecast' column to datetime format\n",
    "        df['date_forecast'] = pd.to_datetime(df['date_forecast'])\n",
    "        \n",
    "        # Extract month, year, hour and day\n",
    "        df['month'] = df['date_forecast'].dt.month\n",
    "        df['year'] = df['date_forecast'].dt.year\n",
    "        df['hour'] = df['date_forecast'].dt.hour\n",
    "        df['day'] = df['date_forecast'].dt.day\n",
    "        df['hour_sin'] = np.sin(2*np.pi*df['hour']/24)\n",
    "        df['hour_cos'] = np.cos(2*np.pi*df['hour']/24)\n",
    "\n",
    "    else:\n",
    "        print(\"Warning: 'date_forecast' column not found in the dataframe. No date features added.\")\n",
    "        return df  # Keep the 'date_forecast' column in the dataframe\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Adding discretized features for the continuous variables to help tree-based models\n",
    "\n",
    "def bin_columns(dataframe, columns_to_bin, n_bins=5):\n",
    "    \"\"\"\n",
    "    Bins the specified columns of the dataframe into equal-sized bins.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframe: pd.DataFrame\n",
    "    - columns_to_bin: list of strings, the names of the columns to bin\n",
    "    - n_bins: int or dict, the number of bins for each column (if int, use the same number for all columns;\n",
    "              if dict, specify individual numbers with column names as keys)\n",
    "    \n",
    "    Returns:\n",
    "    - binned_dataframe: pd.DataFrame, the dataframe with the specified columns binned\n",
    "    \"\"\"\n",
    "    binned_dataframe = dataframe.copy()\n",
    "    \n",
    "    for column in columns_to_bin:\n",
    "        # Determine the number of bins for this column\n",
    "        bins = n_bins if isinstance(n_bins, int) else n_bins.get(column, 5)\n",
    "        \n",
    "        # Create quantile-based bins\n",
    "        binned_dataframe[f'binned_{column}'] = pd.qcut(\n",
    "            binned_dataframe[column],\n",
    "            q=bins,\n",
    "            labels=False,\n",
    "            duplicates='drop'\n",
    "        )\n",
    "        \n",
    "    return binned_dataframe\n",
    "\n",
    "def add_binned_features(df):\n",
    "    columns_to_bin = [\n",
    "        'super_cooled_liquid_water:kgm2',\n",
    "        'ceiling_height_agl:m',\n",
    "        'cloud_base_agl:m'\n",
    "    ]\n",
    "\n",
    "    # Bin the columns\n",
    "    # df = bin_columns(df, columns_to_bin)\n",
    "    df = bin_columns(df, ['effective_cloud_cover:p'], n_bins=2)\n",
    "    df = bin_columns(df, ['ceiling_height_agl:m'], n_bins=3)\n",
    "    df = bin_columns(df, ['average_wind_speed'], n_bins=5)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_rate_of_change_features(df, features, second_order=False):\n",
    "    \"\"\"\n",
    "    Adds rate of change columns for specified features in the dataframe.\n",
    "    Assumes the dataframe is time sorted. If second_order is True, it also adds the second order rate of change.\n",
    "    \"\"\"\n",
    "    for feature in features:\n",
    "        rate_column_name = feature + '_rate_of_change'\n",
    "        df[rate_column_name] = df[feature].diff().fillna(0)  # Handle the first diff NaN if required\n",
    "        \n",
    "        if second_order:  # Check if second order difference is required\n",
    "            second_order_column_name = feature + '_rate_of_change_of_change'\n",
    "            df[second_order_column_name] = df[rate_column_name].diff().fillna(0)  # Second order difference\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_rate_of_change_features_to_df(df):\n",
    "    # Define the features for which to calculate rate of change\n",
    "    features_to_diff = [\n",
    "        't_1000hPa:K',\n",
    "        'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad:W',\n",
    "        'effective_cloud_cover:p', 'total_radiation:W'\n",
    "    ]\n",
    "\n",
    "    # Add rate of change features\n",
    "    return add_rate_of_change_features(df, features_to_diff, second_order=False)\n",
    "\n",
    "def add_est_obs_feature(df):\n",
    "    \"\"\"\n",
    "    Adds a column to the dataframe that indicates whether the data is estimated or observed.\n",
    "    \"\"\"\n",
    "    # Add the est_obs feature\n",
    "    if 'date_calc' not in df.columns:\n",
    "        # If 'date_calc' does not exist, create 'observed' column and set to 1\n",
    "        df['observed'] = 1\n",
    "        return df\n",
    "    else:\n",
    "        # If 'date_calc' exists, create a new column and set values to 0\n",
    "        df['observed'] = 0\n",
    "        return df.drop(columns=['date_calc'])\n",
    "    \n",
    "def remove_constant_regions(dataframe, column_name=\"pv_measurement\", threshold=72):\n",
    "    \"\"\"\n",
    "    Removes rows where the specified column has constant values for more than the given threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the specified column exists in the dataframe\n",
    "    if column_name not in dataframe.columns:\n",
    "        print(f\"Warning: '{column_name}' column not found in the dataframe. No rows removed.\")\n",
    "        return dataframe\n",
    "    \n",
    "    same_as_previous = dataframe[column_name].eq(dataframe[column_name].shift())\n",
    "    group_ids = (~same_as_previous).cumsum()\n",
    "    to_remove = group_ids[same_as_previous].value_counts() > threshold\n",
    "    group_ids_to_remove = to_remove[to_remove].index\n",
    "    \n",
    "    # Drop entire rows that match the conditions\n",
    "    return dataframe.drop(dataframe[group_ids.isin(group_ids_to_remove)].index)\n",
    "\n",
    "def add_lagged_features(df, features_with__lags, fill_value=None):\n",
    "    \"\"\"\n",
    "    Adds lagged columns for specified features in the dataframe with specific lag periods.\n",
    "    'features_with_specific_lags' is a dictionary with features as keys and specific lag as values.\n",
    "    'fill_value' is what to fill the NaNs with, after shifting.\n",
    "    \"\"\"\n",
    "    for feature, specific_lag in features_with__lags.items():\n",
    "        lag_column_name = f\"{feature}_lag_{specific_lag}\"\n",
    "        df[lag_column_name] = df[feature].shift(specific_lag).fillna(fill_value)\n",
    "    return df\n",
    "\n",
    "def add_lagged_features_to_df(df):\n",
    "    features_with_lags = {\n",
    "        'total_radiation:W': 1,\n",
    "        'total_radiation:W': -1,\n",
    "        'rad_diff:W': 1,\n",
    "        'rad_diff:W': -1,\n",
    "        'total_radiation_1h:J': 1,\n",
    "        'total_radiation_1h:J': -1\n",
    "    }\n",
    "\n",
    "    # Add lagged features for specific lags\n",
    "    return add_lagged_features(df, features_with_lags, fill_value=0)\n",
    "\n",
    "def handle_nan(df):\n",
    "    # Remove the rows where target is nan\n",
    "    try:\n",
    "        df = df[df['pv_measurement'].notna()]\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    # Set all remaining nans to 0\n",
    "    return df.fillna(0)\n",
    "\n",
    "# Aggregate the data to hourly with some aggregation methods for each column\n",
    "aggregation_methods = {\n",
    "    'date_forecast': 'first',\n",
    "    'diffuse_rad:W': 'sum',\n",
    "    'direct_rad:W': 'last',\n",
    "    'clear_sky_rad:W': 'sum',\n",
    "    'diffuse_rad_1h:J': 'last',\n",
    "    'direct_rad_1h:J': 'last',\n",
    "    'clear_sky_energy_1h:J': 'last',\n",
    "    'absolute_humidity_2m:gm3': 'mean',\n",
    "    'air_density_2m:kgm3': 'mean',\n",
    "    'ceiling_height_agl:m': 'max',\n",
    "    'cloud_base_agl:m': 'mean',\n",
    "    'dew_or_rime:idx': 'min',\n",
    "    'dew_point_2m:K': 'mean',\n",
    "    'effective_cloud_cover:p': 'sum',\n",
    "    'elevation:m': 'first',\n",
    "    'fresh_snow_12h:cm': 'max',\n",
    "    'fresh_snow_1h:cm': 'sum',\n",
    "    'fresh_snow_24h:cm': 'max',\n",
    "    'fresh_snow_3h:cm': 'max',\n",
    "    'fresh_snow_6h:cm': 'max',\n",
    "    'is_day:idx': 'max',\n",
    "    'is_in_shadow:idx': 'max',\n",
    "    'msl_pressure:hPa': 'mean',\n",
    "    'precip_5min:mm': 'sum',\n",
    "    'precip_type_5min:idx': 'sum',\n",
    "    'pressure_100m:hPa': 'mean',\n",
    "    'pressure_50m:hPa': 'mean',\n",
    "    'prob_rime:p': 'max',\n",
    "    'rain_water:kgm2': 'sum',\n",
    "    'relative_humidity_1000hPa:p': 'mean',\n",
    "    'sfc_pressure:hPa': 'mean',\n",
    "    'snow_density:kgm3': 'mean',\n",
    "    'snow_depth:cm': 'max',\n",
    "    'snow_drift:idx': 'max',\n",
    "    'snow_melt_10min:mm': 'sum',\n",
    "    'snow_water:kgm2': 'sum',\n",
    "    'sun_azimuth:d': 'first',\n",
    "    'sun_elevation:d': 'sum',\n",
    "    'super_cooled_liquid_water:kgm2': 'sum',\n",
    "    't_1000hPa:K': 'mean',\n",
    "    'total_cloud_cover:p': 'mean',\n",
    "    'visibility:m': 'mean',\n",
    "    'wind_speed_10m:ms': 'mean',\n",
    "    'wind_speed_u_10m:ms': 'mean',\n",
    "    'wind_speed_v_10m:ms': 'mean',\n",
    "    'wind_speed_w_1000hPa:ms': 'mean',\n",
    "    'cloud_base_agl:m': 'max',\n",
    "    'snow_density:kgm3': 'mean'\n",
    "}\n",
    "\n",
    "# Read in the data\n",
    "x_target_A = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "x_train_obs_A = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "x_train_est_A = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "x_test_est_A = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "x_target_B = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "x_train_obs_B = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "x_train_est_B = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "x_test_est_B = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "x_target_C = pd.read_parquet('./data/C/train_targets.parquet')\n",
    "x_train_obs_C = pd.read_parquet('./data/C/X_train_observed.parquet')\n",
    "x_train_est_C = pd.read_parquet('./data/C/X_train_estimated.parquet')\n",
    "x_test_est_C = pd.read_parquet('./data/C/X_test_estimated.parquet')\n",
    "\n",
    "# Rename time to date_forecast in target\n",
    "x_target_A.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "x_target_B.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "x_target_C.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "\n",
    "# Fix missing data for test set. Assumin NaN means 0 in these categories\n",
    "x_test_est_A['effective_cloud_cover:p'] = x_test_est_A['effective_cloud_cover:p'].fillna(0)\n",
    "x_test_est_B['effective_cloud_cover:p'] = x_test_est_B['effective_cloud_cover:p'].fillna(0)\n",
    "x_test_est_C['effective_cloud_cover:p'] = x_test_est_C['effective_cloud_cover:p'].fillna(0)\n",
    "\n",
    "x_test_est_A['total_cloud_cover:p'] = x_test_est_A['total_cloud_cover:p'].fillna(0)\n",
    "x_test_est_B['total_cloud_cover:p'] = x_test_est_B['total_cloud_cover:p'].fillna(0)\n",
    "x_test_est_C['total_cloud_cover:p'] = x_test_est_C['total_cloud_cover:p'].fillna(0)\n",
    "\n",
    "x_test_est_A['cloud_base_agl:m'] = x_test_est_A['cloud_base_agl:m'].fillna(0)\n",
    "x_test_est_B['cloud_base_agl:m'] = x_test_est_B['cloud_base_agl:m'].fillna(0)\n",
    "x_test_est_C['cloud_base_agl:m'] = x_test_est_C['cloud_base_agl:m'].fillna(0)\n",
    "\n",
    "x_test_est_A['ceiling_height_agl:m'] = x_test_est_A['ceiling_height_agl:m'].fillna(0)\n",
    "x_test_est_B['ceiling_height_agl:m'] = x_test_est_B['ceiling_height_agl:m'].fillna(0)\n",
    "x_test_est_C['ceiling_height_agl:m'] = x_test_est_C['ceiling_height_agl:m'].fillna(0)\n",
    "\n",
    "x_test_est_A['snow_density:kgm3'] = x_test_est_A['snow_density:kgm3'].fillna(0)\n",
    "x_test_est_B['snow_density:kgm3'] = x_test_est_B['snow_density:kgm3'].fillna(0)\n",
    "x_test_est_C['snow_density:kgm3'] = x_test_est_C['snow_density:kgm3'].fillna(0)\n",
    "\n",
    "x_test_est_A['snow_drift:idx'] = x_test_est_A['snow_drift:idx'].fillna(0)\n",
    "x_test_est_B['snow_drift:idx'] = x_test_est_B['snow_drift:idx'].fillna(0)\n",
    "x_test_est_C['snow_drift:idx'] = x_test_est_C['snow_drift:idx'].fillna(0)\n",
    "\n",
    "# Resample\n",
    "x_train_obs_A_resampled = x_train_obs_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_A_resampled = x_train_est_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_A_resampled = x_test_est_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "x_train_obs_B_resampled = x_train_obs_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_B_resampled = x_train_est_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_B_resampled = x_test_est_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "x_train_obs_C_resampled = x_train_obs_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_C_resampled = x_train_est_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_C_resampled = x_test_est_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "# Merge\n",
    "split_value = x_train_est_A['date_forecast'].iloc[0]\n",
    "split_index = x_target_A[x_target_A['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_A = x_target_A.iloc[:split_index]\n",
    "x_target_est_A = x_target_A.iloc[split_index:]\n",
    "\n",
    "obs_A = x_train_obs_A_resampled.merge(x_target_obs_A, left_index=True, right_on='date_forecast')\n",
    "est_A = x_train_est_A_resampled.merge(x_target_est_A, left_index=True, right_on='date_forecast')\n",
    "\n",
    "split_value = x_train_est_B['date_forecast'].iloc[0]\n",
    "split_index = x_target_B[x_target_B['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_B = x_target_B.iloc[:split_index]\n",
    "x_target_est_B = x_target_B.iloc[split_index:]\n",
    "\n",
    "obs_B = x_train_obs_B_resampled.merge(x_target_obs_B, left_index=True, right_on='date_forecast')\n",
    "est_B = x_train_est_B_resampled.merge(x_target_est_B, left_index=True, right_on='date_forecast')\n",
    "\n",
    "split_value = x_train_est_C['date_forecast'].iloc[0]\n",
    "split_index = x_target_C[x_target_C['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_C = x_target_C.iloc[:split_index]\n",
    "x_target_est_C = x_target_C.iloc[split_index:]\n",
    "\n",
    "obs_C = x_train_obs_C_resampled.merge(x_target_obs_C, left_index=True, right_on='date_forecast')\n",
    "est_C = x_train_est_C_resampled.merge(x_target_est_C, left_index=True, right_on='date_forecast')\n",
    "\n",
    "# Keep date_forecast in test dfs\n",
    "test_A = x_test_est_A_resampled\n",
    "test_B = x_test_est_B_resampled\n",
    "test_C = x_test_est_C_resampled\n",
    "\n",
    "# Drop all the NaNs\n",
    "test_A = test_A.dropna()\n",
    "test_B = test_B.dropna()\n",
    "test_C = test_C.dropna()\n",
    "\n",
    "def preprocessing(df):\n",
    "    df = add_experimental_features(df.copy())\n",
    "    df = add_date_features(df.copy())\n",
    "    df = add_binned_features(df.copy())\n",
    "    df = add_rate_of_change_features_to_df(df.copy())\n",
    "    df = add_est_obs_feature(df.copy())\n",
    "    df = remove_constant_regions(df.copy())\n",
    "    df = add_lagged_features_to_df(df.copy())\n",
    "    df = handle_nan(df.copy())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Preprocess\n",
    "obs_A = preprocessing(obs_A.copy())\n",
    "est_A = preprocessing(est_A.copy())\n",
    "test_A = preprocessing(test_A.copy())\n",
    "\n",
    "obs_B = preprocessing(obs_B.copy())\n",
    "est_B = preprocessing(est_B.copy())\n",
    "test_B = preprocessing(test_B.copy())\n",
    "\n",
    "obs_C = preprocessing(obs_C.copy())\n",
    "est_C = preprocessing(est_C.copy())\n",
    "test_C = preprocessing(test_C.copy())\n",
    "\n",
    "# Random seeds used for reproducibility\n",
    "# 32 weights: 0.3, 0.3, 0.4\n",
    "# 24 weights: 0.3, 0.3, 0.4\n",
    "# 33 (without winter months 1 and 12) weights: 0.2, 0.4, 0.4\n",
    "# 11 (without winter months 1, 2 and 11, 12) weights: 0.25, 0.35, 0.4\n",
    "# 5 weights: 0.4, 0.3, 0.3\n",
    "# 6 weights 0.3, 0.4, 0.3\n",
    "# 7 weights 0.3, 0.4, 0.3\n",
    "\n",
    "# Best score is the mean prediction of all the seeds mentioned above. The first weight is xgboost, the second is catboost, and the third is autogluon.\n",
    "\n",
    "# Set the random seed\n",
    "np.random.seed(7)\n",
    "\n",
    "# Concatinate\n",
    "A = pd.concat([obs_A, est_A])\n",
    "B = pd.concat([obs_B, est_B])\n",
    "C = pd.concat([obs_C, est_C])\n",
    "\n",
    "# Remove characters unparseable for CatBoost \n",
    "A.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in A.columns]\n",
    "B.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in B.columns]\n",
    "C.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in C.columns]\n",
    "\n",
    "test_A.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_A.columns]\n",
    "test_B.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_B.columns]\n",
    "test_C.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_C.columns]\n",
    "\n",
    "# Getting validation data from summer months, because the test set is from summer months. We experimentet with excluding winter months\n",
    "# from the training data here.\n",
    "\n",
    "# Step 1: Filter A to include only months from March to October\n",
    "A = A[A['date_forecast'].dt.month.isin([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])]\n",
    "\n",
    "# Step 2: Identify unique days within May, June, and July\n",
    "summer_months = A[A['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "\n",
    "# Step 3: Sample these days for val_A\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_A = A[A['date_forecast'].dt.date.isin(sampled_days)]\n",
    "\n",
    "# Step 4: Define train_A as the remaining data\n",
    "train_A = A[~A['date_forecast'].dt.date.isin(sampled_days)]\n",
    "\n",
    "# Prepare your features and target variables\n",
    "X_train_A = train_A.drop(columns='pv_measurement')\n",
    "y_train_A = train_A['pv_measurement']\n",
    "X_val_A = val_A.drop(columns='pv_measurement')\n",
    "y_val_A = val_A['pv_measurement']\n",
    "\n",
    "# Repeat for B and C\n",
    "B = B[B['date_forecast'].dt.month.isin([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])]\n",
    "summer_months = B[B['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_B = B[B['date_forecast'].dt.date.isin(sampled_days)]\n",
    "train_B = B[~B['date_forecast'].dt.date.isin(sampled_days)]\n",
    "X_train_B = train_B.drop(columns='pv_measurement')\n",
    "y_train_B = train_B['pv_measurement']\n",
    "X_val_B = val_B.drop(columns='pv_measurement')\n",
    "y_val_B = val_B['pv_measurement']\n",
    "\n",
    "C = C[C['date_forecast'].dt.month.isin([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])]\n",
    "summer_months = C[C['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_C = C[C['date_forecast'].dt.date.isin(sampled_days)]\n",
    "train_C = C[~C['date_forecast'].dt.date.isin(sampled_days)]\n",
    "X_train_C = train_C.drop(columns='pv_measurement')\n",
    "y_train_C = train_C['pv_measurement']\n",
    "X_val_C = val_C.drop(columns='pv_measurement')\n",
    "y_val_C = val_C['pv_measurement']\n",
    "\n",
    "# Drop date_forecast\n",
    "train_A = train_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "train_B = train_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "train_C = train_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_A = val_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_B = val_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_C = val_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_A = X_train_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_B = X_train_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_C = X_train_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_A = X_val_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_B = X_val_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_C = X_val_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "test_A = test_A.drop(columns=['date_forecast'])\n",
    "test_B = test_B.drop(columns=['date_forecast'])\n",
    "test_C = test_C.drop(columns=['date_forecast'])\n",
    "\n",
    "train_auto_A = TabularDataset(train_A)\n",
    "val_auto_A = TabularDataset(val_A)\n",
    "\n",
    "train_auto_B = TabularDataset(train_B)\n",
    "val_auto_B = TabularDataset(val_B)\n",
    "\n",
    "train_auto_C = TabularDataset(train_C)\n",
    "val_auto_C = TabularDataset(val_C)\n",
    "\n",
    "auto_label = 'pv_measurement'\n",
    "\n",
    "# Set the parameters for the XGBoost models\n",
    "params_xgb_A = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.4, \n",
    "    'learning_rate': 0.012, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 0.8, \n",
    "    'reg_lambda': 0.8, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1,\n",
    "    'num_parallel_tree': 2\n",
    "}\n",
    "\n",
    "params_xgb_B = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.8, \n",
    "    'learning_rate': 0.008, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 1, \n",
    "    'reg_lambda': 3, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "params_xgb_C = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.8, \n",
    "    'learning_rate': 0.008, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 1, \n",
    "    'reg_lambda': 3, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "xgb_A = xgb.XGBRegressor(**params_xgb_A)\n",
    "xgb_B = xgb.XGBRegressor(**params_xgb_B)\n",
    "xgb_C = xgb.XGBRegressor(**params_xgb_C)\n",
    "\n",
    "cat_A = CatBoostRegressor(\n",
    "    iterations=5000,         # The number of trees to build\n",
    "    #learning_rate=0.09,     # The learning rate\n",
    "    #depth=10,               # Depth of the tree\n",
    "    loss_function='MAE',     # Loss function to be optimized. RMSE is common for regression.\n",
    "    eval_metric='MAE',       # Evaluation metric for the validation set\n",
    "    #random_seed=42,         # Seed for reproducibility\n",
    "    #verbose=100             # Frequency of logging the training process\n",
    ")\n",
    "\n",
    "cat_B = CatBoostRegressor(\n",
    "    iterations=5000,\n",
    "    #learning_rate=0.09,\n",
    "    #depth=10,\n",
    "    loss_function='MAE',\n",
    "    eval_metric='MAE',\n",
    "    #random_seed=42,\n",
    "    #verbose=100\n",
    ")\n",
    "\n",
    "cat_C = CatBoostRegressor(\n",
    "    iterations=5000,\n",
    "    #learning_rate=0.09,\n",
    "    #depth=10,\n",
    "    loss_function='MAE',\n",
    "    eval_metric='MAE',\n",
    "    #random_seed=42,\n",
    "    #verbose=100\n",
    ")\n",
    "\n",
    "# Prepare data for the XGBoost models. We got them to work the best when having fewer columns\n",
    "xgb_columns = [\n",
    "    'total_radiationW',\n",
    "    'snow_accumulation',\n",
    "    'super_cooled_liquid_waterkgm2',\n",
    "    'average_wind_speed',\n",
    "    'sun_elevationd',\n",
    "    'sun_azimuthd',\n",
    "    'clear_sky_radW',\n",
    "    'month',\n",
    "    't_1000hPaC',\n",
    "    'msl_pressurehPa_scaled',\n",
    "    'rain_waterkgm2',\n",
    "    'cloud_base_aglm',\n",
    "    'effective_cloud_coverp',\n",
    "    'dew_or_rimeidx'\n",
    "]\n",
    "print(train_A.columns)\n",
    "\n",
    "X_train_xgb_A = train_A[xgb_columns]\n",
    "y_train_xgb_A = train_A['pv_measurement']\n",
    "X_test_xgb_A = test_A[xgb_columns]\n",
    "\n",
    "X_train_xgb_B = train_B[xgb_columns]\n",
    "y_train_xgb_B = train_B['pv_measurement']\n",
    "X_test_xgb_B = test_B[xgb_columns]\n",
    "\n",
    "X_train_xgb_C = train_C[xgb_columns]\n",
    "y_train_xgb_C = train_C['pv_measurement']\n",
    "X_test_xgb_C = test_C[xgb_columns]\n",
    "\n",
    "# Train the XGBoost models\n",
    "xgb_A.fit(\n",
    "    X=X_train_xgb_A, y=y_train_xgb_A,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "xgb_B.fit(\n",
    "    X=X_train_xgb_B, y=y_train_xgb_B,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "xgb_C.fit(\n",
    "    X=X_train_xgb_C, y=y_train_xgb_C,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Train the CatBoost models\n",
    "cat_A.fit(\n",
    "    X_train_A, y_train_A,\n",
    "    eval_set=(X_val_A, y_val_A),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "cat_B.fit(\n",
    "    X_train_B, y_train_B,\n",
    "    eval_set=(X_val_B, y_val_B),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "cat_C.fit(\n",
    "    X_train_C, y_train_C,\n",
    "    eval_set=(X_val_C, y_val_C),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "auto_A = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_A, \n",
    "                                                                                   presets='medium_quality', \n",
    "                                                                                   tuning_data=val_auto_A, \n",
    "                                                                                   use_bag_holdout=True, \n",
    "                                                                                   ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "auto_B = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_B,\n",
    "                                                                                      presets='medium_quality',\n",
    "                                                                                      tuning_data=val_auto_B,\n",
    "                                                                                      use_bag_holdout=True,\n",
    "                                                                                      ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "auto_C = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_C,\n",
    "                                                                                        presets='medium_quality',\n",
    "                                                                                        tuning_data=val_auto_C,\n",
    "                                                                                        use_bag_holdout=True,\n",
    "                                                                                        ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "xgb_weight = 0.3\n",
    "cat_weight = 0.4\n",
    "auto_weight = 0.3\n",
    "\n",
    "pred_xgb_A = xgb_A.predict(X_test_xgb_C)\n",
    "pred_xgb_B = xgb_B.predict(X_test_xgb_C)\n",
    "pred_xgb_C = xgb_C.predict(X_test_xgb_C)\n",
    "\n",
    "pred_auto_A = auto_A.predict(test_A)\n",
    "pred_auto_B = auto_B.predict(test_B)\n",
    "pred_auto_C = auto_C.predict(test_C)\n",
    "\n",
    "pred_cat_A = cat_A.predict(test_A)\n",
    "pred_cat_B = cat_B.predict(test_B)\n",
    "pred_cat_C = cat_C.predict(test_C)\n",
    "\n",
    "# Ensemble that seemed the best after some experimentation\n",
    "pred_A = (pred_xgb_A*xgb_weight + pred_cat_A*cat_weight + pred_auto_A*auto_weight)\n",
    "pred_B = (pred_xgb_B*xgb_weight + pred_cat_B*cat_weight + pred_auto_B*auto_weight)\n",
    "pred_C = (pred_xgb_C*xgb_weight + pred_cat_C*cat_weight + pred_auto_C*auto_weight)\n",
    "\n",
    "pred_A = np.clip(pred_A, 0, None)\n",
    "pred_B = np.clip(pred_B, 0, None)\n",
    "pred_C = np.clip(pred_C, 0, None)\n",
    "\n",
    "# Concatenate predictions\n",
    "predictions = np.concatenate([pred_A, pred_B, pred_C])\n",
    "\n",
    "# Save predictions\n",
    "predictions_7 = predictions\n",
    "\n",
    "# Create an id array\n",
    "ids = np.arange(0, len(predictions_7))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'prediction': predictions_7\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('predictions_7.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence 8\n",
    "\n",
    "%reset -f\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostRegressor\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def add_experimental_features(df):\n",
    "    \"\"\"\n",
    "    Experimental feature engineering.\n",
    "    \"\"\"\n",
    "\n",
    "    # Radiation Features\n",
    "    df['total_radiation:W'] = df['direct_rad:W'] + df['diffuse_rad:W']\n",
    "    df['total_radiation_1h:J'] = df['direct_rad_1h:J'] + df['diffuse_rad_1h:J']\n",
    "    df['rad_diff:W'] = df['direct_rad:W'] - df['diffuse_rad:W']\n",
    "    df['rad_diff_1h:J'] = df['direct_rad_1h:J'] - df['diffuse_rad_1h:J']\n",
    "    df['diffuse_direct_ratio'] = df['diffuse_rad:W'] / df['direct_rad:W']\n",
    "\n",
    "    # Temperature and Pressure Features\n",
    "    df['temp_dewpoint_diff'] = df['t_1000hPa:K'] - df['dew_point_2m:K']\n",
    "    df['pressure_gradient'] = df['pressure_100m:hPa'] - df['pressure_50m:hPa']\n",
    "    df['t_1000hPa:C'] = df['t_1000hPa:K'] - 273.15\n",
    "    df['dew_point_2m:C'] = df['dew_point_2m:K'] - 273.15\n",
    "    df['msl_pressure:hPa_scaled'] = MinMaxScaler().fit_transform(df['msl_pressure:hPa'].values.reshape(-1, 1))\n",
    "    df['sfc_pressure:hPa_scaled'] = MinMaxScaler().fit_transform(df['sfc_pressure:hPa'].values.reshape(-1, 1))\n",
    "\n",
    "    # Wind Features\n",
    "    df['wind_vector_magnitude'] = (df['wind_speed_u_10m:ms']**2 + df['wind_speed_v_10m:ms']**2 + df['wind_speed_w_1000hPa:ms']**2)**0.5\n",
    "    df['average_wind_speed'] = (df['wind_speed_10m:ms'] + df['wind_speed_u_10m:ms']) / 2\n",
    "\n",
    "    # Cloud and Snow Features\n",
    "    df['cloud_humidity_product'] = df['total_cloud_cover:p'] * df['absolute_humidity_2m:gm3']\n",
    "    df['snow_accumulation'] = df[['fresh_snow_24h:cm', 'fresh_snow_12h:cm', 'fresh_snow_6h:cm', 'fresh_snow_3h:cm', 'fresh_snow_1h:cm']].sum(axis=1)\n",
    "\n",
    "    # Interaction between radiation and cloud cover\n",
    "    df['radiation_cloud_interaction'] = df['direct_rad:W'] * df['effective_cloud_cover:p']\n",
    "\n",
    "    # Interaction between temperature and radiation (considering that high temperature may reduce efficiency)\n",
    "    df['temp_rad_interaction'] = df['t_1000hPa:K'] * df['total_radiation:W']\n",
    "\n",
    "    # Interaction between wind cooling effect and temperature\n",
    "    df['wind_temp_interaction'] = df['average_wind_speed'] * df['t_1000hPa:K']\n",
    "\n",
    "    # Interaction between humidity and temperature\n",
    "    df['humidity_temp_interaction'] = df['absolute_humidity_2m:gm3'] * df['t_1000hPa:K']\n",
    "\n",
    "    # Interaction between humidity and radiation\n",
    "    df['sun_elevation_direct_rad_interaction'] = df['sun_elevation:d'] * df['direct_rad:W']\n",
    "\n",
    "    # Precipitation Features\n",
    "    df['precip'] = df['precip_5min:mm']*df['precip_type_5min:idx']\n",
    "\n",
    "    # Safeguard in case of inf values\n",
    "    df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_date_features(df):\n",
    "    \"\"\"\n",
    "    Adds 'month', 'year', 'hour' and 'day' columns to the dataframe based on the 'date_forecast' column.\n",
    "    Also adds 'hour_sin' and 'hour_cos' columns for the hour of the day.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if 'date_forecast' exists in the dataframe\n",
    "    if 'date_forecast' in df.columns:\n",
    "        # Convert the 'date_forecast' column to datetime format\n",
    "        df['date_forecast'] = pd.to_datetime(df['date_forecast'])\n",
    "        \n",
    "        # Extract month, year, hour and day\n",
    "        df['month'] = df['date_forecast'].dt.month\n",
    "        df['year'] = df['date_forecast'].dt.year\n",
    "        df['hour'] = df['date_forecast'].dt.hour\n",
    "        df['day'] = df['date_forecast'].dt.day\n",
    "        df['hour_sin'] = np.sin(2*np.pi*df['hour']/24)\n",
    "        df['hour_cos'] = np.cos(2*np.pi*df['hour']/24)\n",
    "\n",
    "    else:\n",
    "        print(\"Warning: 'date_forecast' column not found in the dataframe. No date features added.\")\n",
    "        return df  # Keep the 'date_forecast' column in the dataframe\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Adding discretized features for the continuous variables to help tree-based models\n",
    "\n",
    "def bin_columns(dataframe, columns_to_bin, n_bins=5):\n",
    "    \"\"\"\n",
    "    Bins the specified columns of the dataframe into equal-sized bins.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframe: pd.DataFrame\n",
    "    - columns_to_bin: list of strings, the names of the columns to bin\n",
    "    - n_bins: int or dict, the number of bins for each column (if int, use the same number for all columns;\n",
    "              if dict, specify individual numbers with column names as keys)\n",
    "    \n",
    "    Returns:\n",
    "    - binned_dataframe: pd.DataFrame, the dataframe with the specified columns binned\n",
    "    \"\"\"\n",
    "    binned_dataframe = dataframe.copy()\n",
    "    \n",
    "    for column in columns_to_bin:\n",
    "        # Determine the number of bins for this column\n",
    "        bins = n_bins if isinstance(n_bins, int) else n_bins.get(column, 5)\n",
    "        \n",
    "        # Create quantile-based bins\n",
    "        binned_dataframe[f'binned_{column}'] = pd.qcut(\n",
    "            binned_dataframe[column],\n",
    "            q=bins,\n",
    "            labels=False,\n",
    "            duplicates='drop'\n",
    "        )\n",
    "        \n",
    "    return binned_dataframe\n",
    "\n",
    "def add_binned_features(df):\n",
    "    columns_to_bin = [\n",
    "        'super_cooled_liquid_water:kgm2',\n",
    "        'ceiling_height_agl:m',\n",
    "        'cloud_base_agl:m'\n",
    "    ]\n",
    "\n",
    "    # Bin the columns\n",
    "    # df = bin_columns(df, columns_to_bin)\n",
    "    df = bin_columns(df, ['effective_cloud_cover:p'], n_bins=2)\n",
    "    df = bin_columns(df, ['ceiling_height_agl:m'], n_bins=3)\n",
    "    df = bin_columns(df, ['average_wind_speed'], n_bins=5)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_rate_of_change_features(df, features, second_order=False):\n",
    "    \"\"\"\n",
    "    Adds rate of change columns for specified features in the dataframe.\n",
    "    Assumes the dataframe is time sorted. If second_order is True, it also adds the second order rate of change.\n",
    "    \"\"\"\n",
    "    for feature in features:\n",
    "        rate_column_name = feature + '_rate_of_change'\n",
    "        df[rate_column_name] = df[feature].diff().fillna(0)  # Handle the first diff NaN if required\n",
    "        \n",
    "        if second_order:  # Check if second order difference is required\n",
    "            second_order_column_name = feature + '_rate_of_change_of_change'\n",
    "            df[second_order_column_name] = df[rate_column_name].diff().fillna(0)  # Second order difference\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_rate_of_change_features_to_df(df):\n",
    "    # Define the features for which to calculate rate of change\n",
    "    features_to_diff = [\n",
    "        't_1000hPa:K',\n",
    "        'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad:W',\n",
    "        'effective_cloud_cover:p', 'total_radiation:W'\n",
    "    ]\n",
    "\n",
    "    # Add rate of change features\n",
    "    return add_rate_of_change_features(df, features_to_diff, second_order=False)\n",
    "\n",
    "def add_est_obs_feature(df):\n",
    "    \"\"\"\n",
    "    Adds a column to the dataframe that indicates whether the data is estimated or observed.\n",
    "    \"\"\"\n",
    "    # Add the est_obs feature\n",
    "    if 'date_calc' not in df.columns:\n",
    "        # If 'date_calc' does not exist, create 'observed' column and set to 1\n",
    "        df['observed'] = 1\n",
    "        return df\n",
    "    else:\n",
    "        # If 'date_calc' exists, create a new column and set values to 0\n",
    "        df['observed'] = 0\n",
    "        return df.drop(columns=['date_calc'])\n",
    "    \n",
    "def remove_constant_regions(dataframe, column_name=\"pv_measurement\", threshold=72):\n",
    "    \"\"\"\n",
    "    Removes rows where the specified column has constant values for more than the given threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the specified column exists in the dataframe\n",
    "    if column_name not in dataframe.columns:\n",
    "        print(f\"Warning: '{column_name}' column not found in the dataframe. No rows removed.\")\n",
    "        return dataframe\n",
    "    \n",
    "    same_as_previous = dataframe[column_name].eq(dataframe[column_name].shift())\n",
    "    group_ids = (~same_as_previous).cumsum()\n",
    "    to_remove = group_ids[same_as_previous].value_counts() > threshold\n",
    "    group_ids_to_remove = to_remove[to_remove].index\n",
    "    \n",
    "    # Drop entire rows that match the conditions\n",
    "    return dataframe.drop(dataframe[group_ids.isin(group_ids_to_remove)].index)\n",
    "\n",
    "def add_lagged_features(df, features_with__lags, fill_value=None):\n",
    "    \"\"\"\n",
    "    Adds lagged columns for specified features in the dataframe with specific lag periods.\n",
    "    'features_with_specific_lags' is a dictionary with features as keys and specific lag as values.\n",
    "    'fill_value' is what to fill the NaNs with, after shifting.\n",
    "    \"\"\"\n",
    "    for feature, specific_lag in features_with__lags.items():\n",
    "        lag_column_name = f\"{feature}_lag_{specific_lag}\"\n",
    "        df[lag_column_name] = df[feature].shift(specific_lag).fillna(fill_value)\n",
    "    return df\n",
    "\n",
    "def add_lagged_features_to_df(df):\n",
    "    features_with_lags = {\n",
    "        'total_radiation:W': 1,\n",
    "        'total_radiation:W': -1,\n",
    "        'rad_diff:W': 1,\n",
    "        'rad_diff:W': -1,\n",
    "        'total_radiation_1h:J': 1,\n",
    "        'total_radiation_1h:J': -1\n",
    "    }\n",
    "\n",
    "    # Add lagged features for specific lags\n",
    "    return add_lagged_features(df, features_with_lags, fill_value=0)\n",
    "\n",
    "def handle_nan(df):\n",
    "    # Remove the rows where target is nan\n",
    "    try:\n",
    "        df = df[df['pv_measurement'].notna()]\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    # Set all remaining nans to 0\n",
    "    return df.fillna(0)\n",
    "\n",
    "# Aggregate the data to hourly with some aggregation methods for each column\n",
    "aggregation_methods = {\n",
    "    'date_forecast': 'first',\n",
    "    'diffuse_rad:W': 'sum',\n",
    "    'direct_rad:W': 'last',\n",
    "    'clear_sky_rad:W': 'sum',\n",
    "    'diffuse_rad_1h:J': 'last',\n",
    "    'direct_rad_1h:J': 'last',\n",
    "    'clear_sky_energy_1h:J': 'last',\n",
    "    'absolute_humidity_2m:gm3': 'mean',\n",
    "    'air_density_2m:kgm3': 'mean',\n",
    "    'ceiling_height_agl:m': 'max',\n",
    "    'cloud_base_agl:m': 'mean',\n",
    "    'dew_or_rime:idx': 'min',\n",
    "    'dew_point_2m:K': 'mean',\n",
    "    'effective_cloud_cover:p': 'sum',\n",
    "    'elevation:m': 'first',\n",
    "    'fresh_snow_12h:cm': 'max',\n",
    "    'fresh_snow_1h:cm': 'sum',\n",
    "    'fresh_snow_24h:cm': 'max',\n",
    "    'fresh_snow_3h:cm': 'max',\n",
    "    'fresh_snow_6h:cm': 'max',\n",
    "    'is_day:idx': 'max',\n",
    "    'is_in_shadow:idx': 'max',\n",
    "    'msl_pressure:hPa': 'mean',\n",
    "    'precip_5min:mm': 'sum',\n",
    "    'precip_type_5min:idx': 'sum',\n",
    "    'pressure_100m:hPa': 'mean',\n",
    "    'pressure_50m:hPa': 'mean',\n",
    "    'prob_rime:p': 'max',\n",
    "    'rain_water:kgm2': 'sum',\n",
    "    'relative_humidity_1000hPa:p': 'mean',\n",
    "    'sfc_pressure:hPa': 'mean',\n",
    "    'snow_density:kgm3': 'mean',\n",
    "    'snow_depth:cm': 'max',\n",
    "    'snow_drift:idx': 'max',\n",
    "    'snow_melt_10min:mm': 'sum',\n",
    "    'snow_water:kgm2': 'sum',\n",
    "    'sun_azimuth:d': 'first',\n",
    "    'sun_elevation:d': 'sum',\n",
    "    'super_cooled_liquid_water:kgm2': 'sum',\n",
    "    't_1000hPa:K': 'mean',\n",
    "    'total_cloud_cover:p': 'mean',\n",
    "    'visibility:m': 'mean',\n",
    "    'wind_speed_10m:ms': 'mean',\n",
    "    'wind_speed_u_10m:ms': 'mean',\n",
    "    'wind_speed_v_10m:ms': 'mean',\n",
    "    'wind_speed_w_1000hPa:ms': 'mean',\n",
    "    'cloud_base_agl:m': 'max',\n",
    "    'snow_density:kgm3': 'mean'\n",
    "}\n",
    "\n",
    "# Read in the data\n",
    "x_target_A = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "x_train_obs_A = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "x_train_est_A = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "x_test_est_A = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "x_target_B = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "x_train_obs_B = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "x_train_est_B = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "x_test_est_B = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "x_target_C = pd.read_parquet('./data/C/train_targets.parquet')\n",
    "x_train_obs_C = pd.read_parquet('./data/C/X_train_observed.parquet')\n",
    "x_train_est_C = pd.read_parquet('./data/C/X_train_estimated.parquet')\n",
    "x_test_est_C = pd.read_parquet('./data/C/X_test_estimated.parquet')\n",
    "\n",
    "# Rename time to date_forecast in target\n",
    "x_target_A.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "x_target_B.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "x_target_C.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "\n",
    "# Fix missing data for test set. Assumin NaN means 0 in these categories\n",
    "x_test_est_A['effective_cloud_cover:p'] = x_test_est_A['effective_cloud_cover:p'].fillna(0)\n",
    "x_test_est_B['effective_cloud_cover:p'] = x_test_est_B['effective_cloud_cover:p'].fillna(0)\n",
    "x_test_est_C['effective_cloud_cover:p'] = x_test_est_C['effective_cloud_cover:p'].fillna(0)\n",
    "\n",
    "x_test_est_A['total_cloud_cover:p'] = x_test_est_A['total_cloud_cover:p'].fillna(0)\n",
    "x_test_est_B['total_cloud_cover:p'] = x_test_est_B['total_cloud_cover:p'].fillna(0)\n",
    "x_test_est_C['total_cloud_cover:p'] = x_test_est_C['total_cloud_cover:p'].fillna(0)\n",
    "\n",
    "x_test_est_A['cloud_base_agl:m'] = x_test_est_A['cloud_base_agl:m'].fillna(0)\n",
    "x_test_est_B['cloud_base_agl:m'] = x_test_est_B['cloud_base_agl:m'].fillna(0)\n",
    "x_test_est_C['cloud_base_agl:m'] = x_test_est_C['cloud_base_agl:m'].fillna(0)\n",
    "\n",
    "x_test_est_A['ceiling_height_agl:m'] = x_test_est_A['ceiling_height_agl:m'].fillna(0)\n",
    "x_test_est_B['ceiling_height_agl:m'] = x_test_est_B['ceiling_height_agl:m'].fillna(0)\n",
    "x_test_est_C['ceiling_height_agl:m'] = x_test_est_C['ceiling_height_agl:m'].fillna(0)\n",
    "\n",
    "x_test_est_A['snow_density:kgm3'] = x_test_est_A['snow_density:kgm3'].fillna(0)\n",
    "x_test_est_B['snow_density:kgm3'] = x_test_est_B['snow_density:kgm3'].fillna(0)\n",
    "x_test_est_C['snow_density:kgm3'] = x_test_est_C['snow_density:kgm3'].fillna(0)\n",
    "\n",
    "x_test_est_A['snow_drift:idx'] = x_test_est_A['snow_drift:idx'].fillna(0)\n",
    "x_test_est_B['snow_drift:idx'] = x_test_est_B['snow_drift:idx'].fillna(0)\n",
    "x_test_est_C['snow_drift:idx'] = x_test_est_C['snow_drift:idx'].fillna(0)\n",
    "\n",
    "# Resample\n",
    "x_train_obs_A_resampled = x_train_obs_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_A_resampled = x_train_est_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_A_resampled = x_test_est_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "x_train_obs_B_resampled = x_train_obs_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_B_resampled = x_train_est_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_B_resampled = x_test_est_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "x_train_obs_C_resampled = x_train_obs_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_C_resampled = x_train_est_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_C_resampled = x_test_est_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "# Merge\n",
    "split_value = x_train_est_A['date_forecast'].iloc[0]\n",
    "split_index = x_target_A[x_target_A['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_A = x_target_A.iloc[:split_index]\n",
    "x_target_est_A = x_target_A.iloc[split_index:]\n",
    "\n",
    "obs_A = x_train_obs_A_resampled.merge(x_target_obs_A, left_index=True, right_on='date_forecast')\n",
    "est_A = x_train_est_A_resampled.merge(x_target_est_A, left_index=True, right_on='date_forecast')\n",
    "\n",
    "split_value = x_train_est_B['date_forecast'].iloc[0]\n",
    "split_index = x_target_B[x_target_B['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_B = x_target_B.iloc[:split_index]\n",
    "x_target_est_B = x_target_B.iloc[split_index:]\n",
    "\n",
    "obs_B = x_train_obs_B_resampled.merge(x_target_obs_B, left_index=True, right_on='date_forecast')\n",
    "est_B = x_train_est_B_resampled.merge(x_target_est_B, left_index=True, right_on='date_forecast')\n",
    "\n",
    "split_value = x_train_est_C['date_forecast'].iloc[0]\n",
    "split_index = x_target_C[x_target_C['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_C = x_target_C.iloc[:split_index]\n",
    "x_target_est_C = x_target_C.iloc[split_index:]\n",
    "\n",
    "obs_C = x_train_obs_C_resampled.merge(x_target_obs_C, left_index=True, right_on='date_forecast')\n",
    "est_C = x_train_est_C_resampled.merge(x_target_est_C, left_index=True, right_on='date_forecast')\n",
    "\n",
    "# Keep date_forecast in test dfs\n",
    "test_A = x_test_est_A_resampled\n",
    "test_B = x_test_est_B_resampled\n",
    "test_C = x_test_est_C_resampled\n",
    "\n",
    "# Drop all the NaNs\n",
    "test_A = test_A.dropna()\n",
    "test_B = test_B.dropna()\n",
    "test_C = test_C.dropna()\n",
    "\n",
    "def preprocessing(df):\n",
    "    df = add_experimental_features(df.copy())\n",
    "    df = add_date_features(df.copy())\n",
    "    df = add_binned_features(df.copy())\n",
    "    df = add_rate_of_change_features_to_df(df.copy())\n",
    "    df = add_est_obs_feature(df.copy())\n",
    "    df = remove_constant_regions(df.copy())\n",
    "    df = add_lagged_features_to_df(df.copy())\n",
    "    df = handle_nan(df.copy())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Preprocess\n",
    "obs_A = preprocessing(obs_A.copy())\n",
    "est_A = preprocessing(est_A.copy())\n",
    "test_A = preprocessing(test_A.copy())\n",
    "\n",
    "obs_B = preprocessing(obs_B.copy())\n",
    "est_B = preprocessing(est_B.copy())\n",
    "test_B = preprocessing(test_B.copy())\n",
    "\n",
    "obs_C = preprocessing(obs_C.copy())\n",
    "est_C = preprocessing(est_C.copy())\n",
    "test_C = preprocessing(test_C.copy())\n",
    "\n",
    "# Random seeds used for reproducibility\n",
    "# 32 weights: 0.3, 0.3, 0.4\n",
    "# 24 weights: 0.3, 0.3, 0.4\n",
    "# 33 (without winter months 1 and 12) weights: 0.2, 0.4, 0.4\n",
    "# 11 (without winter months 1, 2 and 11, 12) weights: 0.25, 0.35, 0.4\n",
    "# 5 weights: 0.4, 0.3, 0.3\n",
    "# 6 weights 0.3, 0.4, 0.3\n",
    "# 7 weights 0.3, 0.4, 0.3\n",
    "# 8 weights 0.3, 0.3, 0.4\n",
    "\n",
    "# Best score is the mean prediction of all the seeds mentioned above. The first weight is xgboost, the second is catboost, and the third is autogluon.\n",
    "\n",
    "# Set the random seed\n",
    "np.random.seed(8)\n",
    "\n",
    "# Concatinate\n",
    "A = pd.concat([obs_A, est_A])\n",
    "B = pd.concat([obs_B, est_B])\n",
    "C = pd.concat([obs_C, est_C])\n",
    "\n",
    "# Remove characters unparseable for CatBoost \n",
    "A.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in A.columns]\n",
    "B.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in B.columns]\n",
    "C.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in C.columns]\n",
    "\n",
    "test_A.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_A.columns]\n",
    "test_B.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_B.columns]\n",
    "test_C.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_C.columns]\n",
    "\n",
    "# Getting validation data from summer months, because the test set is from summer months. We experimentet with excluding winter months\n",
    "# from the training data here.\n",
    "\n",
    "# Step 1: Filter A to include only months from March to October\n",
    "A = A[A['date_forecast'].dt.month.isin([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])]\n",
    "\n",
    "# Step 2: Identify unique days within May, June, and July\n",
    "summer_months = A[A['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "\n",
    "# Step 3: Sample these days for val_A\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_A = A[A['date_forecast'].dt.date.isin(sampled_days)]\n",
    "\n",
    "# Step 4: Define train_A as the remaining data\n",
    "train_A = A[~A['date_forecast'].dt.date.isin(sampled_days)]\n",
    "\n",
    "# Prepare your features and target variables\n",
    "X_train_A = train_A.drop(columns='pv_measurement')\n",
    "y_train_A = train_A['pv_measurement']\n",
    "X_val_A = val_A.drop(columns='pv_measurement')\n",
    "y_val_A = val_A['pv_measurement']\n",
    "\n",
    "# Repeat for B and C\n",
    "B = B[B['date_forecast'].dt.month.isin([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])]\n",
    "summer_months = B[B['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_B = B[B['date_forecast'].dt.date.isin(sampled_days)]\n",
    "train_B = B[~B['date_forecast'].dt.date.isin(sampled_days)]\n",
    "X_train_B = train_B.drop(columns='pv_measurement')\n",
    "y_train_B = train_B['pv_measurement']\n",
    "X_val_B = val_B.drop(columns='pv_measurement')\n",
    "y_val_B = val_B['pv_measurement']\n",
    "\n",
    "C = C[C['date_forecast'].dt.month.isin([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])]\n",
    "summer_months = C[C['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_C = C[C['date_forecast'].dt.date.isin(sampled_days)]\n",
    "train_C = C[~C['date_forecast'].dt.date.isin(sampled_days)]\n",
    "X_train_C = train_C.drop(columns='pv_measurement')\n",
    "y_train_C = train_C['pv_measurement']\n",
    "X_val_C = val_C.drop(columns='pv_measurement')\n",
    "y_val_C = val_C['pv_measurement']\n",
    "\n",
    "# Drop date_forecast\n",
    "train_A = train_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "train_B = train_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "train_C = train_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_A = val_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_B = val_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_C = val_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_A = X_train_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_B = X_train_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_C = X_train_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_A = X_val_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_B = X_val_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_C = X_val_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "test_A = test_A.drop(columns=['date_forecast'])\n",
    "test_B = test_B.drop(columns=['date_forecast'])\n",
    "test_C = test_C.drop(columns=['date_forecast'])\n",
    "\n",
    "train_auto_A = TabularDataset(train_A)\n",
    "val_auto_A = TabularDataset(val_A)\n",
    "\n",
    "train_auto_B = TabularDataset(train_B)\n",
    "val_auto_B = TabularDataset(val_B)\n",
    "\n",
    "train_auto_C = TabularDataset(train_C)\n",
    "val_auto_C = TabularDataset(val_C)\n",
    "\n",
    "auto_label = 'pv_measurement'\n",
    "\n",
    "# Set the parameters for the XGBoost models\n",
    "params_xgb_A = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.4, \n",
    "    'learning_rate': 0.012, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 0.8, \n",
    "    'reg_lambda': 0.8, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1,\n",
    "    'num_parallel_tree': 2\n",
    "}\n",
    "\n",
    "params_xgb_B = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.8, \n",
    "    'learning_rate': 0.008, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 1, \n",
    "    'reg_lambda': 3, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "params_xgb_C = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.8, \n",
    "    'learning_rate': 0.008, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 1, \n",
    "    'reg_lambda': 3, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "xgb_A = xgb.XGBRegressor(**params_xgb_A)\n",
    "xgb_B = xgb.XGBRegressor(**params_xgb_B)\n",
    "xgb_C = xgb.XGBRegressor(**params_xgb_C)\n",
    "\n",
    "cat_A = CatBoostRegressor(\n",
    "    iterations=5000,         # The number of trees to build\n",
    "    #learning_rate=0.09,     # The learning rate\n",
    "    #depth=10,               # Depth of the tree\n",
    "    loss_function='MAE',     # Loss function to be optimized. RMSE is common for regression.\n",
    "    eval_metric='MAE',       # Evaluation metric for the validation set\n",
    "    #random_seed=42,         # Seed for reproducibility\n",
    "    #verbose=100             # Frequency of logging the training process\n",
    ")\n",
    "\n",
    "cat_B = CatBoostRegressor(\n",
    "    iterations=5000,\n",
    "    #learning_rate=0.09,\n",
    "    #depth=10,\n",
    "    loss_function='MAE',\n",
    "    eval_metric='MAE',\n",
    "    #random_seed=42,\n",
    "    #verbose=100\n",
    ")\n",
    "\n",
    "cat_C = CatBoostRegressor(\n",
    "    iterations=5000,\n",
    "    #learning_rate=0.09,\n",
    "    #depth=10,\n",
    "    loss_function='MAE',\n",
    "    eval_metric='MAE',\n",
    "    #random_seed=42,\n",
    "    #verbose=100\n",
    ")\n",
    "\n",
    "# Prepare data for the XGBoost models. We got them to work the best when having fewer columns\n",
    "xgb_columns = [\n",
    "    'total_radiationW',\n",
    "    'snow_accumulation',\n",
    "    'super_cooled_liquid_waterkgm2',\n",
    "    'average_wind_speed',\n",
    "    'sun_elevationd',\n",
    "    'sun_azimuthd',\n",
    "    'clear_sky_radW',\n",
    "    'month',\n",
    "    't_1000hPaC',\n",
    "    'msl_pressurehPa_scaled',\n",
    "    'rain_waterkgm2',\n",
    "    'cloud_base_aglm',\n",
    "    'effective_cloud_coverp',\n",
    "    'dew_or_rimeidx'\n",
    "]\n",
    "print(train_A.columns)\n",
    "\n",
    "X_train_xgb_A = train_A[xgb_columns]\n",
    "y_train_xgb_A = train_A['pv_measurement']\n",
    "X_test_xgb_A = test_A[xgb_columns]\n",
    "\n",
    "X_train_xgb_B = train_B[xgb_columns]\n",
    "y_train_xgb_B = train_B['pv_measurement']\n",
    "X_test_xgb_B = test_B[xgb_columns]\n",
    "\n",
    "X_train_xgb_C = train_C[xgb_columns]\n",
    "y_train_xgb_C = train_C['pv_measurement']\n",
    "X_test_xgb_C = test_C[xgb_columns]\n",
    "\n",
    "# Train the XGBoost models\n",
    "xgb_A.fit(\n",
    "    X=X_train_xgb_A, y=y_train_xgb_A,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "xgb_B.fit(\n",
    "    X=X_train_xgb_B, y=y_train_xgb_B,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "xgb_C.fit(\n",
    "    X=X_train_xgb_C, y=y_train_xgb_C,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Train the CatBoost models\n",
    "cat_A.fit(\n",
    "    X_train_A, y_train_A,\n",
    "    eval_set=(X_val_A, y_val_A),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "cat_B.fit(\n",
    "    X_train_B, y_train_B,\n",
    "    eval_set=(X_val_B, y_val_B),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "cat_C.fit(\n",
    "    X_train_C, y_train_C,\n",
    "    eval_set=(X_val_C, y_val_C),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "auto_A = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_A, \n",
    "                                                                                   presets='medium_quality', \n",
    "                                                                                   tuning_data=val_auto_A, \n",
    "                                                                                   use_bag_holdout=True, \n",
    "                                                                                   ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "auto_B = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_B,\n",
    "                                                                                      presets='medium_quality',\n",
    "                                                                                      tuning_data=val_auto_B,\n",
    "                                                                                      use_bag_holdout=True,\n",
    "                                                                                      ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "auto_C = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_C,\n",
    "                                                                                        presets='medium_quality',\n",
    "                                                                                        tuning_data=val_auto_C,\n",
    "                                                                                        use_bag_holdout=True,\n",
    "                                                                                        ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "xgb_weight = 0.3\n",
    "cat_weight = 0.3\n",
    "auto_weight = 0.4\n",
    "\n",
    "pred_xgb_A = xgb_A.predict(X_test_xgb_C)\n",
    "pred_xgb_B = xgb_B.predict(X_test_xgb_C)\n",
    "pred_xgb_C = xgb_C.predict(X_test_xgb_C)\n",
    "\n",
    "pred_auto_A = auto_A.predict(test_A)\n",
    "pred_auto_B = auto_B.predict(test_B)\n",
    "pred_auto_C = auto_C.predict(test_C)\n",
    "\n",
    "pred_cat_A = cat_A.predict(test_A)\n",
    "pred_cat_B = cat_B.predict(test_B)\n",
    "pred_cat_C = cat_C.predict(test_C)\n",
    "\n",
    "# Ensemble that seemed the best after some experimentation\n",
    "pred_A = (pred_xgb_A*xgb_weight + pred_cat_A*cat_weight + pred_auto_A*auto_weight)\n",
    "pred_B = (pred_xgb_B*xgb_weight + pred_cat_B*cat_weight + pred_auto_B*auto_weight)\n",
    "pred_C = (pred_xgb_C*xgb_weight + pred_cat_C*cat_weight + pred_auto_C*auto_weight)\n",
    "\n",
    "pred_A = np.clip(pred_A, 0, None)\n",
    "pred_B = np.clip(pred_B, 0, None)\n",
    "pred_C = np.clip(pred_C, 0, None)\n",
    "\n",
    "# Concatenate predictions\n",
    "predictions = np.concatenate([pred_A, pred_B, pred_C])\n",
    "\n",
    "# Save predictions\n",
    "predictions_8 = predictions\n",
    "\n",
    "# Create an id array\n",
    "ids = np.arange(0, len(predictions_8))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'prediction': predictions_8\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('predictions_8.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence 9\n",
    "\n",
    "%reset -f\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostRegressor\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def add_experimental_features(df):\n",
    "    \"\"\"\n",
    "    Experimental feature engineering.\n",
    "    \"\"\"\n",
    "\n",
    "    # Radiation Features\n",
    "    df['total_radiation:W'] = df['direct_rad:W'] + df['diffuse_rad:W']\n",
    "    df['total_radiation_1h:J'] = df['direct_rad_1h:J'] + df['diffuse_rad_1h:J']\n",
    "    df['rad_diff:W'] = df['direct_rad:W'] - df['diffuse_rad:W']\n",
    "    df['rad_diff_1h:J'] = df['direct_rad_1h:J'] - df['diffuse_rad_1h:J']\n",
    "    df['diffuse_direct_ratio'] = df['diffuse_rad:W'] / df['direct_rad:W']\n",
    "\n",
    "    # Temperature and Pressure Features\n",
    "    df['temp_dewpoint_diff'] = df['t_1000hPa:K'] - df['dew_point_2m:K']\n",
    "    df['pressure_gradient'] = df['pressure_100m:hPa'] - df['pressure_50m:hPa']\n",
    "    df['t_1000hPa:C'] = df['t_1000hPa:K'] - 273.15\n",
    "    df['dew_point_2m:C'] = df['dew_point_2m:K'] - 273.15\n",
    "    df['msl_pressure:hPa_scaled'] = MinMaxScaler().fit_transform(df['msl_pressure:hPa'].values.reshape(-1, 1))\n",
    "    df['sfc_pressure:hPa_scaled'] = MinMaxScaler().fit_transform(df['sfc_pressure:hPa'].values.reshape(-1, 1))\n",
    "\n",
    "    # Wind Features\n",
    "    df['wind_vector_magnitude'] = (df['wind_speed_u_10m:ms']**2 + df['wind_speed_v_10m:ms']**2 + df['wind_speed_w_1000hPa:ms']**2)**0.5\n",
    "    df['average_wind_speed'] = (df['wind_speed_10m:ms'] + df['wind_speed_u_10m:ms']) / 2\n",
    "\n",
    "    # Cloud and Snow Features\n",
    "    df['cloud_humidity_product'] = df['total_cloud_cover:p'] * df['absolute_humidity_2m:gm3']\n",
    "    df['snow_accumulation'] = df[['fresh_snow_24h:cm', 'fresh_snow_12h:cm', 'fresh_snow_6h:cm', 'fresh_snow_3h:cm', 'fresh_snow_1h:cm']].sum(axis=1)\n",
    "\n",
    "    # Interaction between radiation and cloud cover\n",
    "    df['radiation_cloud_interaction'] = df['direct_rad:W'] * df['effective_cloud_cover:p']\n",
    "\n",
    "    # Interaction between temperature and radiation (considering that high temperature may reduce efficiency)\n",
    "    df['temp_rad_interaction'] = df['t_1000hPa:K'] * df['total_radiation:W']\n",
    "\n",
    "    # Interaction between wind cooling effect and temperature\n",
    "    df['wind_temp_interaction'] = df['average_wind_speed'] * df['t_1000hPa:K']\n",
    "\n",
    "    # Interaction between humidity and temperature\n",
    "    df['humidity_temp_interaction'] = df['absolute_humidity_2m:gm3'] * df['t_1000hPa:K']\n",
    "\n",
    "    # Interaction between humidity and radiation\n",
    "    df['sun_elevation_direct_rad_interaction'] = df['sun_elevation:d'] * df['direct_rad:W']\n",
    "\n",
    "    # Precipitation Features\n",
    "    df['precip'] = df['precip_5min:mm']*df['precip_type_5min:idx']\n",
    "\n",
    "    # Safeguard in case of inf values\n",
    "    df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_date_features(df):\n",
    "    \"\"\"\n",
    "    Adds 'month', 'year', 'hour' and 'day' columns to the dataframe based on the 'date_forecast' column.\n",
    "    Also adds 'hour_sin' and 'hour_cos' columns for the hour of the day.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if 'date_forecast' exists in the dataframe\n",
    "    if 'date_forecast' in df.columns:\n",
    "        # Convert the 'date_forecast' column to datetime format\n",
    "        df['date_forecast'] = pd.to_datetime(df['date_forecast'])\n",
    "        \n",
    "        # Extract month, year, hour and day\n",
    "        df['month'] = df['date_forecast'].dt.month\n",
    "        df['year'] = df['date_forecast'].dt.year\n",
    "        df['hour'] = df['date_forecast'].dt.hour\n",
    "        df['day'] = df['date_forecast'].dt.day\n",
    "        df['hour_sin'] = np.sin(2*np.pi*df['hour']/24)\n",
    "        df['hour_cos'] = np.cos(2*np.pi*df['hour']/24)\n",
    "\n",
    "    else:\n",
    "        print(\"Warning: 'date_forecast' column not found in the dataframe. No date features added.\")\n",
    "        return df  # Keep the 'date_forecast' column in the dataframe\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Adding discretized features for the continuous variables to help tree-based models\n",
    "\n",
    "def bin_columns(dataframe, columns_to_bin, n_bins=5):\n",
    "    \"\"\"\n",
    "    Bins the specified columns of the dataframe into equal-sized bins.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframe: pd.DataFrame\n",
    "    - columns_to_bin: list of strings, the names of the columns to bin\n",
    "    - n_bins: int or dict, the number of bins for each column (if int, use the same number for all columns;\n",
    "              if dict, specify individual numbers with column names as keys)\n",
    "    \n",
    "    Returns:\n",
    "    - binned_dataframe: pd.DataFrame, the dataframe with the specified columns binned\n",
    "    \"\"\"\n",
    "    binned_dataframe = dataframe.copy()\n",
    "    \n",
    "    for column in columns_to_bin:\n",
    "        # Determine the number of bins for this column\n",
    "        bins = n_bins if isinstance(n_bins, int) else n_bins.get(column, 5)\n",
    "        \n",
    "        # Create quantile-based bins\n",
    "        binned_dataframe[f'binned_{column}'] = pd.qcut(\n",
    "            binned_dataframe[column],\n",
    "            q=bins,\n",
    "            labels=False,\n",
    "            duplicates='drop'\n",
    "        )\n",
    "        \n",
    "    return binned_dataframe\n",
    "\n",
    "def add_binned_features(df):\n",
    "    columns_to_bin = [\n",
    "        'super_cooled_liquid_water:kgm2',\n",
    "        'ceiling_height_agl:m',\n",
    "        'cloud_base_agl:m'\n",
    "    ]\n",
    "\n",
    "    # Bin the columns\n",
    "    # df = bin_columns(df, columns_to_bin)\n",
    "    df = bin_columns(df, ['effective_cloud_cover:p'], n_bins=2)\n",
    "    df = bin_columns(df, ['ceiling_height_agl:m'], n_bins=3)\n",
    "    df = bin_columns(df, ['average_wind_speed'], n_bins=5)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_rate_of_change_features(df, features, second_order=False):\n",
    "    \"\"\"\n",
    "    Adds rate of change columns for specified features in the dataframe.\n",
    "    Assumes the dataframe is time sorted. If second_order is True, it also adds the second order rate of change.\n",
    "    \"\"\"\n",
    "    for feature in features:\n",
    "        rate_column_name = feature + '_rate_of_change'\n",
    "        df[rate_column_name] = df[feature].diff().fillna(0)  # Handle the first diff NaN if required\n",
    "        \n",
    "        if second_order:  # Check if second order difference is required\n",
    "            second_order_column_name = feature + '_rate_of_change_of_change'\n",
    "            df[second_order_column_name] = df[rate_column_name].diff().fillna(0)  # Second order difference\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_rate_of_change_features_to_df(df):\n",
    "    # Define the features for which to calculate rate of change\n",
    "    features_to_diff = [\n",
    "        't_1000hPa:K',\n",
    "        'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad:W',\n",
    "        'effective_cloud_cover:p', 'total_radiation:W'\n",
    "    ]\n",
    "\n",
    "    # Add rate of change features\n",
    "    return add_rate_of_change_features(df, features_to_diff, second_order=False)\n",
    "\n",
    "def add_est_obs_feature(df):\n",
    "    \"\"\"\n",
    "    Adds a column to the dataframe that indicates whether the data is estimated or observed.\n",
    "    \"\"\"\n",
    "    # Add the est_obs feature\n",
    "    if 'date_calc' not in df.columns:\n",
    "        # If 'date_calc' does not exist, create 'observed' column and set to 1\n",
    "        df['observed'] = 1\n",
    "        return df\n",
    "    else:\n",
    "        # If 'date_calc' exists, create a new column and set values to 0\n",
    "        df['observed'] = 0\n",
    "        return df.drop(columns=['date_calc'])\n",
    "    \n",
    "def remove_constant_regions(dataframe, column_name=\"pv_measurement\", threshold=72):\n",
    "    \"\"\"\n",
    "    Removes rows where the specified column has constant values for more than the given threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the specified column exists in the dataframe\n",
    "    if column_name not in dataframe.columns:\n",
    "        print(f\"Warning: '{column_name}' column not found in the dataframe. No rows removed.\")\n",
    "        return dataframe\n",
    "    \n",
    "    same_as_previous = dataframe[column_name].eq(dataframe[column_name].shift())\n",
    "    group_ids = (~same_as_previous).cumsum()\n",
    "    to_remove = group_ids[same_as_previous].value_counts() > threshold\n",
    "    group_ids_to_remove = to_remove[to_remove].index\n",
    "    \n",
    "    # Drop entire rows that match the conditions\n",
    "    return dataframe.drop(dataframe[group_ids.isin(group_ids_to_remove)].index)\n",
    "\n",
    "def add_lagged_features(df, features_with__lags, fill_value=None):\n",
    "    \"\"\"\n",
    "    Adds lagged columns for specified features in the dataframe with specific lag periods.\n",
    "    'features_with_specific_lags' is a dictionary with features as keys and specific lag as values.\n",
    "    'fill_value' is what to fill the NaNs with, after shifting.\n",
    "    \"\"\"\n",
    "    for feature, specific_lag in features_with__lags.items():\n",
    "        lag_column_name = f\"{feature}_lag_{specific_lag}\"\n",
    "        df[lag_column_name] = df[feature].shift(specific_lag).fillna(fill_value)\n",
    "    return df\n",
    "\n",
    "def add_lagged_features_to_df(df):\n",
    "    features_with_lags = {\n",
    "        'total_radiation:W': 1,\n",
    "        'total_radiation:W': -1,\n",
    "        'rad_diff:W': 1,\n",
    "        'rad_diff:W': -1,\n",
    "        'total_radiation_1h:J': 1,\n",
    "        'total_radiation_1h:J': -1\n",
    "    }\n",
    "\n",
    "    # Add lagged features for specific lags\n",
    "    return add_lagged_features(df, features_with_lags, fill_value=0)\n",
    "\n",
    "def handle_nan(df):\n",
    "    # Remove the rows where target is nan\n",
    "    try:\n",
    "        df = df[df['pv_measurement'].notna()]\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    # Set all remaining nans to 0\n",
    "    return df.fillna(0)\n",
    "\n",
    "# Aggregate the data to hourly with some aggregation methods for each column\n",
    "aggregation_methods = {\n",
    "    'date_forecast': 'first',\n",
    "    'diffuse_rad:W': 'sum',\n",
    "    'direct_rad:W': 'last',\n",
    "    'clear_sky_rad:W': 'sum',\n",
    "    'diffuse_rad_1h:J': 'last',\n",
    "    'direct_rad_1h:J': 'last',\n",
    "    'clear_sky_energy_1h:J': 'last',\n",
    "    'absolute_humidity_2m:gm3': 'mean',\n",
    "    'air_density_2m:kgm3': 'mean',\n",
    "    'ceiling_height_agl:m': 'max',\n",
    "    'cloud_base_agl:m': 'mean',\n",
    "    'dew_or_rime:idx': 'min',\n",
    "    'dew_point_2m:K': 'mean',\n",
    "    'effective_cloud_cover:p': 'sum',\n",
    "    'elevation:m': 'first',\n",
    "    'fresh_snow_12h:cm': 'max',\n",
    "    'fresh_snow_1h:cm': 'sum',\n",
    "    'fresh_snow_24h:cm': 'max',\n",
    "    'fresh_snow_3h:cm': 'max',\n",
    "    'fresh_snow_6h:cm': 'max',\n",
    "    'is_day:idx': 'max',\n",
    "    'is_in_shadow:idx': 'max',\n",
    "    'msl_pressure:hPa': 'mean',\n",
    "    'precip_5min:mm': 'sum',\n",
    "    'precip_type_5min:idx': 'sum',\n",
    "    'pressure_100m:hPa': 'mean',\n",
    "    'pressure_50m:hPa': 'mean',\n",
    "    'prob_rime:p': 'max',\n",
    "    'rain_water:kgm2': 'sum',\n",
    "    'relative_humidity_1000hPa:p': 'mean',\n",
    "    'sfc_pressure:hPa': 'mean',\n",
    "    'snow_density:kgm3': 'mean',\n",
    "    'snow_depth:cm': 'max',\n",
    "    'snow_drift:idx': 'max',\n",
    "    'snow_melt_10min:mm': 'sum',\n",
    "    'snow_water:kgm2': 'sum',\n",
    "    'sun_azimuth:d': 'first',\n",
    "    'sun_elevation:d': 'sum',\n",
    "    'super_cooled_liquid_water:kgm2': 'sum',\n",
    "    't_1000hPa:K': 'mean',\n",
    "    'total_cloud_cover:p': 'mean',\n",
    "    'visibility:m': 'mean',\n",
    "    'wind_speed_10m:ms': 'mean',\n",
    "    'wind_speed_u_10m:ms': 'mean',\n",
    "    'wind_speed_v_10m:ms': 'mean',\n",
    "    'wind_speed_w_1000hPa:ms': 'mean',\n",
    "    'cloud_base_agl:m': 'max',\n",
    "    'snow_density:kgm3': 'mean'\n",
    "}\n",
    "\n",
    "# Read in the data\n",
    "x_target_A = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "x_train_obs_A = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "x_train_est_A = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "x_test_est_A = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "x_target_B = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "x_train_obs_B = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "x_train_est_B = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "x_test_est_B = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "x_target_C = pd.read_parquet('./data/C/train_targets.parquet')\n",
    "x_train_obs_C = pd.read_parquet('./data/C/X_train_observed.parquet')\n",
    "x_train_est_C = pd.read_parquet('./data/C/X_train_estimated.parquet')\n",
    "x_test_est_C = pd.read_parquet('./data/C/X_test_estimated.parquet')\n",
    "\n",
    "# Rename time to date_forecast in target\n",
    "x_target_A.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "x_target_B.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "x_target_C.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "\n",
    "# Fix missing data for test set. Assumin NaN means 0 in these categories\n",
    "x_test_est_A['effective_cloud_cover:p'] = x_test_est_A['effective_cloud_cover:p'].fillna(0)\n",
    "x_test_est_B['effective_cloud_cover:p'] = x_test_est_B['effective_cloud_cover:p'].fillna(0)\n",
    "x_test_est_C['effective_cloud_cover:p'] = x_test_est_C['effective_cloud_cover:p'].fillna(0)\n",
    "\n",
    "x_test_est_A['total_cloud_cover:p'] = x_test_est_A['total_cloud_cover:p'].fillna(0)\n",
    "x_test_est_B['total_cloud_cover:p'] = x_test_est_B['total_cloud_cover:p'].fillna(0)\n",
    "x_test_est_C['total_cloud_cover:p'] = x_test_est_C['total_cloud_cover:p'].fillna(0)\n",
    "\n",
    "x_test_est_A['cloud_base_agl:m'] = x_test_est_A['cloud_base_agl:m'].fillna(0)\n",
    "x_test_est_B['cloud_base_agl:m'] = x_test_est_B['cloud_base_agl:m'].fillna(0)\n",
    "x_test_est_C['cloud_base_agl:m'] = x_test_est_C['cloud_base_agl:m'].fillna(0)\n",
    "\n",
    "x_test_est_A['ceiling_height_agl:m'] = x_test_est_A['ceiling_height_agl:m'].fillna(0)\n",
    "x_test_est_B['ceiling_height_agl:m'] = x_test_est_B['ceiling_height_agl:m'].fillna(0)\n",
    "x_test_est_C['ceiling_height_agl:m'] = x_test_est_C['ceiling_height_agl:m'].fillna(0)\n",
    "\n",
    "x_test_est_A['snow_density:kgm3'] = x_test_est_A['snow_density:kgm3'].fillna(0)\n",
    "x_test_est_B['snow_density:kgm3'] = x_test_est_B['snow_density:kgm3'].fillna(0)\n",
    "x_test_est_C['snow_density:kgm3'] = x_test_est_C['snow_density:kgm3'].fillna(0)\n",
    "\n",
    "x_test_est_A['snow_drift:idx'] = x_test_est_A['snow_drift:idx'].fillna(0)\n",
    "x_test_est_B['snow_drift:idx'] = x_test_est_B['snow_drift:idx'].fillna(0)\n",
    "x_test_est_C['snow_drift:idx'] = x_test_est_C['snow_drift:idx'].fillna(0)\n",
    "\n",
    "# Resample\n",
    "x_train_obs_A_resampled = x_train_obs_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_A_resampled = x_train_est_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_A_resampled = x_test_est_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "x_train_obs_B_resampled = x_train_obs_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_B_resampled = x_train_est_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_B_resampled = x_test_est_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "x_train_obs_C_resampled = x_train_obs_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_C_resampled = x_train_est_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_C_resampled = x_test_est_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "# Merge\n",
    "split_value = x_train_est_A['date_forecast'].iloc[0]\n",
    "split_index = x_target_A[x_target_A['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_A = x_target_A.iloc[:split_index]\n",
    "x_target_est_A = x_target_A.iloc[split_index:]\n",
    "\n",
    "obs_A = x_train_obs_A_resampled.merge(x_target_obs_A, left_index=True, right_on='date_forecast')\n",
    "est_A = x_train_est_A_resampled.merge(x_target_est_A, left_index=True, right_on='date_forecast')\n",
    "\n",
    "split_value = x_train_est_B['date_forecast'].iloc[0]\n",
    "split_index = x_target_B[x_target_B['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_B = x_target_B.iloc[:split_index]\n",
    "x_target_est_B = x_target_B.iloc[split_index:]\n",
    "\n",
    "obs_B = x_train_obs_B_resampled.merge(x_target_obs_B, left_index=True, right_on='date_forecast')\n",
    "est_B = x_train_est_B_resampled.merge(x_target_est_B, left_index=True, right_on='date_forecast')\n",
    "\n",
    "split_value = x_train_est_C['date_forecast'].iloc[0]\n",
    "split_index = x_target_C[x_target_C['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_C = x_target_C.iloc[:split_index]\n",
    "x_target_est_C = x_target_C.iloc[split_index:]\n",
    "\n",
    "obs_C = x_train_obs_C_resampled.merge(x_target_obs_C, left_index=True, right_on='date_forecast')\n",
    "est_C = x_train_est_C_resampled.merge(x_target_est_C, left_index=True, right_on='date_forecast')\n",
    "\n",
    "# Keep date_forecast in test dfs\n",
    "test_A = x_test_est_A_resampled\n",
    "test_B = x_test_est_B_resampled\n",
    "test_C = x_test_est_C_resampled\n",
    "\n",
    "# Drop all the NaNs\n",
    "test_A = test_A.dropna()\n",
    "test_B = test_B.dropna()\n",
    "test_C = test_C.dropna()\n",
    "\n",
    "def preprocessing(df):\n",
    "    df = add_experimental_features(df.copy())\n",
    "    df = add_date_features(df.copy())\n",
    "    df = add_binned_features(df.copy())\n",
    "    df = add_rate_of_change_features_to_df(df.copy())\n",
    "    df = add_est_obs_feature(df.copy())\n",
    "    df = remove_constant_regions(df.copy())\n",
    "    df = add_lagged_features_to_df(df.copy())\n",
    "    df = handle_nan(df.copy())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Preprocess\n",
    "obs_A = preprocessing(obs_A.copy())\n",
    "est_A = preprocessing(est_A.copy())\n",
    "test_A = preprocessing(test_A.copy())\n",
    "\n",
    "obs_B = preprocessing(obs_B.copy())\n",
    "est_B = preprocessing(est_B.copy())\n",
    "test_B = preprocessing(test_B.copy())\n",
    "\n",
    "obs_C = preprocessing(obs_C.copy())\n",
    "est_C = preprocessing(est_C.copy())\n",
    "test_C = preprocessing(test_C.copy())\n",
    "\n",
    "# Random seeds used for reproducibility\n",
    "# 32 weights: 0.3, 0.3, 0.4\n",
    "# 24 weights: 0.3, 0.3, 0.4\n",
    "# 33 (without winter months 1 and 12) weights: 0.2, 0.4, 0.4\n",
    "# 11 (without winter months 1, 2 and 11, 12) weights: 0.25, 0.35, 0.4\n",
    "# 5 weights: 0.4, 0.3, 0.3\n",
    "# 6 weights 0.3, 0.4, 0.3\n",
    "# 7 weights 0.3, 0.4, 0.3\n",
    "# 8 weights 0.3, 0.3, 0.4\n",
    "# 9 weights 0.3, 0.3, 0.4\n",
    "\n",
    "# Best score is the mean prediction of all the seeds mentioned above. The first weight is xgboost, the second is catboost, and the third is autogluon.\n",
    "\n",
    "# Set the random seed\n",
    "np.random.seed(9)\n",
    "\n",
    "# Concatinate\n",
    "A = pd.concat([obs_A, est_A])\n",
    "B = pd.concat([obs_B, est_B])\n",
    "C = pd.concat([obs_C, est_C])\n",
    "\n",
    "# Remove characters unparseable for CatBoost \n",
    "A.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in A.columns]\n",
    "B.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in B.columns]\n",
    "C.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in C.columns]\n",
    "\n",
    "test_A.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_A.columns]\n",
    "test_B.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_B.columns]\n",
    "test_C.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_C.columns]\n",
    "\n",
    "# Getting validation data from summer months, because the test set is from summer months. We experimentet with excluding winter months\n",
    "# from the training data here.\n",
    "\n",
    "# Step 1: Filter A to include only months from March to October\n",
    "A = A[A['date_forecast'].dt.month.isin([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])]\n",
    "\n",
    "# Step 2: Identify unique days within May, June, and July\n",
    "summer_months = A[A['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "\n",
    "# Step 3: Sample these days for val_A\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_A = A[A['date_forecast'].dt.date.isin(sampled_days)]\n",
    "\n",
    "# Step 4: Define train_A as the remaining data\n",
    "train_A = A[~A['date_forecast'].dt.date.isin(sampled_days)]\n",
    "\n",
    "# Prepare your features and target variables\n",
    "X_train_A = train_A.drop(columns='pv_measurement')\n",
    "y_train_A = train_A['pv_measurement']\n",
    "X_val_A = val_A.drop(columns='pv_measurement')\n",
    "y_val_A = val_A['pv_measurement']\n",
    "\n",
    "# Repeat for B and C\n",
    "B = B[B['date_forecast'].dt.month.isin([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])]\n",
    "summer_months = B[B['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_B = B[B['date_forecast'].dt.date.isin(sampled_days)]\n",
    "train_B = B[~B['date_forecast'].dt.date.isin(sampled_days)]\n",
    "X_train_B = train_B.drop(columns='pv_measurement')\n",
    "y_train_B = train_B['pv_measurement']\n",
    "X_val_B = val_B.drop(columns='pv_measurement')\n",
    "y_val_B = val_B['pv_measurement']\n",
    "\n",
    "C = C[C['date_forecast'].dt.month.isin([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])]\n",
    "summer_months = C[C['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_C = C[C['date_forecast'].dt.date.isin(sampled_days)]\n",
    "train_C = C[~C['date_forecast'].dt.date.isin(sampled_days)]\n",
    "X_train_C = train_C.drop(columns='pv_measurement')\n",
    "y_train_C = train_C['pv_measurement']\n",
    "X_val_C = val_C.drop(columns='pv_measurement')\n",
    "y_val_C = val_C['pv_measurement']\n",
    "\n",
    "# Drop date_forecast\n",
    "train_A = train_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "train_B = train_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "train_C = train_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_A = val_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_B = val_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_C = val_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_A = X_train_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_B = X_train_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_C = X_train_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_A = X_val_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_B = X_val_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_C = X_val_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "test_A = test_A.drop(columns=['date_forecast'])\n",
    "test_B = test_B.drop(columns=['date_forecast'])\n",
    "test_C = test_C.drop(columns=['date_forecast'])\n",
    "\n",
    "train_auto_A = TabularDataset(train_A)\n",
    "val_auto_A = TabularDataset(val_A)\n",
    "\n",
    "train_auto_B = TabularDataset(train_B)\n",
    "val_auto_B = TabularDataset(val_B)\n",
    "\n",
    "train_auto_C = TabularDataset(train_C)\n",
    "val_auto_C = TabularDataset(val_C)\n",
    "\n",
    "auto_label = 'pv_measurement'\n",
    "\n",
    "# Set the parameters for the XGBoost models\n",
    "params_xgb_A = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.4, \n",
    "    'learning_rate': 0.012, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 0.8, \n",
    "    'reg_lambda': 0.8, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1,\n",
    "    'num_parallel_tree': 2\n",
    "}\n",
    "\n",
    "params_xgb_B = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.8, \n",
    "    'learning_rate': 0.008, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 1, \n",
    "    'reg_lambda': 3, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "params_xgb_C = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.8, \n",
    "    'learning_rate': 0.008, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 1, \n",
    "    'reg_lambda': 3, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "xgb_A = xgb.XGBRegressor(**params_xgb_A)\n",
    "xgb_B = xgb.XGBRegressor(**params_xgb_B)\n",
    "xgb_C = xgb.XGBRegressor(**params_xgb_C)\n",
    "\n",
    "cat_A = CatBoostRegressor(\n",
    "    iterations=5000,         # The number of trees to build\n",
    "    #learning_rate=0.09,     # The learning rate\n",
    "    #depth=10,               # Depth of the tree\n",
    "    loss_function='MAE',     # Loss function to be optimized. RMSE is common for regression.\n",
    "    eval_metric='MAE',       # Evaluation metric for the validation set\n",
    "    #random_seed=42,         # Seed for reproducibility\n",
    "    #verbose=100             # Frequency of logging the training process\n",
    ")\n",
    "\n",
    "cat_B = CatBoostRegressor(\n",
    "    iterations=5000,\n",
    "    #learning_rate=0.09,\n",
    "    #depth=10,\n",
    "    loss_function='MAE',\n",
    "    eval_metric='MAE',\n",
    "    #random_seed=42,\n",
    "    #verbose=100\n",
    ")\n",
    "\n",
    "cat_C = CatBoostRegressor(\n",
    "    iterations=5000,\n",
    "    #learning_rate=0.09,\n",
    "    #depth=10,\n",
    "    loss_function='MAE',\n",
    "    eval_metric='MAE',\n",
    "    #random_seed=42,\n",
    "    #verbose=100\n",
    ")\n",
    "\n",
    "# Prepare data for the XGBoost models. We got them to work the best when having fewer columns\n",
    "xgb_columns = [\n",
    "    'total_radiationW',\n",
    "    'snow_accumulation',\n",
    "    'super_cooled_liquid_waterkgm2',\n",
    "    'average_wind_speed',\n",
    "    'sun_elevationd',\n",
    "    'sun_azimuthd',\n",
    "    'clear_sky_radW',\n",
    "    'month',\n",
    "    't_1000hPaC',\n",
    "    'msl_pressurehPa_scaled',\n",
    "    'rain_waterkgm2',\n",
    "    'cloud_base_aglm',\n",
    "    'effective_cloud_coverp',\n",
    "    'dew_or_rimeidx'\n",
    "]\n",
    "print(train_A.columns)\n",
    "\n",
    "X_train_xgb_A = train_A[xgb_columns]\n",
    "y_train_xgb_A = train_A['pv_measurement']\n",
    "X_test_xgb_A = test_A[xgb_columns]\n",
    "\n",
    "X_train_xgb_B = train_B[xgb_columns]\n",
    "y_train_xgb_B = train_B['pv_measurement']\n",
    "X_test_xgb_B = test_B[xgb_columns]\n",
    "\n",
    "X_train_xgb_C = train_C[xgb_columns]\n",
    "y_train_xgb_C = train_C['pv_measurement']\n",
    "X_test_xgb_C = test_C[xgb_columns]\n",
    "\n",
    "# Train the XGBoost models\n",
    "xgb_A.fit(\n",
    "    X=X_train_xgb_A, y=y_train_xgb_A,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "xgb_B.fit(\n",
    "    X=X_train_xgb_B, y=y_train_xgb_B,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "xgb_C.fit(\n",
    "    X=X_train_xgb_C, y=y_train_xgb_C,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Train the CatBoost models\n",
    "cat_A.fit(\n",
    "    X_train_A, y_train_A,\n",
    "    eval_set=(X_val_A, y_val_A),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "cat_B.fit(\n",
    "    X_train_B, y_train_B,\n",
    "    eval_set=(X_val_B, y_val_B),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "cat_C.fit(\n",
    "    X_train_C, y_train_C,\n",
    "    eval_set=(X_val_C, y_val_C),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "auto_A = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_A, \n",
    "                                                                                   presets='medium_quality', \n",
    "                                                                                   tuning_data=val_auto_A, \n",
    "                                                                                   use_bag_holdout=True, \n",
    "                                                                                   ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "auto_B = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_B,\n",
    "                                                                                      presets='medium_quality',\n",
    "                                                                                      tuning_data=val_auto_B,\n",
    "                                                                                      use_bag_holdout=True,\n",
    "                                                                                      ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "auto_C = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_C,\n",
    "                                                                                        presets='medium_quality',\n",
    "                                                                                        tuning_data=val_auto_C,\n",
    "                                                                                        use_bag_holdout=True,\n",
    "                                                                                        ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "xgb_weight = 0.3\n",
    "cat_weight = 0.3\n",
    "auto_weight = 0.4\n",
    "\n",
    "pred_xgb_A = xgb_A.predict(X_test_xgb_C)\n",
    "pred_xgb_B = xgb_B.predict(X_test_xgb_C)\n",
    "pred_xgb_C = xgb_C.predict(X_test_xgb_C)\n",
    "\n",
    "pred_auto_A = auto_A.predict(test_A)\n",
    "pred_auto_B = auto_B.predict(test_B)\n",
    "pred_auto_C = auto_C.predict(test_C)\n",
    "\n",
    "pred_cat_A = cat_A.predict(test_A)\n",
    "pred_cat_B = cat_B.predict(test_B)\n",
    "pred_cat_C = cat_C.predict(test_C)\n",
    "\n",
    "# Ensemble that seemed the best after some experimentation\n",
    "pred_A = (pred_xgb_A*xgb_weight + pred_cat_A*cat_weight + pred_auto_A*auto_weight)\n",
    "pred_B = (pred_xgb_B*xgb_weight + pred_cat_B*cat_weight + pred_auto_B*auto_weight)\n",
    "pred_C = (pred_xgb_C*xgb_weight + pred_cat_C*cat_weight + pred_auto_C*auto_weight)\n",
    "\n",
    "pred_A = np.clip(pred_A, 0, None)\n",
    "pred_B = np.clip(pred_B, 0, None)\n",
    "pred_C = np.clip(pred_C, 0, None)\n",
    "\n",
    "# Concatenate predictions\n",
    "predictions = np.concatenate([pred_A, pred_B, pred_C])\n",
    "\n",
    "# Save predictions\n",
    "predictions_9 = predictions\n",
    "\n",
    "# Create an id array\n",
    "ids = np.arange(0, len(predictions_9))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'prediction': predictions_9\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('predictions_9.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence 10\n",
    "\n",
    "%reset -f\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostRegressor\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def add_experimental_features(df):\n",
    "    \"\"\"\n",
    "    Experimental feature engineering.\n",
    "    \"\"\"\n",
    "\n",
    "    # Radiation Features\n",
    "    df['total_radiation:W'] = df['direct_rad:W'] + df['diffuse_rad:W']\n",
    "    df['total_radiation_1h:J'] = df['direct_rad_1h:J'] + df['diffuse_rad_1h:J']\n",
    "    df['rad_diff:W'] = df['direct_rad:W'] - df['diffuse_rad:W']\n",
    "    df['rad_diff_1h:J'] = df['direct_rad_1h:J'] - df['diffuse_rad_1h:J']\n",
    "    df['diffuse_direct_ratio'] = df['diffuse_rad:W'] / df['direct_rad:W']\n",
    "\n",
    "    # Temperature and Pressure Features\n",
    "    df['temp_dewpoint_diff'] = df['t_1000hPa:K'] - df['dew_point_2m:K']\n",
    "    df['pressure_gradient'] = df['pressure_100m:hPa'] - df['pressure_50m:hPa']\n",
    "    df['t_1000hPa:C'] = df['t_1000hPa:K'] - 273.15\n",
    "    df['dew_point_2m:C'] = df['dew_point_2m:K'] - 273.15\n",
    "    df['msl_pressure:hPa_scaled'] = MinMaxScaler().fit_transform(df['msl_pressure:hPa'].values.reshape(-1, 1))\n",
    "    df['sfc_pressure:hPa_scaled'] = MinMaxScaler().fit_transform(df['sfc_pressure:hPa'].values.reshape(-1, 1))\n",
    "\n",
    "    # Wind Features\n",
    "    df['wind_vector_magnitude'] = (df['wind_speed_u_10m:ms']**2 + df['wind_speed_v_10m:ms']**2 + df['wind_speed_w_1000hPa:ms']**2)**0.5\n",
    "    df['average_wind_speed'] = (df['wind_speed_10m:ms'] + df['wind_speed_u_10m:ms']) / 2\n",
    "\n",
    "    # Cloud and Snow Features\n",
    "    df['cloud_humidity_product'] = df['total_cloud_cover:p'] * df['absolute_humidity_2m:gm3']\n",
    "    df['snow_accumulation'] = df[['fresh_snow_24h:cm', 'fresh_snow_12h:cm', 'fresh_snow_6h:cm', 'fresh_snow_3h:cm', 'fresh_snow_1h:cm']].sum(axis=1)\n",
    "\n",
    "    # Interaction between radiation and cloud cover\n",
    "    df['radiation_cloud_interaction'] = df['direct_rad:W'] * df['effective_cloud_cover:p']\n",
    "\n",
    "    # Interaction between temperature and radiation (considering that high temperature may reduce efficiency)\n",
    "    df['temp_rad_interaction'] = df['t_1000hPa:K'] * df['total_radiation:W']\n",
    "\n",
    "    # Interaction between wind cooling effect and temperature\n",
    "    df['wind_temp_interaction'] = df['average_wind_speed'] * df['t_1000hPa:K']\n",
    "\n",
    "    # Interaction between humidity and temperature\n",
    "    df['humidity_temp_interaction'] = df['absolute_humidity_2m:gm3'] * df['t_1000hPa:K']\n",
    "\n",
    "    # Interaction between humidity and radiation\n",
    "    df['sun_elevation_direct_rad_interaction'] = df['sun_elevation:d'] * df['direct_rad:W']\n",
    "\n",
    "    # Precipitation Features\n",
    "    df['precip'] = df['precip_5min:mm']*df['precip_type_5min:idx']\n",
    "\n",
    "    # Safeguard in case of inf values\n",
    "    df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_date_features(df):\n",
    "    \"\"\"\n",
    "    Adds 'month', 'year', 'hour' and 'day' columns to the dataframe based on the 'date_forecast' column.\n",
    "    Also adds 'hour_sin' and 'hour_cos' columns for the hour of the day.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if 'date_forecast' exists in the dataframe\n",
    "    if 'date_forecast' in df.columns:\n",
    "        # Convert the 'date_forecast' column to datetime format\n",
    "        df['date_forecast'] = pd.to_datetime(df['date_forecast'])\n",
    "        \n",
    "        # Extract month, year, hour and day\n",
    "        df['month'] = df['date_forecast'].dt.month\n",
    "        df['year'] = df['date_forecast'].dt.year\n",
    "        df['hour'] = df['date_forecast'].dt.hour\n",
    "        df['day'] = df['date_forecast'].dt.day\n",
    "        df['hour_sin'] = np.sin(2*np.pi*df['hour']/24)\n",
    "        df['hour_cos'] = np.cos(2*np.pi*df['hour']/24)\n",
    "\n",
    "    else:\n",
    "        print(\"Warning: 'date_forecast' column not found in the dataframe. No date features added.\")\n",
    "        return df  # Keep the 'date_forecast' column in the dataframe\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Adding discretized features for the continuous variables to help tree-based models\n",
    "\n",
    "def bin_columns(dataframe, columns_to_bin, n_bins=5):\n",
    "    \"\"\"\n",
    "    Bins the specified columns of the dataframe into equal-sized bins.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframe: pd.DataFrame\n",
    "    - columns_to_bin: list of strings, the names of the columns to bin\n",
    "    - n_bins: int or dict, the number of bins for each column (if int, use the same number for all columns;\n",
    "              if dict, specify individual numbers with column names as keys)\n",
    "    \n",
    "    Returns:\n",
    "    - binned_dataframe: pd.DataFrame, the dataframe with the specified columns binned\n",
    "    \"\"\"\n",
    "    binned_dataframe = dataframe.copy()\n",
    "    \n",
    "    for column in columns_to_bin:\n",
    "        # Determine the number of bins for this column\n",
    "        bins = n_bins if isinstance(n_bins, int) else n_bins.get(column, 5)\n",
    "        \n",
    "        # Create quantile-based bins\n",
    "        binned_dataframe[f'binned_{column}'] = pd.qcut(\n",
    "            binned_dataframe[column],\n",
    "            q=bins,\n",
    "            labels=False,\n",
    "            duplicates='drop'\n",
    "        )\n",
    "        \n",
    "    return binned_dataframe\n",
    "\n",
    "def add_binned_features(df):\n",
    "    columns_to_bin = [\n",
    "        'super_cooled_liquid_water:kgm2',\n",
    "        'ceiling_height_agl:m',\n",
    "        'cloud_base_agl:m'\n",
    "    ]\n",
    "\n",
    "    # Bin the columns\n",
    "    # df = bin_columns(df, columns_to_bin)\n",
    "    df = bin_columns(df, ['effective_cloud_cover:p'], n_bins=2)\n",
    "    df = bin_columns(df, ['ceiling_height_agl:m'], n_bins=3)\n",
    "    df = bin_columns(df, ['average_wind_speed'], n_bins=5)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_rate_of_change_features(df, features, second_order=False):\n",
    "    \"\"\"\n",
    "    Adds rate of change columns for specified features in the dataframe.\n",
    "    Assumes the dataframe is time sorted. If second_order is True, it also adds the second order rate of change.\n",
    "    \"\"\"\n",
    "    for feature in features:\n",
    "        rate_column_name = feature + '_rate_of_change'\n",
    "        df[rate_column_name] = df[feature].diff().fillna(0)  # Handle the first diff NaN if required\n",
    "        \n",
    "        if second_order:  # Check if second order difference is required\n",
    "            second_order_column_name = feature + '_rate_of_change_of_change'\n",
    "            df[second_order_column_name] = df[rate_column_name].diff().fillna(0)  # Second order difference\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_rate_of_change_features_to_df(df):\n",
    "    # Define the features for which to calculate rate of change\n",
    "    features_to_diff = [\n",
    "        't_1000hPa:K',\n",
    "        'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad:W',\n",
    "        'effective_cloud_cover:p', 'total_radiation:W'\n",
    "    ]\n",
    "\n",
    "    # Add rate of change features\n",
    "    return add_rate_of_change_features(df, features_to_diff, second_order=False)\n",
    "\n",
    "def add_est_obs_feature(df):\n",
    "    \"\"\"\n",
    "    Adds a column to the dataframe that indicates whether the data is estimated or observed.\n",
    "    \"\"\"\n",
    "    # Add the est_obs feature\n",
    "    if 'date_calc' not in df.columns:\n",
    "        # If 'date_calc' does not exist, create 'observed' column and set to 1\n",
    "        df['observed'] = 1\n",
    "        return df\n",
    "    else:\n",
    "        # If 'date_calc' exists, create a new column and set values to 0\n",
    "        df['observed'] = 0\n",
    "        return df.drop(columns=['date_calc'])\n",
    "    \n",
    "def remove_constant_regions(dataframe, column_name=\"pv_measurement\", threshold=72):\n",
    "    \"\"\"\n",
    "    Removes rows where the specified column has constant values for more than the given threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the specified column exists in the dataframe\n",
    "    if column_name not in dataframe.columns:\n",
    "        print(f\"Warning: '{column_name}' column not found in the dataframe. No rows removed.\")\n",
    "        return dataframe\n",
    "    \n",
    "    same_as_previous = dataframe[column_name].eq(dataframe[column_name].shift())\n",
    "    group_ids = (~same_as_previous).cumsum()\n",
    "    to_remove = group_ids[same_as_previous].value_counts() > threshold\n",
    "    group_ids_to_remove = to_remove[to_remove].index\n",
    "    \n",
    "    # Drop entire rows that match the conditions\n",
    "    return dataframe.drop(dataframe[group_ids.isin(group_ids_to_remove)].index)\n",
    "\n",
    "def add_lagged_features(df, features_with__lags, fill_value=None):\n",
    "    \"\"\"\n",
    "    Adds lagged columns for specified features in the dataframe with specific lag periods.\n",
    "    'features_with_specific_lags' is a dictionary with features as keys and specific lag as values.\n",
    "    'fill_value' is what to fill the NaNs with, after shifting.\n",
    "    \"\"\"\n",
    "    for feature, specific_lag in features_with__lags.items():\n",
    "        lag_column_name = f\"{feature}_lag_{specific_lag}\"\n",
    "        df[lag_column_name] = df[feature].shift(specific_lag).fillna(fill_value)\n",
    "    return df\n",
    "\n",
    "def add_lagged_features_to_df(df):\n",
    "    features_with_lags = {\n",
    "        'total_radiation:W': 1,\n",
    "        'total_radiation:W': -1,\n",
    "        'rad_diff:W': 1,\n",
    "        'rad_diff:W': -1,\n",
    "        'total_radiation_1h:J': 1,\n",
    "        'total_radiation_1h:J': -1\n",
    "    }\n",
    "\n",
    "    # Add lagged features for specific lags\n",
    "    return add_lagged_features(df, features_with_lags, fill_value=0)\n",
    "\n",
    "def handle_nan(df):\n",
    "    # Remove the rows where target is nan\n",
    "    try:\n",
    "        df = df[df['pv_measurement'].notna()]\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    # Set all remaining nans to 0\n",
    "    return df.fillna(0)\n",
    "\n",
    "# Aggregate the data to hourly with some aggregation methods for each column\n",
    "aggregation_methods = {\n",
    "    'date_forecast': 'first',\n",
    "    'diffuse_rad:W': 'sum',\n",
    "    'direct_rad:W': 'last',\n",
    "    'clear_sky_rad:W': 'sum',\n",
    "    'diffuse_rad_1h:J': 'last',\n",
    "    'direct_rad_1h:J': 'last',\n",
    "    'clear_sky_energy_1h:J': 'last',\n",
    "    'absolute_humidity_2m:gm3': 'mean',\n",
    "    'air_density_2m:kgm3': 'mean',\n",
    "    'ceiling_height_agl:m': 'max',\n",
    "    'cloud_base_agl:m': 'mean',\n",
    "    'dew_or_rime:idx': 'min',\n",
    "    'dew_point_2m:K': 'mean',\n",
    "    'effective_cloud_cover:p': 'sum',\n",
    "    'elevation:m': 'first',\n",
    "    'fresh_snow_12h:cm': 'max',\n",
    "    'fresh_snow_1h:cm': 'sum',\n",
    "    'fresh_snow_24h:cm': 'max',\n",
    "    'fresh_snow_3h:cm': 'max',\n",
    "    'fresh_snow_6h:cm': 'max',\n",
    "    'is_day:idx': 'max',\n",
    "    'is_in_shadow:idx': 'max',\n",
    "    'msl_pressure:hPa': 'mean',\n",
    "    'precip_5min:mm': 'sum',\n",
    "    'precip_type_5min:idx': 'sum',\n",
    "    'pressure_100m:hPa': 'mean',\n",
    "    'pressure_50m:hPa': 'mean',\n",
    "    'prob_rime:p': 'max',\n",
    "    'rain_water:kgm2': 'sum',\n",
    "    'relative_humidity_1000hPa:p': 'mean',\n",
    "    'sfc_pressure:hPa': 'mean',\n",
    "    'snow_density:kgm3': 'mean',\n",
    "    'snow_depth:cm': 'max',\n",
    "    'snow_drift:idx': 'max',\n",
    "    'snow_melt_10min:mm': 'sum',\n",
    "    'snow_water:kgm2': 'sum',\n",
    "    'sun_azimuth:d': 'first',\n",
    "    'sun_elevation:d': 'sum',\n",
    "    'super_cooled_liquid_water:kgm2': 'sum',\n",
    "    't_1000hPa:K': 'mean',\n",
    "    'total_cloud_cover:p': 'mean',\n",
    "    'visibility:m': 'mean',\n",
    "    'wind_speed_10m:ms': 'mean',\n",
    "    'wind_speed_u_10m:ms': 'mean',\n",
    "    'wind_speed_v_10m:ms': 'mean',\n",
    "    'wind_speed_w_1000hPa:ms': 'mean',\n",
    "    'cloud_base_agl:m': 'max',\n",
    "    'snow_density:kgm3': 'mean'\n",
    "}\n",
    "\n",
    "# Read in the data\n",
    "x_target_A = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "x_train_obs_A = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "x_train_est_A = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "x_test_est_A = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "x_target_B = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "x_train_obs_B = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "x_train_est_B = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "x_test_est_B = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "x_target_C = pd.read_parquet('./data/C/train_targets.parquet')\n",
    "x_train_obs_C = pd.read_parquet('./data/C/X_train_observed.parquet')\n",
    "x_train_est_C = pd.read_parquet('./data/C/X_train_estimated.parquet')\n",
    "x_test_est_C = pd.read_parquet('./data/C/X_test_estimated.parquet')\n",
    "\n",
    "# Rename time to date_forecast in target\n",
    "x_target_A.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "x_target_B.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "x_target_C.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "\n",
    "# Fix missing data for test set. Assumin NaN means 0 in these categories\n",
    "x_test_est_A['effective_cloud_cover:p'] = x_test_est_A['effective_cloud_cover:p'].fillna(0)\n",
    "x_test_est_B['effective_cloud_cover:p'] = x_test_est_B['effective_cloud_cover:p'].fillna(0)\n",
    "x_test_est_C['effective_cloud_cover:p'] = x_test_est_C['effective_cloud_cover:p'].fillna(0)\n",
    "\n",
    "x_test_est_A['total_cloud_cover:p'] = x_test_est_A['total_cloud_cover:p'].fillna(0)\n",
    "x_test_est_B['total_cloud_cover:p'] = x_test_est_B['total_cloud_cover:p'].fillna(0)\n",
    "x_test_est_C['total_cloud_cover:p'] = x_test_est_C['total_cloud_cover:p'].fillna(0)\n",
    "\n",
    "x_test_est_A['cloud_base_agl:m'] = x_test_est_A['cloud_base_agl:m'].fillna(0)\n",
    "x_test_est_B['cloud_base_agl:m'] = x_test_est_B['cloud_base_agl:m'].fillna(0)\n",
    "x_test_est_C['cloud_base_agl:m'] = x_test_est_C['cloud_base_agl:m'].fillna(0)\n",
    "\n",
    "x_test_est_A['ceiling_height_agl:m'] = x_test_est_A['ceiling_height_agl:m'].fillna(0)\n",
    "x_test_est_B['ceiling_height_agl:m'] = x_test_est_B['ceiling_height_agl:m'].fillna(0)\n",
    "x_test_est_C['ceiling_height_agl:m'] = x_test_est_C['ceiling_height_agl:m'].fillna(0)\n",
    "\n",
    "x_test_est_A['snow_density:kgm3'] = x_test_est_A['snow_density:kgm3'].fillna(0)\n",
    "x_test_est_B['snow_density:kgm3'] = x_test_est_B['snow_density:kgm3'].fillna(0)\n",
    "x_test_est_C['snow_density:kgm3'] = x_test_est_C['snow_density:kgm3'].fillna(0)\n",
    "\n",
    "x_test_est_A['snow_drift:idx'] = x_test_est_A['snow_drift:idx'].fillna(0)\n",
    "x_test_est_B['snow_drift:idx'] = x_test_est_B['snow_drift:idx'].fillna(0)\n",
    "x_test_est_C['snow_drift:idx'] = x_test_est_C['snow_drift:idx'].fillna(0)\n",
    "\n",
    "# Resample\n",
    "x_train_obs_A_resampled = x_train_obs_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_A_resampled = x_train_est_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_A_resampled = x_test_est_A.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "x_train_obs_B_resampled = x_train_obs_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_B_resampled = x_train_est_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_B_resampled = x_test_est_B.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "x_train_obs_C_resampled = x_train_obs_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_train_est_C_resampled = x_train_est_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "x_test_est_C_resampled = x_test_est_C.groupby(pd.Grouper(key='date_forecast', freq='1H')).aggregate(aggregation_methods)\n",
    "\n",
    "# Merge\n",
    "split_value = x_train_est_A['date_forecast'].iloc[0]\n",
    "split_index = x_target_A[x_target_A['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_A = x_target_A.iloc[:split_index]\n",
    "x_target_est_A = x_target_A.iloc[split_index:]\n",
    "\n",
    "obs_A = x_train_obs_A_resampled.merge(x_target_obs_A, left_index=True, right_on='date_forecast')\n",
    "est_A = x_train_est_A_resampled.merge(x_target_est_A, left_index=True, right_on='date_forecast')\n",
    "\n",
    "split_value = x_train_est_B['date_forecast'].iloc[0]\n",
    "split_index = x_target_B[x_target_B['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_B = x_target_B.iloc[:split_index]\n",
    "x_target_est_B = x_target_B.iloc[split_index:]\n",
    "\n",
    "obs_B = x_train_obs_B_resampled.merge(x_target_obs_B, left_index=True, right_on='date_forecast')\n",
    "est_B = x_train_est_B_resampled.merge(x_target_est_B, left_index=True, right_on='date_forecast')\n",
    "\n",
    "split_value = x_train_est_C['date_forecast'].iloc[0]\n",
    "split_index = x_target_C[x_target_C['date_forecast'] == split_value].index[0]\n",
    "\n",
    "x_target_obs_C = x_target_C.iloc[:split_index]\n",
    "x_target_est_C = x_target_C.iloc[split_index:]\n",
    "\n",
    "obs_C = x_train_obs_C_resampled.merge(x_target_obs_C, left_index=True, right_on='date_forecast')\n",
    "est_C = x_train_est_C_resampled.merge(x_target_est_C, left_index=True, right_on='date_forecast')\n",
    "\n",
    "# Keep date_forecast in test dfs\n",
    "test_A = x_test_est_A_resampled\n",
    "test_B = x_test_est_B_resampled\n",
    "test_C = x_test_est_C_resampled\n",
    "\n",
    "# Drop all the NaNs\n",
    "test_A = test_A.dropna()\n",
    "test_B = test_B.dropna()\n",
    "test_C = test_C.dropna()\n",
    "\n",
    "def preprocessing(df):\n",
    "    df = add_experimental_features(df.copy())\n",
    "    df = add_date_features(df.copy())\n",
    "    df = add_binned_features(df.copy())\n",
    "    df = add_rate_of_change_features_to_df(df.copy())\n",
    "    df = add_est_obs_feature(df.copy())\n",
    "    df = remove_constant_regions(df.copy())\n",
    "    df = add_lagged_features_to_df(df.copy())\n",
    "    df = handle_nan(df.copy())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Preprocess\n",
    "obs_A = preprocessing(obs_A.copy())\n",
    "est_A = preprocessing(est_A.copy())\n",
    "test_A = preprocessing(test_A.copy())\n",
    "\n",
    "obs_B = preprocessing(obs_B.copy())\n",
    "est_B = preprocessing(est_B.copy())\n",
    "test_B = preprocessing(test_B.copy())\n",
    "\n",
    "obs_C = preprocessing(obs_C.copy())\n",
    "est_C = preprocessing(est_C.copy())\n",
    "test_C = preprocessing(test_C.copy())\n",
    "\n",
    "# Random seeds used for reproducibility\n",
    "# 32 weights: 0.3, 0.3, 0.4\n",
    "# 24 weights: 0.3, 0.3, 0.4\n",
    "# 33 (without winter months 1 and 12) weights: 0.2, 0.4, 0.4\n",
    "# 11 (without winter months 1, 2 and 11, 12) weights: 0.25, 0.35, 0.4\n",
    "# 5 weights: 0.4, 0.3, 0.3\n",
    "# 6 weights 0.3, 0.4, 0.3\n",
    "# 7 weights 0.3, 0.4, 0.3\n",
    "# 8 weights 0.3, 0.3, 0.4\n",
    "# 9 weights 0.3, 0.3, 0.4\n",
    "# 10 weights 0.3, 0.3, 0.4\n",
    "\n",
    "# Best score is the mean prediction of all the seeds mentioned above. The first weight is xgboost, the second is catboost, and the third is autogluon.\n",
    "\n",
    "# Set the random seed\n",
    "np.random.seed(10)\n",
    "\n",
    "# Concatinate\n",
    "A = pd.concat([obs_A, est_A])\n",
    "B = pd.concat([obs_B, est_B])\n",
    "C = pd.concat([obs_C, est_C])\n",
    "\n",
    "# Remove characters unparseable for CatBoost \n",
    "A.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in A.columns]\n",
    "B.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in B.columns]\n",
    "C.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in C.columns]\n",
    "\n",
    "test_A.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_A.columns]\n",
    "test_B.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_B.columns]\n",
    "test_C.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in test_C.columns]\n",
    "\n",
    "# Getting validation data from summer months, because the test set is from summer months. We experimentet with excluding winter months\n",
    "# from the training data here.\n",
    "\n",
    "# Step 1: Filter A to include only months from March to October\n",
    "A = A[A['date_forecast'].dt.month.isin([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])]\n",
    "\n",
    "# Step 2: Identify unique days within May, June, and July\n",
    "summer_months = A[A['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "\n",
    "# Step 3: Sample these days for val_A\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_A = A[A['date_forecast'].dt.date.isin(sampled_days)]\n",
    "\n",
    "# Step 4: Define train_A as the remaining data\n",
    "train_A = A[~A['date_forecast'].dt.date.isin(sampled_days)]\n",
    "\n",
    "# Prepare your features and target variables\n",
    "X_train_A = train_A.drop(columns='pv_measurement')\n",
    "y_train_A = train_A['pv_measurement']\n",
    "X_val_A = val_A.drop(columns='pv_measurement')\n",
    "y_val_A = val_A['pv_measurement']\n",
    "\n",
    "# Repeat for B and C\n",
    "B = B[B['date_forecast'].dt.month.isin([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])]\n",
    "summer_months = B[B['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_B = B[B['date_forecast'].dt.date.isin(sampled_days)]\n",
    "train_B = B[~B['date_forecast'].dt.date.isin(sampled_days)]\n",
    "X_train_B = train_B.drop(columns='pv_measurement')\n",
    "y_train_B = train_B['pv_measurement']\n",
    "X_val_B = val_B.drop(columns='pv_measurement')\n",
    "y_val_B = val_B['pv_measurement']\n",
    "\n",
    "C = C[C['date_forecast'].dt.month.isin([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])]\n",
    "summer_months = C[C['date_forecast'].dt.month.isin([5, 6])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.4), replace=False)\n",
    "val_C = C[C['date_forecast'].dt.date.isin(sampled_days)]\n",
    "train_C = C[~C['date_forecast'].dt.date.isin(sampled_days)]\n",
    "X_train_C = train_C.drop(columns='pv_measurement')\n",
    "y_train_C = train_C['pv_measurement']\n",
    "X_val_C = val_C.drop(columns='pv_measurement')\n",
    "y_val_C = val_C['pv_measurement']\n",
    "\n",
    "# Drop date_forecast\n",
    "train_A = train_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "train_B = train_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "train_C = train_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_A = val_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_B = val_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "val_C = val_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_A = X_train_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_B = X_train_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_train_C = X_train_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_A = X_val_A.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_B = X_val_B.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "X_val_C = X_val_C.drop(columns=['date_forecast', 'date_forecast_x', 'date_forecast_y'])\n",
    "test_A = test_A.drop(columns=['date_forecast'])\n",
    "test_B = test_B.drop(columns=['date_forecast'])\n",
    "test_C = test_C.drop(columns=['date_forecast'])\n",
    "\n",
    "train_auto_A = TabularDataset(train_A)\n",
    "val_auto_A = TabularDataset(val_A)\n",
    "\n",
    "train_auto_B = TabularDataset(train_B)\n",
    "val_auto_B = TabularDataset(val_B)\n",
    "\n",
    "train_auto_C = TabularDataset(train_C)\n",
    "val_auto_C = TabularDataset(val_C)\n",
    "\n",
    "auto_label = 'pv_measurement'\n",
    "\n",
    "# Set the parameters for the XGBoost models\n",
    "params_xgb_A = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.4, \n",
    "    'learning_rate': 0.012, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 0.8, \n",
    "    'reg_lambda': 0.8, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1,\n",
    "    'num_parallel_tree': 2\n",
    "}\n",
    "\n",
    "params_xgb_B = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.8, \n",
    "    'learning_rate': 0.008, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 1, \n",
    "    'reg_lambda': 3, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "params_xgb_C = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.8, \n",
    "    'learning_rate': 0.008, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 1, \n",
    "    'reg_lambda': 3, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "xgb_A = xgb.XGBRegressor(**params_xgb_A)\n",
    "xgb_B = xgb.XGBRegressor(**params_xgb_B)\n",
    "xgb_C = xgb.XGBRegressor(**params_xgb_C)\n",
    "\n",
    "cat_A = CatBoostRegressor(\n",
    "    iterations=5000,         # The number of trees to build\n",
    "    #learning_rate=0.09,     # The learning rate\n",
    "    #depth=10,               # Depth of the tree\n",
    "    loss_function='MAE',     # Loss function to be optimized. RMSE is common for regression.\n",
    "    eval_metric='MAE',       # Evaluation metric for the validation set\n",
    "    #random_seed=42,         # Seed for reproducibility\n",
    "    #verbose=100             # Frequency of logging the training process\n",
    ")\n",
    "\n",
    "cat_B = CatBoostRegressor(\n",
    "    iterations=5000,\n",
    "    #learning_rate=0.09,\n",
    "    #depth=10,\n",
    "    loss_function='MAE',\n",
    "    eval_metric='MAE',\n",
    "    #random_seed=42,\n",
    "    #verbose=100\n",
    ")\n",
    "\n",
    "cat_C = CatBoostRegressor(\n",
    "    iterations=5000,\n",
    "    #learning_rate=0.09,\n",
    "    #depth=10,\n",
    "    loss_function='MAE',\n",
    "    eval_metric='MAE',\n",
    "    #random_seed=42,\n",
    "    #verbose=100\n",
    ")\n",
    "\n",
    "# Prepare data for the XGBoost models. We got them to work the best when having fewer columns\n",
    "xgb_columns = [\n",
    "    'total_radiationW',\n",
    "    'snow_accumulation',\n",
    "    'super_cooled_liquid_waterkgm2',\n",
    "    'average_wind_speed',\n",
    "    'sun_elevationd',\n",
    "    'sun_azimuthd',\n",
    "    'clear_sky_radW',\n",
    "    'month',\n",
    "    't_1000hPaC',\n",
    "    'msl_pressurehPa_scaled',\n",
    "    'rain_waterkgm2',\n",
    "    'cloud_base_aglm',\n",
    "    'effective_cloud_coverp',\n",
    "    'dew_or_rimeidx'\n",
    "]\n",
    "print(train_A.columns)\n",
    "\n",
    "X_train_xgb_A = train_A[xgb_columns]\n",
    "y_train_xgb_A = train_A['pv_measurement']\n",
    "X_test_xgb_A = test_A[xgb_columns]\n",
    "\n",
    "X_train_xgb_B = train_B[xgb_columns]\n",
    "y_train_xgb_B = train_B['pv_measurement']\n",
    "X_test_xgb_B = test_B[xgb_columns]\n",
    "\n",
    "X_train_xgb_C = train_C[xgb_columns]\n",
    "y_train_xgb_C = train_C['pv_measurement']\n",
    "X_test_xgb_C = test_C[xgb_columns]\n",
    "\n",
    "# Train the XGBoost models\n",
    "xgb_A.fit(\n",
    "    X=X_train_xgb_A, y=y_train_xgb_A,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "xgb_B.fit(\n",
    "    X=X_train_xgb_B, y=y_train_xgb_B,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "xgb_C.fit(\n",
    "    X=X_train_xgb_C, y=y_train_xgb_C,\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Train the CatBoost models\n",
    "cat_A.fit(\n",
    "    X_train_A, y_train_A,\n",
    "    eval_set=(X_val_A, y_val_A),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "cat_B.fit(\n",
    "    X_train_B, y_train_B,\n",
    "    eval_set=(X_val_B, y_val_B),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "cat_C.fit(\n",
    "    X_train_C, y_train_C,\n",
    "    eval_set=(X_val_C, y_val_C),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "auto_A = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_A, \n",
    "                                                                                   presets='medium_quality', \n",
    "                                                                                   tuning_data=val_auto_A, \n",
    "                                                                                   use_bag_holdout=True, \n",
    "                                                                                   ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "auto_B = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_B,\n",
    "                                                                                      presets='medium_quality',\n",
    "                                                                                      tuning_data=val_auto_B,\n",
    "                                                                                      use_bag_holdout=True,\n",
    "                                                                                      ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "auto_C = TabularPredictor(label=auto_label, eval_metric='mean_absolute_error', problem_type='regression').fit(train_auto_C,\n",
    "                                                                                        presets='medium_quality',\n",
    "                                                                                        tuning_data=val_auto_C,\n",
    "                                                                                        use_bag_holdout=True,\n",
    "                                                                                        ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})\n",
    "\n",
    "xgb_weight = 0.3\n",
    "cat_weight = 0.3\n",
    "auto_weight = 0.4\n",
    "\n",
    "pred_xgb_A = xgb_A.predict(X_test_xgb_C)\n",
    "pred_xgb_B = xgb_B.predict(X_test_xgb_C)\n",
    "pred_xgb_C = xgb_C.predict(X_test_xgb_C)\n",
    "\n",
    "pred_auto_A = auto_A.predict(test_A)\n",
    "pred_auto_B = auto_B.predict(test_B)\n",
    "pred_auto_C = auto_C.predict(test_C)\n",
    "\n",
    "pred_cat_A = cat_A.predict(test_A)\n",
    "pred_cat_B = cat_B.predict(test_B)\n",
    "pred_cat_C = cat_C.predict(test_C)\n",
    "\n",
    "# Ensemble that seemed the best after some experimentation\n",
    "pred_A = (pred_xgb_A*xgb_weight + pred_cat_A*cat_weight + pred_auto_A*auto_weight)\n",
    "pred_B = (pred_xgb_B*xgb_weight + pred_cat_B*cat_weight + pred_auto_B*auto_weight)\n",
    "pred_C = (pred_xgb_C*xgb_weight + pred_cat_C*cat_weight + pred_auto_C*auto_weight)\n",
    "\n",
    "pred_A = np.clip(pred_A, 0, None)\n",
    "pred_B = np.clip(pred_B, 0, None)\n",
    "pred_C = np.clip(pred_C, 0, None)\n",
    "\n",
    "# Concatenate predictions\n",
    "predictions = np.concatenate([pred_A, pred_B, pred_C])\n",
    "\n",
    "# Save predictions\n",
    "predictions_10 = predictions\n",
    "\n",
    "# Create an id array\n",
    "ids = np.arange(0, len(predictions_10))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'prediction': predictions_10\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('predictions_10.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Create submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_1 = pd.read_csv('predictions_1.csv')['prediction'].values\n",
    "predictions_2 = pd.read_csv('predictions_2.csv')['prediction'].values\n",
    "predictions_3 = pd.read_csv('predictions_3.csv')['prediction'].values\n",
    "predictions_4 = pd.read_csv('predictions_4.csv')['prediction'].values\n",
    "predictions_5 = pd.read_csv('predictions_5.csv')['prediction'].values\n",
    "predictions_6 = pd.read_csv('predictions_6.csv')['prediction'].values\n",
    "predictions_7 = pd.read_csv('predictions_7.csv')['prediction'].values\n",
    "predictions_8 = pd.read_csv('predictions_8.csv')['prediction'].values\n",
    "predictions_9 = pd.read_csv('predictions_9.csv')['prediction'].values\n",
    "predictions_10 = pd.read_csv('predictions_10.csv')['prediction'].values\n",
    "\n",
    "predictions = (predictions_1 + predictions_2 + predictions_3 + predictions_4 + predictions_5 + predictions_6 + predictions_7 + predictions_8 + predictions_9 + predictions_10) / 10\n",
    "\n",
    "output_file = 'submission.csv'\n",
    "\n",
    "# Create an id array\n",
    "ids = np.arange(0, len(predictions))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'prediction': predictions\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Submission saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TDT4173-MPC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
