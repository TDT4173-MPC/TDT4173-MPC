{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "obs_A = pd.read_parquet('../../preprocessing/data/obs_A.parquet')\n",
    "obs_B = pd.read_parquet('../../preprocessing/data/obs_B.parquet')\n",
    "obs_C = pd.read_parquet('../../preprocessing/data/obs_C.parquet')\n",
    "est_A = pd.read_parquet('../../preprocessing/data/est_A.parquet')\n",
    "est_B = pd.read_parquet('../../preprocessing/data/est_B.parquet')\n",
    "est_C = pd.read_parquet('../../preprocessing/data/est_C.parquet')\n",
    "test_A = pd.read_parquet('../../preprocessing/data/test_A.parquet')\n",
    "test_B = pd.read_parquet('../../preprocessing/data/test_B.parquet')\n",
    "test_C = pd.read_parquet('../../preprocessing/data/test_C.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatinate\n",
    "A = pd.concat([obs_A, est_A])\n",
    "B = pd.concat([obs_B, est_B])\n",
    "C = pd.concat([obs_C, est_C])\n",
    "\n",
    "# Data splits for submissions\n",
    "X_A = A.drop(columns='pv_measurement')\n",
    "y_A = A['pv_measurement']\n",
    "X_B = B.drop(columns='pv_measurement')\n",
    "y_B = B['pv_measurement']\n",
    "X_C = C.drop(columns='pv_measurement')\n",
    "y_C = C['pv_measurement']\n",
    "\n",
    "# Data splits for testing\n",
    "train_A, test_A = train_test_split(A, test_size=0.2, shuffle=True, random_state=42)\n",
    "X_train_A = train_A.drop(columns='pv_measurement')\n",
    "y_train_A = train_A['pv_measurement']\n",
    "X_test_A = test_A.drop(columns='pv_measurement')\n",
    "y_test_A = test_A['pv_measurement']\n",
    "\n",
    "train_B, test_B = train_test_split(B, test_size=0.2, shuffle=True, random_state=42)\n",
    "X_train_B = train_B.drop(columns='pv_measurement')\n",
    "y_train_B = train_B['pv_measurement']\n",
    "X_test_B = test_B.drop(columns='pv_measurement')\n",
    "y_test_B = test_B['pv_measurement']\n",
    "\n",
    "train_C, test_C = train_test_split(C, test_size=0.2, shuffle=True, random_state=42)\n",
    "X_train_C = train_C.drop(columns='pv_measurement')\n",
    "y_train_C = train_C['pv_measurement']\n",
    "X_test_C = test_C.drop(columns='pv_measurement')\n",
    "y_test_C = test_C['pv_measurement']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 27233 entries, 1851 to 15795\n",
      "Data columns (total 38 columns):\n",
      " #   Column                                   Non-Null Count  Dtype  \n",
      "---  ------                                   --------------  -----  \n",
      " 0   snow_accumulation                        27233 non-null  float32\n",
      " 1   total_radiation                          27233 non-null  float32\n",
      " 2   sfc_pressure:hPa                         27233 non-null  float32\n",
      " 3   month                                    27233 non-null  int32  \n",
      " 4   year                                     27233 non-null  int32  \n",
      " 5   date_forecast_fft_amplitude              27233 non-null  float64\n",
      " 6   date_forecast_fft_phase                  27233 non-null  float64\n",
      " 7   sun_elevation:d_fft_amplitude            27233 non-null  float64\n",
      " 8   sun_elevation:d_fft_phase                27233 non-null  float64\n",
      " 9   t_1000hPa:K_rate_of_change               27233 non-null  float32\n",
      " 10  clear_sky_rad:W_rate_of_change           27233 non-null  float32\n",
      " 11  direct_rad:W_rate_of_change              27233 non-null  float32\n",
      " 12  effective_cloud_cover:p_rate_of_change   27233 non-null  float32\n",
      " 13  total_cloud_cover:p_rate_of_change       27233 non-null  float32\n",
      " 14  observed                                 27233 non-null  float64\n",
      " 15  sun_azimuth:d_lag_7                      27233 non-null  float32\n",
      " 16  msl_pressure:hPa_lag_3                   27233 non-null  float32\n",
      " 17  sfc_pressure:hPa_lag_8                   27233 non-null  float32\n",
      " 18  t_1000hPa:K_lag_4                        27233 non-null  float32\n",
      " 19  dew_or_rime:idx_lag_11                   27233 non-null  float32\n",
      " 20  relative_humidity_1000hPa:p_lag_-3       27233 non-null  float32\n",
      " 21  temp_dewpoint_diff_lag_-4                27233 non-null  float32\n",
      " 22  dew_point_2m:K_lag_19                    27233 non-null  float32\n",
      " 23  visibility:m_lag_-2                      27233 non-null  float32\n",
      " 24  t_1000hPa:K_rolling_avg_24               27233 non-null  float64\n",
      " 25  msl_pressure:hPa_rolling_avg_24          27233 non-null  float64\n",
      " 26  absolute_humidity_2m:gm3_rolling_avg_24  27233 non-null  float64\n",
      " 27  total_cloud_cover:p_rolling_avg_6        27233 non-null  float64\n",
      " 28  sun_elevation:d_rolling_avg_6            27233 non-null  float64\n",
      " 29  total_radiation_rolling_avg_3            27233 non-null  float64\n",
      " 30  diffuse_rad:W                            27233 non-null  float32\n",
      " 31  diffuse_rad_1h:J                         27233 non-null  float32\n",
      " 32  direct_rad:W                             27233 non-null  float32\n",
      " 33  direct_rad_1h:J                          27233 non-null  float32\n",
      " 34  clear_sky_rad:W                          27233 non-null  float32\n",
      " 35  sun_elevation:d                          27233 non-null  float32\n",
      " 36  effective_cloud_cover:p                  27233 non-null  float32\n",
      " 37  clear_sky_energy_1h:J                    27233 non-null  float32\n",
      "dtypes: float32(25), float64(11), int32(2)\n",
      "memory usage: 5.3 MB\n"
     ]
    }
   ],
   "source": [
    "# Inspect data\n",
    "X_train_A.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost parameters\n",
    "xgb_parameters = {'colsample_bytree': 0.664, \n",
    "              'gamma': 3, \n",
    "              'learning_rate': 0.012, \n",
    "              'max_depth': 12, \n",
    "              'min_child_weight': 15, \n",
    "              'n_estimators': 400, \n",
    "              'reg_alpha': 2, \n",
    "              'reg_lambda': 2, \n",
    "              'subsample': 0.912}\n",
    "\n",
    "# Define base models\n",
    "base_models = [\n",
    "    ('ridge', Ridge()),\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, n_jobs=-1)),\n",
    "    ('xgb', XGBRegressor(**xgb_parameters, n_jobs=-1)),\n",
    "    ('svr', SVR()),\n",
    "    ('cat', CatBoostRegressor(verbose=0)),\n",
    "    ('knn', KNeighborsRegressor()),\n",
    "    ('mlp', MLPRegressor(hidden_layer_sizes=(100,), max_iter=500))\n",
    "]\n",
    "\n",
    "# Define meta-model\n",
    "meta_model = Ridge()\n",
    "\n",
    "# Define stacking ensemble\n",
    "stack_A = StackingRegressor(estimators=base_models, final_estimator=meta_model, cv=5, n_jobs=-1, verbose=1)\n",
    "stack_B = StackingRegressor(estimators=base_models, final_estimator=meta_model, cv=5, n_jobs=-1, verbose=1)\n",
    "stack_C = StackingRegressor(estimators=base_models, final_estimator=meta_model, cv=5, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathiasotnes/anaconda3/envs/TDT4173-MPC/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.24244e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n"
     ]
    }
   ],
   "source": [
    "# Fit stacking ensemble\n",
    "stack_A.fit(X_A, y_A)\n",
    "pickle.dump(stack_A, open('./stack_models/stack_A.pkl', 'wb'))\n",
    "\n",
    "# stack_B.fit(X_B, y_B)\n",
    "# pickle.dump(stack_B, open('./stack_models/stack_B.pkl', 'wb'))\n",
    "\n",
    "# stack_C.fit(X_C, y_C)\n",
    "# pickle.dump(stack_C, open('./stack_models/stack_C.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE A: 105.10166213678608\n",
      "MAE B: 15.987515736930082\n",
      "MAE C: 13.354463918146225\n",
      "Total MAE: 44.814547263954125\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "print('MAE A:', mean_absolute_error(y_test_A, stack_A.predict(X_test_A)))\n",
    "# print('MAE B:', mean_absolute_error(y_test_B, stack_B.predict(X_test_B)))\n",
    "# print('MAE C:', mean_absolute_error(y_test_C, stack_C.predict(X_test_C)))\n",
    "\n",
    "# Total MAE for all three locations\n",
    "# print('Total MAE:', (mean_absolute_error(y_test_A, stack_A.predict(X_test_A)) + mean_absolute_error(y_test_B, stack_B.predict(X_test_B)) + mean_absolute_error(y_test_C, stack_C.predict(X_test_C)))/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.961812905434636"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_A.score(X_A, y_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Create submission\n",
    "\n",
    "output_file = 'submission.csv'\n",
    "\n",
    "pred_A = stack_A.predict(test_A)\n",
    "pred_B = stack_B.predict(test_B)\n",
    "pred_C = stack_C.predict(test_C)\n",
    "\n",
    "pred_A = np.clip(pred_A, 0, None)\n",
    "pred_B = np.clip(pred_B, 0, None)\n",
    "pred_C = np.clip(pred_C, 0, None)\n",
    "\n",
    "# Concatenate predictions\n",
    "predictions = np.concatenate([pred_A, pred_B, pred_C])\n",
    "\n",
    "# Create an id array\n",
    "ids = np.arange(0, len(predictions))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'prediction': predictions\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Submission saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TDT4173-MPC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
