{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "obs_A = pd.read_parquet('../../preprocessing/data/obs_A.parquet')\n",
    "obs_B = pd.read_parquet('../../preprocessing/data/obs_B.parquet')\n",
    "obs_C = pd.read_parquet('../../preprocessing/data/obs_C.parquet')\n",
    "est_A = pd.read_parquet('../../preprocessing/data/est_A.parquet')\n",
    "est_B = pd.read_parquet('../../preprocessing/data/est_B.parquet')\n",
    "est_C = pd.read_parquet('../../preprocessing/data/est_C.parquet')\n",
    "test_A = pd.read_parquet('../../preprocessing/data/test_A.parquet').drop(columns='date_forecast')\n",
    "test_B = pd.read_parquet('../../preprocessing/data/test_B.parquet').drop(columns='date_forecast')\n",
    "test_C = pd.read_parquet('../../preprocessing/data/test_C.parquet').drop(columns='date_forecast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatinate\n",
    "A = pd.concat([obs_A, est_A])\n",
    "B = pd.concat([obs_B, est_B])\n",
    "C = pd.concat([obs_C, est_C])\n",
    "\n",
    "A.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in A.columns]\n",
    "B.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in B.columns]\n",
    "C.columns = [col.replace('[', '').replace(']', '').replace(',', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\"', '').replace(\"'\", '').replace(':', '').replace('\\\\', '') for col in C.columns]\n",
    "\n",
    "# Step 1: Filter A to include only months from March to October\n",
    "A = A[A['date_forecast'].dt.month.isin([3, 4, 5, 6, 7, 8, 9, 10])]\n",
    "\n",
    "# Step 2: Identify unique days within May, June, and July\n",
    "summer_months = A[A['date_forecast'].dt.month.isin([5, 6, 7])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "\n",
    "# Step 3: Sample these days for val_A\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.2), replace=False)\n",
    "val_A = A[A['date_forecast'].dt.date.isin(sampled_days)]\n",
    "\n",
    "# Step 4: Define train_A as the remaining data\n",
    "train_A = A[~A['date_forecast'].dt.date.isin(sampled_days)]\n",
    "\n",
    "# Prepare your features and target variables\n",
    "X_train_A = train_A.drop(columns='pv_measurement')\n",
    "y_train_A = train_A['pv_measurement']\n",
    "X_val_A = val_A.drop(columns='pv_measurement')\n",
    "y_val_A = val_A['pv_measurement']\n",
    "\n",
    "# Repeat for B and C\n",
    "B = B[B['date_forecast'].dt.month.isin([3, 4, 5, 6, 7, 8, 9, 10])]\n",
    "summer_months = B[B['date_forecast'].dt.month.isin([5, 6, 7])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.2), replace=False)\n",
    "val_B = B[B['date_forecast'].dt.date.isin(sampled_days)]\n",
    "train_B = B[~B['date_forecast'].dt.date.isin(sampled_days)]\n",
    "X_train_B = train_B.drop(columns='pv_measurement')\n",
    "y_train_B = train_B['pv_measurement']\n",
    "X_val_B = val_B.drop(columns='pv_measurement')\n",
    "y_val_B = val_B['pv_measurement']\n",
    "\n",
    "C = C[C['date_forecast'].dt.month.isin([3, 4, 5, 6, 7, 8, 9, 10])]\n",
    "summer_months = C[C['date_forecast'].dt.month.isin([5, 6, 7])]\n",
    "unique_summer_days = summer_months['date_forecast'].dt.date.unique()\n",
    "sampled_days = np.random.choice(unique_summer_days, size=int(len(unique_summer_days) * 0.2), replace=False)\n",
    "val_C = C[C['date_forecast'].dt.date.isin(sampled_days)]\n",
    "train_C = C[~C['date_forecast'].dt.date.isin(sampled_days)]\n",
    "X_train_C = train_C.drop(columns='pv_measurement')\n",
    "y_train_C = train_C['pv_measurement']\n",
    "X_val_C = val_C.drop(columns='pv_measurement')\n",
    "y_val_C = val_C['pv_measurement']\n",
    "\n",
    "# Drop date_forecast\n",
    "X_train_A = X_train_A.drop(columns='date_forecast')\n",
    "X_train_B = X_train_B.drop(columns='date_forecast')\n",
    "X_train_C = X_train_C.drop(columns='date_forecast')\n",
    "X_val_A = X_val_A.drop(columns='date_forecast')\n",
    "X_val_B = X_val_B.drop(columns='date_forecast')\n",
    "X_val_C = X_val_C.drop(columns='date_forecast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(A.columns)\n",
    "# print(X_train_A['month'].describe())\n",
    "# print(X_val_A['month'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_A = lgb.Dataset(X_train_A, label=y_train_A)\n",
    "val_data_A = lgb.Dataset(X_val_A, label=y_val_A, reference=train_data_A)\n",
    "\n",
    "train_data_B = lgb.Dataset(X_train_B, label=y_train_B)\n",
    "val_data_B = lgb.Dataset(X_val_B, label=y_val_B, reference=train_data_B)\n",
    "\n",
    "train_data_C = lgb.Dataset(X_train_C, label=y_train_C)\n",
    "val_data_C = lgb.Dataset(X_val_C, label=y_val_C, reference=train_data_C)\n",
    "\n",
    "# Set the parameters for the LGBM models\n",
    "params = {\n",
    "    'boosting_type': 'dart',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'num_leaves': 100,\n",
    "    'learning_rate': 0.05,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "# Set the parameters for the XGBoost models\n",
    "parameters_A = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.4, \n",
    "    'learning_rate': 0.012, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 0.8, \n",
    "    'reg_lambda': 0.8, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1,\n",
    "    'eval_set': [(X_train_A, y_train_A), (X_val_A, y_val_A)],\n",
    "    'num_parallel_tree': 5\n",
    "}\n",
    "\n",
    "parameters_B = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.8, \n",
    "    'learning_rate': 0.008, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 1, \n",
    "    'reg_lambda': 3, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1,\n",
    "    'eval_set': [(X_train_B, y_train_B), (X_val_B, y_val_B)]\n",
    "}\n",
    "\n",
    "parameters_C = {\n",
    "    'colsample_bytree': 0.8, \n",
    "    'gamma': 0.8, \n",
    "    'learning_rate': 0.008, \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'n_estimators': 600, \n",
    "    'reg_alpha': 1, \n",
    "    'reg_lambda': 3, \n",
    "    'subsample': 0.912,\n",
    "    'random_state': 0, \n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1,\n",
    "    'eval_set': [(X_train_C, y_train_C), (X_val_C, y_val_C)]\n",
    "}\n",
    "\n",
    "xgb_A = xgb.XGBRegressor(**parameters_A)\n",
    "xgb_B = xgb.XGBRegressor(**parameters_B)\n",
    "xgb_C = xgb.XGBRegressor(**parameters_C)\n",
    "\n",
    "cat_A = CatBoostRegressor(\n",
    "    iterations=12000,        # The number of trees to build\n",
    "    #learning_rate=0.09,     # The learning rate\n",
    "    #depth=10,               # Depth of the tree\n",
    "    loss_function='MAE',     # Loss function to be optimized. RMSE is common for regression.\n",
    "    eval_metric='MAE',       # Evaluation metric for the validation set\n",
    "    #random_seed=42,         # Seed for reproducibility\n",
    "    #verbose=100             # Frequency of logging the training process\n",
    ")\n",
    "\n",
    "cat_B = CatBoostRegressor(\n",
    "    iterations=12000,\n",
    "    #learning_rate=0.09,\n",
    "    #depth=10,\n",
    "    loss_function='MAE',\n",
    "    eval_metric='MAE',\n",
    "    #random_seed=42,\n",
    "    #verbose=100\n",
    ")\n",
    "\n",
    "cat_C = CatBoostRegressor(\n",
    "    iterations=12000,\n",
    "    #learning_rate=0.09,\n",
    "    #depth=10,\n",
    "    loss_function='MAE',\n",
    "    eval_metric='MAE',\n",
    "    #random_seed=42,\n",
    "    #verbose=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LGBM models\n",
    "\n",
    "lgbm_A = lgb.train(params,\n",
    "                train_data_A,\n",
    "                num_boost_round=500,\n",
    "                valid_sets=[val_data_A],\n",
    "                early_stopping_rounds=50,\n",
    "                verbose_eval=50)\n",
    "\n",
    "lgbm_B = lgb.train(params,\n",
    "                train_data_B,\n",
    "                num_boost_round=500,\n",
    "                valid_sets=[val_data_B],\n",
    "                early_stopping_rounds=50,\n",
    "                verbose_eval=50)\n",
    "\n",
    "lgbm_C = lgb.train(params,\n",
    "                train_data_C,\n",
    "                num_boost_round=500,\n",
    "                valid_sets=[val_data_C],\n",
    "                early_stopping_rounds=50,\n",
    "                verbose_eval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the XGBoost models\n",
    "xgb_A.fit(\n",
    "    X=X_train_A, y=y_train_A,\n",
    "    eval_set=[(X_train_A, y_train_A), (X_val_A, y_val_A)],\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "xgb_B.fit(\n",
    "    X=X_train_B, y=y_train_B,\n",
    "    eval_set=[(X_train_B, y_train_B), (X_val_B, y_val_B)],\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "xgb_C.fit(\n",
    "    X=X_train_C, y=y_train_C,\n",
    "    eval_set=[(X_train_C, y_train_C), (X_val_C, y_val_C)],\n",
    "    eval_metric='mae',\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CatBoost models\n",
    "cat_A.fit(\n",
    "    X_train_A, y_train_A,\n",
    "    eval_set=(X_val_A, y_val_A),\n",
    "    use_best_model=True,\n",
    "    plot=True\n",
    ")\n",
    "\n",
    "cat_B.fit(\n",
    "    X_train_B, y_train_B,\n",
    "    eval_set=(X_val_B, y_val_B),\n",
    "    use_best_model=True,\n",
    "    plot=True\n",
    ")\n",
    "\n",
    "cat_C.fit(\n",
    "    X_train_C, y_train_C,\n",
    "    eval_set=(X_val_C, y_val_C),\n",
    "    use_best_model=True,\n",
    "    plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models\n",
    "y_pred_lgbm_A = lgbm_A.predict(X_val_A, num_iteration=lgbm_A.best_iteration)\n",
    "y_pred_lgbm_B = lgbm_B.predict(X_val_B, num_iteration=lgbm_B.best_iteration)\n",
    "y_pred_lgbm_C = lgbm_C.predict(X_val_C, num_iteration=lgbm_C.best_iteration)\n",
    "print('LGBM MAE:', (mean_absolute_error(y_val_A, y_pred_lgbm_A) + mean_absolute_error(y_val_B, y_pred_lgbm_B) + mean_absolute_error(y_val_C, y_pred_lgbm_C)) / 3)\n",
    "print('LGBM ME: ', (np.mean(y_val_A - y_pred_lgbm_A) + np.mean(y_val_B - y_pred_lgbm_B) + np.mean(y_val_C - y_pred_lgbm_C)) / 3)\n",
    "\n",
    "y_pred_xgb_A = xgb_A.predict(X_val_A)\n",
    "y_pred_xgb_B = xgb_B.predict(X_val_B)\n",
    "y_pred_xgb_C = xgb_C.predict(X_val_C)\n",
    "print('XGBoost MAE:', (mean_absolute_error(y_val_A, y_pred_xgb_A) + mean_absolute_error(y_val_B, y_pred_xgb_B) + mean_absolute_error(y_val_C, y_pred_xgb_C)) / 3)\n",
    "print('XGBoost ME: ', (np.mean(y_val_A - y_pred_xgb_A) + np.mean(y_val_B - y_pred_xgb_B) + np.mean(y_val_C - y_pred_xgb_C)) / 3)\n",
    "\n",
    "y_pred_cat_A = cat_A.predict(X_val_A)\n",
    "y_pred_cat_B = cat_B.predict(X_val_B)\n",
    "y_pred_cat_C = cat_C.predict(X_val_C)\n",
    "print('CatBoost MAE:', (mean_absolute_error(y_val_A, y_pred_cat_A) + mean_absolute_error(y_val_B, y_pred_cat_B) + mean_absolute_error(y_val_C, y_pred_cat_C)) / 3)\n",
    "print('CatBoost ME: ', (np.mean(y_val_A - y_pred_cat_A) + np.mean(y_val_B - y_pred_cat_B) + np.mean(y_val_C - y_pred_cat_C)) / 3)\n",
    "\n",
    "y_pred_A = (y_pred_lgbm_A + y_pred_xgb_A + y_pred_cat_A) / 3\n",
    "y_pred_B = (y_pred_lgbm_B + y_pred_xgb_B + y_pred_cat_B) / 3\n",
    "y_pred_C = (y_pred_lgbm_C + y_pred_xgb_C + y_pred_cat_C) / 3\n",
    "print('MAE:', (mean_absolute_error(y_val_A, y_pred_A) + mean_absolute_error(y_val_B, y_pred_B) + mean_absolute_error(y_val_C, y_pred_C)) / 3)\n",
    "print('ME: ', (np.mean(y_val_A - y_pred_A) + np.mean(y_val_B - y_pred_B) + np.mean(y_val_C - y_pred_C)) / 3) \n",
    "\n",
    "# Correlation between models\n",
    "corr_lgbm_xgb = np.corrcoef(y_pred_lgbm_A, y_pred_xgb_A)[0, 1]\n",
    "print(\"LGBM X XGB:\", corr_between_models)\n",
    "\n",
    "corr_lgbm_cat = np.corrcoef(y_pred_lgbm_A, y_pred_cat_A)[0, 1]\n",
    "print(\"LGBM X CAT:\", corr_between_models)\n",
    "\n",
    "corr_xgb_cat = np.corrcoef(y_pred_xgb_A, y_pred_cat_A)[0, 1]\n",
    "print(\"XGB X CAT:\", corr_between_models)\n",
    "\n",
    "# Correlation with target\n",
    "corr_with_target = np.corrcoef(y_val_A, y_pred_lgbm_A)[0, 1]\n",
    "print(\"Corr target:\", corr_with_target)\n",
    "\n",
    "# Summer months\n",
    "# LGBM MAE: 148.28078911595432\n",
    "# LGBM ME:  11.773495640279117\n",
    "# XGBoost MAE: 145.50960966911052\n",
    "# XGBoost ME:  0.8305439643785095\n",
    "# MAE: 146.10835453794607\n",
    "# ME:  6.3020198023288145\n",
    "\n",
    "# LGBM MAE: 126.37001068523796\n",
    "# LGBM ME:  5.5407880468098485\n",
    "# XGBoost MAE: 124.35035350695496\n",
    "# XGBoost ME:  -3.8881690714502617\n",
    "# MAE: 124.70628429080047\n",
    "# ME:  0.8263094876797941\n",
    "\n",
    "# LGBM MAE: 113.34762612551407\n",
    "# LGBM ME:  16.62480425349082\n",
    "# XGBoost MAE: 111.98328058203373\n",
    "# XGBoost ME:  6.948942789763294\n",
    "# MAE: 112.08168399219078\n",
    "# ME:  11.786873521627056\n",
    "\n",
    "# LGBM MAE: 151.2805862765213\n",
    "# LGBM ME:  -0.3526988319681325\n",
    "# XGBoost MAE: 151.96840624926145\n",
    "# XGBoost ME:  -9.558960567470928\n",
    "# MAE: 150.68537589106043\n",
    "# ME:  -4.955829699719531\n",
    "# Corr models: 0.9964906912560275\n",
    "# Corr target: 0.9196798206675115"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TDT4173-MPC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
